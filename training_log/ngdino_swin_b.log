2024/10/23 07:23:04 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 307970475
    GPU 0,1,2,3: NVIDIA A100-SXM4-40GB
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0a0
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 307970475
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 4
------------------------------------------------------------

2024/10/23 07:23:06 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=32, enable=False)
backend_args = None
coco_od_dataset = dict(
    ann_file='o365v1_train_odvg.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='data/objects365v1/',
    filter_cfg=dict(filter_empty_gt=False),
    label_map_file='o365v1_label_map.json',
    pipeline=[
        dict(backend_args=None, type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            transforms=[
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                400,
                                4200,
                            ),
                            (
                                500,
                                4200,
                            ),
                            (
                                600,
                                4200,
                            ),
                        ],
                        type='RandomChoiceResize'),
                    dict(
                        allow_negative_crop=True,
                        crop_size=(
                            384,
                            600,
                        ),
                        crop_type='absolute_range',
                        type='RandomCrop'),
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
            ],
            type='RandomChoice'),
        dict(min_gt_bbox_wh=(
            0.01,
            0.01,
        ), type='FilterAnnotations'),
        dict(
            max_tokens=256,
            num_sample_negative=85,
            tokenizer_name=
            '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
            type='RandomSamplingNegPos'),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'flip',
                'flip_direction',
                'text',
                'custom_entities',
                'tokens_positive',
                'dataset_mode',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    type='ODVGDataset')
data_root = 'data/objects365v1/'
dataset_prefixes = [
    'grefcoco_val',
]
dataset_type = 'ODVGDataset'
datasets = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        backend_args=None,
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        test_mode=True,
        type='MDETRStyleRefCocoDataset'),
]
default_hooks = dict(
    checkpoint=dict(interval=1, type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='GroundingVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
lang_model_name = '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased'
launcher = 'pytorch'
load_from = '/mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-b_numbranch_pretrain.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
max_epochs = 5
metrics = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        iou_thrs=0.5,
        metric='bbox',
        thresh_f1=1.0,
        thresh_score=0.7,
        type='RefDroneMetric'),
]
model = dict(
    as_two_stage=True,
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.3,
        drop_rate=0.0,
        embed_dims=128,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            '/mnt/public/usr/sunzhichao/mmdetection/swin_base_patch4_window12_384_22k.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            4,
            8,
            16,
            32,
        ],
        out_indices=(
            1,
            2,
            3,
        ),
        patch_norm=True,
        pretrain_img_size=384,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=12,
        with_cp=True),
    bbox_head=dict(
        contrastive_cfg=dict(bias=True, log_scale='auto', max_text_len=256),
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            type='FocalLoss',
            use_sigmoid=True),
        num_classes=256,
        sync_cls_avg_factor=True,
        type='GroundingDINOHeadNumv11'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    decoder=dict(
        layer_cfg=dict(
            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            cross_attn_text_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),
        num_layers=6,
        post_norm_cfg=None,
        return_intermediate=True),
    dn_cfg=dict(
        box_noise_scale=1.0,
        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
        label_noise_scale=0.5),
    encoder=dict(
        fusion_layer_cfg=dict(
            embed_dim=1024,
            init_values=0.0001,
            l_dim=256,
            num_heads=4,
            v_dim=256),
        layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=4)),
        num_cp=6,
        num_layers=6,
        text_layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=1024, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=4))),
    language_model=dict(
        add_pooling_layer=False,
        max_tokens=256,
        name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        pad_to_max=False,
        special_tokens_list=[
            '[CLS]',
            '[SEP]',
            '.',
            '?',
        ],
        type='BertModel',
        use_sub_sentence_represent=True),
    neck=dict(
        act_cfg=None,
        bias=True,
        in_channels=[
            256,
            512,
            1024,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=4,
        out_channels=256,
        type='ChannelMapper'),
    num_queries=900,
    positional_encoding=dict(
        normalize=True, num_feats=128, offset=0.0, temperature=20),
    test_cfg=dict(max_per_img=300),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='BinaryFocalLossCost', weight=2.0),
                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                dict(iou_mode='giou', type='IoUCost', weight=2.0),
            ],
            type='HungarianAssigner')),
    type='NumGroundingDINO',
    use_autocast=True,
    with_box_refine=True)
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0002, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.1))),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=5,
        gamma=0.1,
        milestones=[
            3,
        ],
        type='MultiStepLR'),
]
pretrained = '/mnt/public/usr/sunzhichao/mmdetection/swin_base_patch4_window12_384_22k.pth'
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
                backend_args=None,
                data_prefix=dict(
                    img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='pillow',
                        type='LoadImageFromFile'),
                    dict(
                        backend='pillow',
                        keep_ratio=True,
                        scale=(
                            800,
                            1333,
                        ),
                        type='FixScaleResize'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(
                        meta_keys=(
                            'img_id',
                            'img_path',
                            'ori_shape',
                            'img_shape',
                            'scale_factor',
                            'text',
                            'custom_entities',
                            'tokens_positive',
                        ),
                        type='PackDetInputs'),
                ],
                return_classes=True,
                test_mode=True,
                type='MDETRStyleRefCocoDataset'),
        ],
        type='ConcatDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'tokens_positive',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=5, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=4,
    dataset=dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_train7_vg.json',
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        filter_cfg=dict(filter_empty_gt=False),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(prob=0.0, type='RandomFlip'),
            dict(
                keep_ratio=True,
                scales=[
                    (
                        480,
                        1333,
                    ),
                    (
                        512,
                        1333,
                    ),
                    (
                        544,
                        1333,
                    ),
                    (
                        576,
                        1333,
                    ),
                    (
                        608,
                        1333,
                    ),
                    (
                        640,
                        1333,
                    ),
                    (
                        672,
                        1333,
                    ),
                    (
                        704,
                        1333,
                    ),
                    (
                        736,
                        1333,
                    ),
                    (
                        768,
                        1333,
                    ),
                    (
                        800,
                        1333,
                    ),
                ],
                type='RandomChoiceResize'),
            dict(
                max_tokens=256,
                num_sample_negative=85,
                tokenizer_name=
                '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
                type='RandomSamplingNegPos'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                    'dataset_mode',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        type='ODVGDataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.0, type='RandomFlip'),
    dict(
        keep_ratio=True,
        scales=[
            (
                480,
                1333,
            ),
            (
                512,
                1333,
            ),
            (
                544,
                1333,
            ),
            (
                576,
                1333,
            ),
            (
                608,
                1333,
            ),
            (
                640,
                1333,
            ),
            (
                672,
                1333,
            ),
            (
                704,
                1333,
            ),
            (
                736,
                1333,
            ),
            (
                768,
                1333,
            ),
            (
                800,
                1333,
            ),
        ],
        type='RandomChoiceResize'),
    dict(
        max_tokens=256,
        num_sample_negative=85,
        tokenizer_name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        type='RandomSamplingNegPos'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'text',
            'custom_entities',
            'tokens_positive',
            'dataset_mode',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
                backend_args=None,
                data_prefix=dict(
                    img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='pillow',
                        type='LoadImageFromFile'),
                    dict(
                        backend='pillow',
                        keep_ratio=True,
                        scale=(
                            800,
                            1333,
                        ),
                        type='FixScaleResize'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(
                        meta_keys=(
                            'img_id',
                            'img_path',
                            'ori_shape',
                            'img_shape',
                            'scale_factor',
                            'text',
                            'custom_entities',
                            'tokens_positive',
                        ),
                        type='PackDetInputs'),
                ],
                return_classes=True,
                test_mode=True,
                type='MDETRStyleRefCocoDataset'),
        ],
        type='ConcatDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_dataset_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    backend_args=None,
    data_prefix=dict(img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='pillow',
            type='LoadImageFromFile'),
        dict(
            backend='pillow',
            keep_ratio=True,
            scale=(
                800,
                1333,
            ),
            type='FixScaleResize'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'text',
                'custom_entities',
                'tokens_positive',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    test_mode=True,
    type='MDETRStyleRefCocoDataset')
val_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
val_evaluator_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    iou_thrs=0.5,
    metric='bbox',
    thresh_f1=1.0,
    thresh_score=0.7,
    type='RefDroneMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero'

2024/10/23 07:23:11 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr_mult=0.1
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr=2e-05
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:weight_decay=0.0001
2024/10/23 07:23:14 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr_mult=0.1
2024/10/23 07:23:17 - mmengine - INFO - Loads checkpoint by local backend from path: /mnt/public/usr/sunzhichao/mmdetection/swin_base_patch4_window12_384_22k.pth
Name of parameter - Initialization information

level_embed - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.patch_embed.projection.weight - torch.Size([128, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([1024, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.conv.weight - torch.Size([256, 1024, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.extra_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.3.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.5.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.6.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.single_value_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

query_embedding.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.word_embeddings.weight - torch.Size([30522, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.position_embeddings.weight - torch.Size([512, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight - torch.Size([2, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.weight - torch.Size([256, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

dn_query_generator.label_embedding.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  
2024/10/23 07:23:28 - mmengine - INFO - Load checkpoint from /mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-b_numbranch_pretrain.pth
2024/10/23 07:23:28 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2024/10/23 07:23:28 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2024/10/23 07:23:28 - mmengine - INFO - Checkpoints will be saved to /mnt/public/usr/sunzhichao/mmdetection/work_dirs/pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero.
2024/10/23 07:24:32 - mmengine - INFO - Epoch(train) [1][ 50/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:25:34  time: 1.2773  data_time: 0.0146  memory: 10627  grad_norm: 72.8508  loss: 12.1002  loss_cls: 0.7566  loss_bbox: 0.0769  loss_iou: 0.5189  d0.loss_cls: 0.7839  d0.loss_bbox: 0.0739  d0.loss_iou: 0.5138  d1.loss_cls: 0.7608  d1.loss_bbox: 0.0761  d1.loss_iou: 0.5263  d2.loss_cls: 0.7612  d2.loss_bbox: 0.0756  d2.loss_iou: 0.5317  d3.loss_cls: 0.7609  d3.loss_bbox: 0.0769  d3.loss_iou: 0.5275  d4.loss_cls: 0.7566  d4.loss_bbox: 0.0777  d4.loss_iou: 0.5267  enc_loss_cls: 0.7672  enc_loss_bbox: 0.0794  enc_loss_iou: 0.5404  dn_loss_cls: 0.0187  dn_loss_bbox: 0.0390  dn_loss_iou: 0.3276  d0.dn_loss_cls: 0.0633  d0.dn_loss_bbox: 0.0560  d0.dn_loss_iou: 0.4314  d1.dn_loss_cls: 0.0283  d1.dn_loss_bbox: 0.0416  d1.dn_loss_iou: 0.3464  d2.dn_loss_cls: 0.0219  d2.dn_loss_bbox: 0.0394  d2.dn_loss_iou: 0.3301  d3.dn_loss_cls: 0.0193  d3.dn_loss_bbox: 0.0391  d3.dn_loss_iou: 0.3275  d4.dn_loss_cls: 0.0188  d4.dn_loss_bbox: 0.0389  d4.dn_loss_iou: 0.3268  loss_num: 0.0030  d0.loss_num: 0.0025  d1.loss_num: 0.0028  d2.loss_num: 0.0028  d3.loss_num: 0.0029  d4.loss_num: 0.0029
2024/10/23 07:25:31 - mmengine - INFO - Epoch(train) [1][100/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:21:40  time: 1.1917  data_time: 0.0116  memory: 10338  grad_norm: 68.1927  loss: 9.4741  loss_cls: 0.5867  loss_bbox: 0.0585  loss_iou: 0.3793  d0.loss_cls: 0.6025  d0.loss_bbox: 0.0605  d0.loss_iou: 0.3857  d1.loss_cls: 0.5876  d1.loss_bbox: 0.0612  d1.loss_iou: 0.3942  d2.loss_cls: 0.5969  d2.loss_bbox: 0.0588  d2.loss_iou: 0.3840  d3.loss_cls: 0.5901  d3.loss_bbox: 0.0581  d3.loss_iou: 0.3834  d4.loss_cls: 0.5893  d4.loss_bbox: 0.0580  d4.loss_iou: 0.3771  enc_loss_cls: 0.5968  enc_loss_bbox: 0.0616  enc_loss_iou: 0.3988  dn_loss_cls: 0.0341  dn_loss_bbox: 0.0348  dn_loss_iou: 0.2672  d0.dn_loss_cls: 0.0633  d0.dn_loss_bbox: 0.0518  d0.dn_loss_iou: 0.3631  d1.dn_loss_cls: 0.0424  d1.dn_loss_bbox: 0.0380  d1.dn_loss_iou: 0.2853  d2.dn_loss_cls: 0.0379  d2.dn_loss_bbox: 0.0354  d2.dn_loss_iou: 0.2714  d3.dn_loss_cls: 0.0331  d3.dn_loss_bbox: 0.0349  d3.dn_loss_iou: 0.2670  d4.dn_loss_cls: 0.0319  d4.dn_loss_bbox: 0.0347  d4.dn_loss_iou: 0.2667  loss_num: 0.0020  d0.loss_num: 0.0021  d1.loss_num: 0.0020  d2.loss_num: 0.0020  d3.loss_num: 0.0021  d4.loss_num: 0.0020
2024/10/23 07:26:31 - mmengine - INFO - Epoch(train) [1][150/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:19:45  time: 1.1937  data_time: 0.0112  memory: 10621  grad_norm: 61.0369  loss: 9.5651  loss_cls: 0.5315  loss_bbox: 0.0621  loss_iou: 0.4073  d0.loss_cls: 0.5648  d0.loss_bbox: 0.0588  d0.loss_iou: 0.3952  d1.loss_cls: 0.5500  d1.loss_bbox: 0.0581  d1.loss_iou: 0.3944  d2.loss_cls: 0.5426  d2.loss_bbox: 0.0591  d2.loss_iou: 0.3970  d3.loss_cls: 0.5309  d3.loss_bbox: 0.0630  d3.loss_iou: 0.4096  d4.loss_cls: 0.5284  d4.loss_bbox: 0.0630  d4.loss_iou: 0.4116  enc_loss_cls: 0.5578  enc_loss_bbox: 0.0622  enc_loss_iou: 0.4237  dn_loss_cls: 0.0329  dn_loss_bbox: 0.0398  dn_loss_iou: 0.3066  d0.dn_loss_cls: 0.0750  d0.dn_loss_bbox: 0.0595  d0.dn_loss_iou: 0.4158  d1.dn_loss_cls: 0.0388  d1.dn_loss_bbox: 0.0432  d1.dn_loss_iou: 0.3283  d2.dn_loss_cls: 0.0348  d2.dn_loss_bbox: 0.0404  d2.dn_loss_iou: 0.3096  d3.dn_loss_cls: 0.0325  d3.dn_loss_bbox: 0.0399  d3.dn_loss_iou: 0.3066  d4.dn_loss_cls: 0.0327  d4.dn_loss_bbox: 0.0398  d4.dn_loss_iou: 0.3062  loss_num: 0.0020  d0.loss_num: 0.0019  d1.loss_num: 0.0020  d2.loss_num: 0.0019  d3.loss_num: 0.0019  d4.loss_num: 0.0020
2024/10/23 07:27:30 - mmengine - INFO - Epoch(train) [1][200/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:18:11  time: 1.1869  data_time: 0.0111  memory: 10624  grad_norm: 67.4036  loss: 10.3502  loss_cls: 0.6067  loss_bbox: 0.0539  loss_iou: 0.4545  d0.loss_cls: 0.6425  d0.loss_bbox: 0.0530  d0.loss_iou: 0.4503  d1.loss_cls: 0.6278  d1.loss_bbox: 0.0523  d1.loss_iou: 0.4467  d2.loss_cls: 0.6182  d2.loss_bbox: 0.0529  d2.loss_iou: 0.4479  d3.loss_cls: 0.6100  d3.loss_bbox: 0.0531  d3.loss_iou: 0.4527  d4.loss_cls: 0.6099  d4.loss_bbox: 0.0526  d4.loss_iou: 0.4506  enc_loss_cls: 0.6313  enc_loss_bbox: 0.0581  enc_loss_iou: 0.4797  dn_loss_cls: 0.0386  dn_loss_bbox: 0.0337  dn_loss_iou: 0.3027  d0.dn_loss_cls: 0.0722  d0.dn_loss_bbox: 0.0497  d0.dn_loss_iou: 0.4040  d1.dn_loss_cls: 0.0456  d1.dn_loss_bbox: 0.0360  d1.dn_loss_iou: 0.3222  d2.dn_loss_cls: 0.0407  d2.dn_loss_bbox: 0.0342  d2.dn_loss_iou: 0.3065  d3.dn_loss_cls: 0.0375  d3.dn_loss_bbox: 0.0338  d3.dn_loss_iou: 0.3028  d4.dn_loss_cls: 0.0379  d4.dn_loss_bbox: 0.0337  d4.dn_loss_iou: 0.3022  loss_num: 0.0019  d0.loss_num: 0.0019  d1.loss_num: 0.0019  d2.loss_num: 0.0019  d3.loss_num: 0.0019  d4.loss_num: 0.0019
2024/10/23 07:28:30 - mmengine - INFO - Epoch(train) [1][250/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:16:54  time: 1.1909  data_time: 0.0115  memory: 10627  grad_norm: 70.4847  loss: 10.2785  loss_cls: 0.5599  loss_bbox: 0.0592  loss_iou: 0.4781  d0.loss_cls: 0.5986  d0.loss_bbox: 0.0605  d0.loss_iou: 0.4876  d1.loss_cls: 0.5770  d1.loss_bbox: 0.0588  d1.loss_iou: 0.4894  d2.loss_cls: 0.5707  d2.loss_bbox: 0.0596  d2.loss_iou: 0.4770  d3.loss_cls: 0.5627  d3.loss_bbox: 0.0595  d3.loss_iou: 0.4783  d4.loss_cls: 0.5598  d4.loss_bbox: 0.0592  d4.loss_iou: 0.4775  enc_loss_cls: 0.5884  enc_loss_bbox: 0.0631  enc_loss_iou: 0.5198  dn_loss_cls: 0.0160  dn_loss_bbox: 0.0360  dn_loss_iou: 0.3183  d0.dn_loss_cls: 0.0556  d0.dn_loss_bbox: 0.0521  d0.dn_loss_iou: 0.4316  d1.dn_loss_cls: 0.0233  d1.dn_loss_bbox: 0.0380  d1.dn_loss_iou: 0.3367  d2.dn_loss_cls: 0.0176  d2.dn_loss_bbox: 0.0363  d2.dn_loss_iou: 0.3220  d3.dn_loss_cls: 0.0164  d3.dn_loss_bbox: 0.0360  d3.dn_loss_iou: 0.3178  d4.dn_loss_cls: 0.0160  d4.dn_loss_bbox: 0.0360  d4.dn_loss_iou: 0.3178  loss_num: 0.0018  d0.loss_num: 0.0018  d1.loss_num: 0.0017  d2.loss_num: 0.0018  d3.loss_num: 0.0017  d4.loss_num: 0.0018
2024/10/23 07:29:29 - mmengine - INFO - Epoch(train) [1][300/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:15:37  time: 1.1812  data_time: 0.0103  memory: 10627  grad_norm: 62.2323  loss: 11.3275  loss_cls: 0.6794  loss_bbox: 0.0646  loss_iou: 0.4688  d0.loss_cls: 0.7056  d0.loss_bbox: 0.0712  d0.loss_iou: 0.4875  d1.loss_cls: 0.6802  d1.loss_bbox: 0.0714  d1.loss_iou: 0.4886  d2.loss_cls: 0.6843  d2.loss_bbox: 0.0682  d2.loss_iou: 0.4771  d3.loss_cls: 0.6842  d3.loss_bbox: 0.0663  d3.loss_iou: 0.4689  d4.loss_cls: 0.6808  d4.loss_bbox: 0.0652  d4.loss_iou: 0.4714  enc_loss_cls: 0.6988  enc_loss_bbox: 0.0733  enc_loss_iou: 0.5091  dn_loss_cls: 0.0866  dn_loss_bbox: 0.0356  dn_loss_iou: 0.2871  d0.dn_loss_cls: 0.1158  d0.dn_loss_bbox: 0.0566  d0.dn_loss_iou: 0.4007  d1.dn_loss_cls: 0.0897  d1.dn_loss_bbox: 0.0389  d1.dn_loss_iou: 0.3077  d2.dn_loss_cls: 0.0878  d2.dn_loss_bbox: 0.0364  d2.dn_loss_iou: 0.2915  d3.dn_loss_cls: 0.0868  d3.dn_loss_bbox: 0.0358  d3.dn_loss_iou: 0.2880  d4.dn_loss_cls: 0.0849  d4.dn_loss_bbox: 0.0356  d4.dn_loss_iou: 0.2870  loss_num: 0.0017  d0.loss_num: 0.0016  d1.loss_num: 0.0017  d2.loss_num: 0.0016  d3.loss_num: 0.0017  d4.loss_num: 0.0017
2024/10/23 07:30:28 - mmengine - INFO - Epoch(train) [1][350/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:14:29  time: 1.1879  data_time: 0.0106  memory: 10627  grad_norm: 70.5052  loss: 8.9600  loss_cls: 0.4833  loss_bbox: 0.0557  loss_iou: 0.4225  d0.loss_cls: 0.5066  d0.loss_bbox: 0.0577  d0.loss_iou: 0.4287  d1.loss_cls: 0.4894  d1.loss_bbox: 0.0609  d1.loss_iou: 0.4364  d2.loss_cls: 0.4887  d2.loss_bbox: 0.0567  d2.loss_iou: 0.4270  d3.loss_cls: 0.4857  d3.loss_bbox: 0.0562  d3.loss_iou: 0.4247  d4.loss_cls: 0.4818  d4.loss_bbox: 0.0575  d4.loss_iou: 0.4256  enc_loss_cls: 0.5078  enc_loss_bbox: 0.0628  enc_loss_iou: 0.4486  dn_loss_cls: 0.0190  dn_loss_bbox: 0.0336  dn_loss_iou: 0.2671  d0.dn_loss_cls: 0.0480  d0.dn_loss_bbox: 0.0490  d0.dn_loss_iou: 0.3607  d1.dn_loss_cls: 0.0256  d1.dn_loss_bbox: 0.0358  d1.dn_loss_iou: 0.2826  d2.dn_loss_cls: 0.0217  d2.dn_loss_bbox: 0.0339  d2.dn_loss_iou: 0.2690  d3.dn_loss_cls: 0.0196  d3.dn_loss_bbox: 0.0336  d3.dn_loss_iou: 0.2665  d4.dn_loss_cls: 0.0184  d4.dn_loss_bbox: 0.0336  d4.dn_loss_iou: 0.2667  loss_num: 0.0018  d0.loss_num: 0.0019  d1.loss_num: 0.0018  d2.loss_num: 0.0018  d3.loss_num: 0.0018  d4.loss_num: 0.0018
2024/10/23 07:31:28 - mmengine - INFO - Epoch(train) [1][400/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:13:27  time: 1.1982  data_time: 0.0109  memory: 10633  grad_norm: 62.5385  loss: 8.9976  loss_cls: 0.4705  loss_bbox: 0.0548  loss_iou: 0.4220  d0.loss_cls: 0.5101  d0.loss_bbox: 0.0536  d0.loss_iou: 0.4207  d1.loss_cls: 0.5016  d1.loss_bbox: 0.0550  d1.loss_iou: 0.4193  d2.loss_cls: 0.4854  d2.loss_bbox: 0.0529  d2.loss_iou: 0.4154  d3.loss_cls: 0.4865  d3.loss_bbox: 0.0536  d3.loss_iou: 0.4116  d4.loss_cls: 0.4738  d4.loss_bbox: 0.0549  d4.loss_iou: 0.4215  enc_loss_cls: 0.5048  enc_loss_bbox: 0.0582  enc_loss_iou: 0.4450  dn_loss_cls: 0.0134  dn_loss_bbox: 0.0364  dn_loss_iou: 0.2898  d0.dn_loss_cls: 0.0472  d0.dn_loss_bbox: 0.0536  d0.dn_loss_iou: 0.3857  d1.dn_loss_cls: 0.0213  d1.dn_loss_bbox: 0.0386  d1.dn_loss_iou: 0.3048  d2.dn_loss_cls: 0.0161  d2.dn_loss_bbox: 0.0367  d2.dn_loss_iou: 0.2919  d3.dn_loss_cls: 0.0143  d3.dn_loss_bbox: 0.0365  d3.dn_loss_iou: 0.2892  d4.dn_loss_cls: 0.0138  d4.dn_loss_bbox: 0.0364  d4.dn_loss_iou: 0.2890  loss_num: 0.0020  d0.loss_num: 0.0020  d1.loss_num: 0.0020  d2.loss_num: 0.0020  d3.loss_num: 0.0020  d4.loss_num: 0.0020
2024/10/23 07:32:28 - mmengine - INFO - Epoch(train) [1][450/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:12:22  time: 1.1897  data_time: 0.0107  memory: 10615  grad_norm: 65.1767  loss: 9.5199  loss_cls: 0.5352  loss_bbox: 0.0565  loss_iou: 0.4221  d0.loss_cls: 0.5498  d0.loss_bbox: 0.0603  d0.loss_iou: 0.4426  d1.loss_cls: 0.5392  d1.loss_bbox: 0.0612  d1.loss_iou: 0.4423  d2.loss_cls: 0.5262  d2.loss_bbox: 0.0629  d2.loss_iou: 0.4341  d3.loss_cls: 0.5320  d3.loss_bbox: 0.0586  d3.loss_iou: 0.4253  d4.loss_cls: 0.5263  d4.loss_bbox: 0.0609  d4.loss_iou: 0.4313  enc_loss_cls: 0.5580  enc_loss_bbox: 0.0581  enc_loss_iou: 0.4497  dn_loss_cls: 0.0260  dn_loss_bbox: 0.0337  dn_loss_iou: 0.2895  d0.dn_loss_cls: 0.0588  d0.dn_loss_bbox: 0.0511  d0.dn_loss_iou: 0.3989  d1.dn_loss_cls: 0.0302  d1.dn_loss_bbox: 0.0360  d1.dn_loss_iou: 0.3081  d2.dn_loss_cls: 0.0270  d2.dn_loss_bbox: 0.0338  d2.dn_loss_iou: 0.2917  d3.dn_loss_cls: 0.0250  d3.dn_loss_bbox: 0.0335  d3.dn_loss_iou: 0.2885  d4.dn_loss_cls: 0.0255  d4.dn_loss_bbox: 0.0336  d4.dn_loss_iou: 0.2888  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0014  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/23 07:33:27 - mmengine - INFO - Epoch(train) [1][500/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:11:17  time: 1.1855  data_time: 0.0110  memory: 10624  grad_norm: 69.0197  loss: 9.4179  loss_cls: 0.5207  loss_bbox: 0.0509  loss_iou: 0.4026  d0.loss_cls: 0.5343  d0.loss_bbox: 0.0553  d0.loss_iou: 0.4366  d1.loss_cls: 0.5267  d1.loss_bbox: 0.0555  d1.loss_iou: 0.4302  d2.loss_cls: 0.5239  d2.loss_bbox: 0.0527  d2.loss_iou: 0.4153  d3.loss_cls: 0.5239  d3.loss_bbox: 0.0510  d3.loss_iou: 0.4062  d4.loss_cls: 0.5217  d4.loss_bbox: 0.0510  d4.loss_iou: 0.4023  enc_loss_cls: 0.5505  enc_loss_bbox: 0.0554  enc_loss_iou: 0.4442  dn_loss_cls: 0.0201  dn_loss_bbox: 0.0364  dn_loss_iou: 0.3090  d0.dn_loss_cls: 0.0553  d0.dn_loss_bbox: 0.0548  d0.dn_loss_iou: 0.4285  d1.dn_loss_cls: 0.0247  d1.dn_loss_bbox: 0.0393  d1.dn_loss_iou: 0.3302  d2.dn_loss_cls: 0.0215  d2.dn_loss_bbox: 0.0367  d2.dn_loss_iou: 0.3120  d3.dn_loss_cls: 0.0197  d3.dn_loss_bbox: 0.0363  d3.dn_loss_iou: 0.3081  d4.dn_loss_cls: 0.0199  d4.dn_loss_bbox: 0.0363  d4.dn_loss_iou: 0.3082  loss_num: 0.0016  d0.loss_num: 0.0016  d1.loss_num: 0.0016  d2.loss_num: 0.0017  d3.loss_num: 0.0016  d4.loss_num: 0.0016
2024/10/23 07:34:26 - mmengine - INFO - Epoch(train) [1][550/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:10:11  time: 1.1781  data_time: 0.0120  memory: 10607  grad_norm: 65.8159  loss: 9.5457  loss_cls: 0.5563  loss_bbox: 0.0572  loss_iou: 0.4033  d0.loss_cls: 0.5884  d0.loss_bbox: 0.0643  d0.loss_iou: 0.4246  d1.loss_cls: 0.5745  d1.loss_bbox: 0.0611  d1.loss_iou: 0.4204  d2.loss_cls: 0.5640  d2.loss_bbox: 0.0567  d2.loss_iou: 0.4114  d3.loss_cls: 0.5594  d3.loss_bbox: 0.0561  d3.loss_iou: 0.4072  d4.loss_cls: 0.5585  d4.loss_bbox: 0.0570  d4.loss_iou: 0.4027  enc_loss_cls: 0.5902  enc_loss_bbox: 0.0653  enc_loss_iou: 0.4420  dn_loss_cls: 0.0141  dn_loss_bbox: 0.0351  dn_loss_iou: 0.2864  d0.dn_loss_cls: 0.0559  d0.dn_loss_bbox: 0.0523  d0.dn_loss_iou: 0.3881  d1.dn_loss_cls: 0.0217  d1.dn_loss_bbox: 0.0380  d1.dn_loss_iou: 0.3073  d2.dn_loss_cls: 0.0167  d2.dn_loss_bbox: 0.0356  d2.dn_loss_iou: 0.2906  d3.dn_loss_cls: 0.0150  d3.dn_loss_bbox: 0.0351  d3.dn_loss_iou: 0.2862  d4.dn_loss_cls: 0.0143  d4.dn_loss_bbox: 0.0351  d4.dn_loss_iou: 0.2859  loss_num: 0.0020  d0.loss_num: 0.0019  d1.loss_num: 0.0020  d2.loss_num: 0.0019  d3.loss_num: 0.0019  d4.loss_num: 0.0020
2024/10/23 07:35:26 - mmengine - INFO - Epoch(train) [1][600/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:09:10  time: 1.1935  data_time: 0.0112  memory: 10612  grad_norm: 76.2474  loss: 9.5510  loss_cls: 0.5030  loss_bbox: 0.0665  loss_iou: 0.4271  d0.loss_cls: 0.5506  d0.loss_bbox: 0.0598  d0.loss_iou: 0.4144  d1.loss_cls: 0.5272  d1.loss_bbox: 0.0611  d1.loss_iou: 0.4123  d2.loss_cls: 0.5071  d2.loss_bbox: 0.0661  d2.loss_iou: 0.4220  d3.loss_cls: 0.5186  d3.loss_bbox: 0.0590  d3.loss_iou: 0.4012  d4.loss_cls: 0.5139  d4.loss_bbox: 0.0600  d4.loss_iou: 0.4117  enc_loss_cls: 0.5373  enc_loss_bbox: 0.0683  enc_loss_iou: 0.4530  dn_loss_cls: 0.0265  dn_loss_bbox: 0.0415  dn_loss_iou: 0.3099  d0.dn_loss_cls: 0.0744  d0.dn_loss_bbox: 0.0631  d0.dn_loss_iou: 0.4273  d1.dn_loss_cls: 0.0387  d1.dn_loss_bbox: 0.0447  d1.dn_loss_iou: 0.3305  d2.dn_loss_cls: 0.0316  d2.dn_loss_bbox: 0.0421  d2.dn_loss_iou: 0.3136  d3.dn_loss_cls: 0.0285  d3.dn_loss_bbox: 0.0416  d3.dn_loss_iou: 0.3095  d4.dn_loss_cls: 0.0281  d4.dn_loss_bbox: 0.0414  d4.dn_loss_iou: 0.3089  loss_num: 0.0014  d0.loss_num: 0.0016  d1.loss_num: 0.0015  d2.loss_num: 0.0014  d3.loss_num: 0.0015  d4.loss_num: 0.0014
2024/10/23 07:36:25 - mmengine - INFO - Epoch(train) [1][650/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:08:09  time: 1.1906  data_time: 0.0113  memory: 10621  grad_norm: 69.3338  loss: 9.7392  loss_cls: 0.5293  loss_bbox: 0.0563  loss_iou: 0.4552  d0.loss_cls: 0.5580  d0.loss_bbox: 0.0591  d0.loss_iou: 0.4769  d1.loss_cls: 0.5487  d1.loss_bbox: 0.0585  d1.loss_iou: 0.4662  d2.loss_cls: 0.5393  d2.loss_bbox: 0.0570  d2.loss_iou: 0.4571  d3.loss_cls: 0.5397  d3.loss_bbox: 0.0556  d3.loss_iou: 0.4510  d4.loss_cls: 0.5310  d4.loss_bbox: 0.0561  d4.loss_iou: 0.4539  enc_loss_cls: 0.5656  enc_loss_bbox: 0.0636  enc_loss_iou: 0.5042  dn_loss_cls: 0.0346  dn_loss_bbox: 0.0328  dn_loss_iou: 0.2796  d0.dn_loss_cls: 0.0657  d0.dn_loss_bbox: 0.0488  d0.dn_loss_iou: 0.3800  d1.dn_loss_cls: 0.0398  d1.dn_loss_bbox: 0.0348  d1.dn_loss_iou: 0.2918  d2.dn_loss_cls: 0.0365  d2.dn_loss_bbox: 0.0330  d2.dn_loss_iou: 0.2798  d3.dn_loss_cls: 0.0339  d3.dn_loss_bbox: 0.0327  d3.dn_loss_iou: 0.2780  d4.dn_loss_cls: 0.0336  d4.dn_loss_bbox: 0.0327  d4.dn_loss_iou: 0.2786  loss_num: 0.0017  d0.loss_num: 0.0017  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0017
2024/10/23 07:37:25 - mmengine - INFO - Epoch(train) [1][700/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:07:08  time: 1.1895  data_time: 0.0108  memory: 10618  grad_norm: 68.6106  loss: 9.4729  loss_cls: 0.5461  loss_bbox: 0.0477  loss_iou: 0.3949  d0.loss_cls: 0.5700  d0.loss_bbox: 0.0511  d0.loss_iou: 0.4215  d1.loss_cls: 0.5546  d1.loss_bbox: 0.0517  d1.loss_iou: 0.4106  d2.loss_cls: 0.5530  d2.loss_bbox: 0.0485  d2.loss_iou: 0.3936  d3.loss_cls: 0.5491  d3.loss_bbox: 0.0483  d3.loss_iou: 0.3950  d4.loss_cls: 0.5460  d4.loss_bbox: 0.0483  d4.loss_iou: 0.3944  enc_loss_cls: 0.5718  enc_loss_bbox: 0.0579  enc_loss_iou: 0.4462  dn_loss_cls: 0.0286  dn_loss_bbox: 0.0346  dn_loss_iou: 0.2982  d0.dn_loss_cls: 0.0644  d0.dn_loss_bbox: 0.0510  d0.dn_loss_iou: 0.4002  d1.dn_loss_cls: 0.0383  d1.dn_loss_bbox: 0.0372  d1.dn_loss_iou: 0.3182  d2.dn_loss_cls: 0.0315  d2.dn_loss_bbox: 0.0350  d2.dn_loss_iou: 0.3012  d3.dn_loss_cls: 0.0307  d3.dn_loss_bbox: 0.0346  d3.dn_loss_iou: 0.2986  d4.dn_loss_cls: 0.0295  d4.dn_loss_bbox: 0.0346  d4.dn_loss_iou: 0.2977  loss_num: 0.0014  d0.loss_num: 0.0015  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2024/10/23 07:38:24 - mmengine - INFO - Epoch(train) [1][750/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:06:06  time: 1.1853  data_time: 0.0106  memory: 10618  grad_norm: 67.0088  loss: 9.9853  loss_cls: 0.5379  loss_bbox: 0.0621  loss_iou: 0.4817  d0.loss_cls: 0.5685  d0.loss_bbox: 0.0631  d0.loss_iou: 0.4882  d1.loss_cls: 0.5430  d1.loss_bbox: 0.0633  d1.loss_iou: 0.4924  d2.loss_cls: 0.5397  d2.loss_bbox: 0.0615  d2.loss_iou: 0.4850  d3.loss_cls: 0.5460  d3.loss_bbox: 0.0602  d3.loss_iou: 0.4758  d4.loss_cls: 0.5378  d4.loss_bbox: 0.0619  d4.loss_iou: 0.4805  enc_loss_cls: 0.5637  enc_loss_bbox: 0.0701  enc_loss_iou: 0.5174  dn_loss_cls: 0.0177  dn_loss_bbox: 0.0339  dn_loss_iou: 0.2951  d0.dn_loss_cls: 0.0479  d0.dn_loss_bbox: 0.0520  d0.dn_loss_iou: 0.4074  d1.dn_loss_cls: 0.0232  d1.dn_loss_bbox: 0.0368  d1.dn_loss_iou: 0.3166  d2.dn_loss_cls: 0.0194  d2.dn_loss_bbox: 0.0343  d2.dn_loss_iou: 0.2990  d3.dn_loss_cls: 0.0185  d3.dn_loss_bbox: 0.0339  d3.dn_loss_iou: 0.2947  d4.dn_loss_cls: 0.0173  d4.dn_loss_bbox: 0.0339  d4.dn_loss_iou: 0.2945  loss_num: 0.0016  d0.loss_num: 0.0017  d1.loss_num: 0.0016  d2.loss_num: 0.0016  d3.loss_num: 0.0017  d4.loss_num: 0.0016
2024/10/23 07:39:24 - mmengine - INFO - Epoch(train) [1][800/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:05:07  time: 1.2010  data_time: 0.0105  memory: 10343  grad_norm: 61.3197  loss: 9.5135  loss_cls: 0.5327  loss_bbox: 0.0571  loss_iou: 0.4161  d0.loss_cls: 0.5589  d0.loss_bbox: 0.0603  d0.loss_iou: 0.4289  d1.loss_cls: 0.5372  d1.loss_bbox: 0.0598  d1.loss_iou: 0.4260  d2.loss_cls: 0.5403  d2.loss_bbox: 0.0579  d2.loss_iou: 0.4159  d3.loss_cls: 0.5363  d3.loss_bbox: 0.0582  d3.loss_iou: 0.4213  d4.loss_cls: 0.5335  d4.loss_bbox: 0.0577  d4.loss_iou: 0.4172  enc_loss_cls: 0.5539  enc_loss_bbox: 0.0654  enc_loss_iou: 0.4630  dn_loss_cls: 0.0263  dn_loss_bbox: 0.0354  dn_loss_iou: 0.2908  d0.dn_loss_cls: 0.0651  d0.dn_loss_bbox: 0.0540  d0.dn_loss_iou: 0.3968  d1.dn_loss_cls: 0.0338  d1.dn_loss_bbox: 0.0382  d1.dn_loss_iou: 0.3088  d2.dn_loss_cls: 0.0284  d2.dn_loss_bbox: 0.0358  d2.dn_loss_iou: 0.2928  d3.dn_loss_cls: 0.0256  d3.dn_loss_bbox: 0.0353  d3.dn_loss_iou: 0.2898  d4.dn_loss_cls: 0.0262  d4.dn_loss_bbox: 0.0353  d4.dn_loss_iou: 0.2900  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0013  d4.loss_num: 0.0012
2024/10/23 07:39:40 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 07:39:40 - mmengine - INFO - Saving checkpoint at 1 epochs
2024/10/23 07:39:54 - mmengine - INFO - Epoch(val) [1][ 50/858]    eta: 0:01:24  time: 0.1046  data_time: 0.0057  memory: 10213  
2024/10/23 07:39:58 - mmengine - INFO - Epoch(val) [1][100/858]    eta: 0:01:16  time: 0.0974  data_time: 0.0018  memory: 4267  
2024/10/23 07:40:03 - mmengine - INFO - Epoch(val) [1][150/858]    eta: 0:01:10  time: 0.0973  data_time: 0.0018  memory: 4267  
2024/10/23 07:40:08 - mmengine - INFO - Epoch(val) [1][200/858]    eta: 0:01:05  time: 0.0976  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:13 - mmengine - INFO - Epoch(val) [1][250/858]    eta: 0:01:00  time: 0.0976  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:18 - mmengine - INFO - Epoch(val) [1][300/858]    eta: 0:00:55  time: 0.0974  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:23 - mmengine - INFO - Epoch(val) [1][350/858]    eta: 0:00:50  time: 0.0980  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:28 - mmengine - INFO - Epoch(val) [1][400/858]    eta: 0:00:45  time: 0.0991  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:33 - mmengine - INFO - Epoch(val) [1][450/858]    eta: 0:00:40  time: 0.0985  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:38 - mmengine - INFO - Epoch(val) [1][500/858]    eta: 0:00:35  time: 0.0987  data_time: 0.0020  memory: 4267  
2024/10/23 07:40:43 - mmengine - INFO - Epoch(val) [1][550/858]    eta: 0:00:30  time: 0.0987  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:48 - mmengine - INFO - Epoch(val) [1][600/858]    eta: 0:00:25  time: 0.0987  data_time: 0.0019  memory: 4267  
2024/10/23 07:40:53 - mmengine - INFO - Epoch(val) [1][650/858]    eta: 0:00:20  time: 0.0990  data_time: 0.0020  memory: 4267  
2024/10/23 07:40:57 - mmengine - INFO - Epoch(val) [1][700/858]    eta: 0:00:15  time: 0.0985  data_time: 0.0019  memory: 4267  
2024/10/23 07:41:02 - mmengine - INFO - Epoch(val) [1][750/858]    eta: 0:00:10  time: 0.0984  data_time: 0.0022  memory: 4267  
2024/10/23 07:41:07 - mmengine - INFO - Epoch(val) [1][800/858]    eta: 0:00:05  time: 0.0976  data_time: 0.0019  memory: 4267  
2024/10/23 07:41:12 - mmengine - INFO - Epoch(val) [1][850/858]    eta: 0:00:00  time: 0.0969  data_time: 0.0019  memory: 4267  
2024/10/23 07:41:14 - mmengine - INFO - {'instance_F1_score': 0.002841383921109811, 'instance_acc': 0.01429161503510946, 'image_F1_score': 0.007905138339920948, 'image_acc': 0.04924242424242424}
2024/10/23 07:41:14 - mmengine - INFO - Epoch(val) [1][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.0028  grefcoco_val/refdrone/instance_acc: 0.0143  grefcoco_val/refdrone/image_F1_score: 0.0079  grefcoco_val/refdrone/image_acc: 0.0492  data_time: 0.0021  time: 0.0985
2024/10/23 07:42:14 - mmengine - INFO - Epoch(train) [2][ 50/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:03:48  time: 1.1883  data_time: 0.0098  memory: 10633  grad_norm: 61.4585  loss: 8.6020  loss_cls: 0.4796  loss_bbox: 0.0444  loss_iou: 0.3554  d0.loss_cls: 0.4913  d0.loss_bbox: 0.0501  d0.loss_iou: 0.3872  d1.loss_cls: 0.4878  d1.loss_bbox: 0.0464  d1.loss_iou: 0.3657  d2.loss_cls: 0.4726  d2.loss_bbox: 0.0494  d2.loss_iou: 0.3761  d3.loss_cls: 0.4805  d3.loss_bbox: 0.0465  d3.loss_iou: 0.3633  d4.loss_cls: 0.4832  d4.loss_bbox: 0.0440  d4.loss_iou: 0.3536  enc_loss_cls: 0.5033  enc_loss_bbox: 0.0533  enc_loss_iou: 0.4026  dn_loss_cls: 0.0226  dn_loss_bbox: 0.0334  dn_loss_iou: 0.2885  d0.dn_loss_cls: 0.0573  d0.dn_loss_bbox: 0.0512  d0.dn_loss_iou: 0.3964  d1.dn_loss_cls: 0.0282  d1.dn_loss_bbox: 0.0359  d1.dn_loss_iou: 0.3056  d2.dn_loss_cls: 0.0254  d2.dn_loss_bbox: 0.0337  d2.dn_loss_iou: 0.2911  d3.dn_loss_cls: 0.0228  d3.dn_loss_bbox: 0.0335  d3.dn_loss_iou: 0.2884  d4.dn_loss_cls: 0.0229  d4.dn_loss_bbox: 0.0334  d4.dn_loss_iou: 0.2882  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 07:43:12 - mmengine - INFO - Epoch(train) [2][100/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:02:45  time: 1.1707  data_time: 0.0104  memory: 10491  grad_norm: 57.5264  loss: 7.5703  loss_cls: 0.4010  loss_bbox: 0.0416  loss_iou: 0.3277  d0.loss_cls: 0.4324  d0.loss_bbox: 0.0435  d0.loss_iou: 0.3413  d1.loss_cls: 0.4136  d1.loss_bbox: 0.0412  d1.loss_iou: 0.3285  d2.loss_cls: 0.4016  d2.loss_bbox: 0.0411  d2.loss_iou: 0.3292  d3.loss_cls: 0.3997  d3.loss_bbox: 0.0409  d3.loss_iou: 0.3286  d4.loss_cls: 0.4023  d4.loss_bbox: 0.0407  d4.loss_iou: 0.3255  enc_loss_cls: 0.4418  enc_loss_bbox: 0.0445  enc_loss_iou: 0.3480  dn_loss_cls: 0.0276  dn_loss_bbox: 0.0332  dn_loss_iou: 0.2511  d0.dn_loss_cls: 0.0539  d0.dn_loss_bbox: 0.0506  d0.dn_loss_iou: 0.3501  d1.dn_loss_cls: 0.0335  d1.dn_loss_bbox: 0.0356  d1.dn_loss_iou: 0.2676  d2.dn_loss_cls: 0.0296  d2.dn_loss_bbox: 0.0337  d2.dn_loss_iou: 0.2548  d3.dn_loss_cls: 0.0285  d3.dn_loss_bbox: 0.0333  d3.dn_loss_iou: 0.2517  d4.dn_loss_cls: 0.0287  d4.dn_loss_bbox: 0.0331  d4.dn_loss_iou: 0.2510  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0012  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/23 07:44:11 - mmengine - INFO - Epoch(train) [2][150/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:01:43  time: 1.1811  data_time: 0.0106  memory: 10482  grad_norm: 59.7942  loss: 8.4642  loss_cls: 0.4591  loss_bbox: 0.0469  loss_iou: 0.3621  d0.loss_cls: 0.4784  d0.loss_bbox: 0.0540  d0.loss_iou: 0.3949  d1.loss_cls: 0.4727  d1.loss_bbox: 0.0479  d1.loss_iou: 0.3643  d2.loss_cls: 0.4606  d2.loss_bbox: 0.0477  d2.loss_iou: 0.3630  d3.loss_cls: 0.4591  d3.loss_bbox: 0.0468  d3.loss_iou: 0.3610  d4.loss_cls: 0.4604  d4.loss_bbox: 0.0467  d4.loss_iou: 0.3611  enc_loss_cls: 0.4874  enc_loss_bbox: 0.0561  enc_loss_iou: 0.4102  dn_loss_cls: 0.0233  dn_loss_bbox: 0.0341  dn_loss_iou: 0.2801  d0.dn_loss_cls: 0.0526  d0.dn_loss_bbox: 0.0528  d0.dn_loss_iou: 0.3935  d1.dn_loss_cls: 0.0290  d1.dn_loss_bbox: 0.0367  d1.dn_loss_iou: 0.2967  d2.dn_loss_cls: 0.0238  d2.dn_loss_bbox: 0.0345  d2.dn_loss_iou: 0.2829  d3.dn_loss_cls: 0.0232  d3.dn_loss_bbox: 0.0342  d3.dn_loss_iou: 0.2805  d4.dn_loss_cls: 0.0234  d4.dn_loss_bbox: 0.0341  d4.dn_loss_iou: 0.2798  loss_num: 0.0014  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2024/10/23 07:44:55 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 07:45:11 - mmengine - INFO - Epoch(train) [2][200/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:00:44  time: 1.1953  data_time: 0.0111  memory: 10618  grad_norm: 56.9976  loss: 8.4431  loss_cls: 0.4302  loss_bbox: 0.0452  loss_iou: 0.3644  d0.loss_cls: 0.4882  d0.loss_bbox: 0.0489  d0.loss_iou: 0.3752  d1.loss_cls: 0.4434  d1.loss_bbox: 0.0512  d1.loss_iou: 0.3798  d2.loss_cls: 0.4353  d2.loss_bbox: 0.0462  d2.loss_iou: 0.3664  d3.loss_cls: 0.4335  d3.loss_bbox: 0.0446  d3.loss_iou: 0.3614  d4.loss_cls: 0.4321  d4.loss_bbox: 0.0447  d4.loss_iou: 0.3621  enc_loss_cls: 0.4708  enc_loss_bbox: 0.0564  enc_loss_iou: 0.4071  dn_loss_cls: 0.0123  dn_loss_bbox: 0.0385  dn_loss_iou: 0.3075  d0.dn_loss_cls: 0.0456  d0.dn_loss_bbox: 0.0564  d0.dn_loss_iou: 0.4235  d1.dn_loss_cls: 0.0168  d1.dn_loss_bbox: 0.0404  d1.dn_loss_iou: 0.3262  d2.dn_loss_cls: 0.0130  d2.dn_loss_bbox: 0.0390  d2.dn_loss_iou: 0.3126  d3.dn_loss_cls: 0.0125  d3.dn_loss_bbox: 0.0385  d3.dn_loss_iou: 0.3076  d4.dn_loss_cls: 0.0122  d4.dn_loss_bbox: 0.0385  d4.dn_loss_iou: 0.3072  loss_num: 0.0013  d0.loss_num: 0.0013  d1.loss_num: 0.0013  d2.loss_num: 0.0012  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/23 07:46:10 - mmengine - INFO - Epoch(train) [2][250/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:59:43  time: 1.1813  data_time: 0.0110  memory: 10482  grad_norm: 62.9146  loss: 8.7849  loss_cls: 0.4884  loss_bbox: 0.0457  loss_iou: 0.3709  d0.loss_cls: 0.5325  d0.loss_bbox: 0.0473  d0.loss_iou: 0.3795  d1.loss_cls: 0.5154  d1.loss_bbox: 0.0455  d1.loss_iou: 0.3666  d2.loss_cls: 0.5011  d2.loss_bbox: 0.0456  d2.loss_iou: 0.3674  d3.loss_cls: 0.4941  d3.loss_bbox: 0.0456  d3.loss_iou: 0.3709  d4.loss_cls: 0.4897  d4.loss_bbox: 0.0456  d4.loss_iou: 0.3709  enc_loss_cls: 0.5308  enc_loss_bbox: 0.0526  enc_loss_iou: 0.4110  dn_loss_cls: 0.0138  dn_loss_bbox: 0.0365  dn_loss_iou: 0.2934  d0.dn_loss_cls: 0.0528  d0.dn_loss_bbox: 0.0546  d0.dn_loss_iou: 0.4009  d1.dn_loss_cls: 0.0198  d1.dn_loss_bbox: 0.0395  d1.dn_loss_iou: 0.3125  d2.dn_loss_cls: 0.0149  d2.dn_loss_bbox: 0.0370  d2.dn_loss_iou: 0.2972  d3.dn_loss_cls: 0.0141  d3.dn_loss_bbox: 0.0364  d3.dn_loss_iou: 0.2928  d4.dn_loss_cls: 0.0138  d4.dn_loss_bbox: 0.0365  d4.dn_loss_iou: 0.2927  loss_num: 0.0014  d0.loss_num: 0.0015  d1.loss_num: 0.0014  d2.loss_num: 0.0015  d3.loss_num: 0.0014  d4.loss_num: 0.0015
2024/10/23 07:47:10 - mmengine - INFO - Epoch(train) [2][300/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:58:43  time: 1.1895  data_time: 0.0103  memory: 10627  grad_norm: 60.4106  loss: 8.0044  loss_cls: 0.4117  loss_bbox: 0.0609  loss_iou: 0.3576  d0.loss_cls: 0.4442  d0.loss_bbox: 0.0605  d0.loss_iou: 0.3713  d1.loss_cls: 0.4252  d1.loss_bbox: 0.0624  d1.loss_iou: 0.3670  d2.loss_cls: 0.4243  d2.loss_bbox: 0.0586  d2.loss_iou: 0.3495  d3.loss_cls: 0.4173  d3.loss_bbox: 0.0583  d3.loss_iou: 0.3511  d4.loss_cls: 0.4167  d4.loss_bbox: 0.0583  d4.loss_iou: 0.3498  enc_loss_cls: 0.4457  enc_loss_bbox: 0.0631  enc_loss_iou: 0.3973  dn_loss_cls: 0.0128  dn_loss_bbox: 0.0345  dn_loss_iou: 0.2636  d0.dn_loss_cls: 0.0393  d0.dn_loss_bbox: 0.0525  d0.dn_loss_iou: 0.3705  d1.dn_loss_cls: 0.0166  d1.dn_loss_bbox: 0.0372  d1.dn_loss_iou: 0.2799  d2.dn_loss_cls: 0.0142  d2.dn_loss_bbox: 0.0352  d2.dn_loss_iou: 0.2673  d3.dn_loss_cls: 0.0132  d3.dn_loss_bbox: 0.0345  d3.dn_loss_iou: 0.2638  d4.dn_loss_cls: 0.0129  d4.dn_loss_bbox: 0.0345  d4.dn_loss_iou: 0.2631  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/23 07:48:09 - mmengine - INFO - Epoch(train) [2][350/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:57:43  time: 1.1926  data_time: 0.0124  memory: 10621  grad_norm: 61.2887  loss: 9.0305  loss_cls: 0.4954  loss_bbox: 0.0525  loss_iou: 0.4009  d0.loss_cls: 0.5240  d0.loss_bbox: 0.0542  d0.loss_iou: 0.4184  d1.loss_cls: 0.5053  d1.loss_bbox: 0.0542  d1.loss_iou: 0.4124  d2.loss_cls: 0.4951  d2.loss_bbox: 0.0570  d2.loss_iou: 0.4060  d3.loss_cls: 0.5015  d3.loss_bbox: 0.0516  d3.loss_iou: 0.3978  d4.loss_cls: 0.4959  d4.loss_bbox: 0.0517  d4.loss_iou: 0.3995  enc_loss_cls: 0.5169  enc_loss_bbox: 0.0639  enc_loss_iou: 0.4474  dn_loss_cls: 0.0270  dn_loss_bbox: 0.0342  dn_loss_iou: 0.2779  d0.dn_loss_cls: 0.0583  d0.dn_loss_bbox: 0.0520  d0.dn_loss_iou: 0.3822  d1.dn_loss_cls: 0.0336  d1.dn_loss_bbox: 0.0372  d1.dn_loss_iou: 0.2956  d2.dn_loss_cls: 0.0285  d2.dn_loss_bbox: 0.0349  d2.dn_loss_iou: 0.2819  d3.dn_loss_cls: 0.0272  d3.dn_loss_bbox: 0.0343  d3.dn_loss_iou: 0.2783  d4.dn_loss_cls: 0.0266  d4.dn_loss_bbox: 0.0342  d4.dn_loss_iou: 0.2773  loss_num: 0.0013  d0.loss_num: 0.0013  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/23 07:49:09 - mmengine - INFO - Epoch(train) [2][400/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:56:44  time: 1.1917  data_time: 0.0102  memory: 10615  grad_norm: 64.5544  loss: 8.5489  loss_cls: 0.4537  loss_bbox: 0.0493  loss_iou: 0.3699  d0.loss_cls: 0.4723  d0.loss_bbox: 0.0513  d0.loss_iou: 0.3868  d1.loss_cls: 0.4580  d1.loss_bbox: 0.0541  d1.loss_iou: 0.3798  d2.loss_cls: 0.4543  d2.loss_bbox: 0.0546  d2.loss_iou: 0.3828  d3.loss_cls: 0.4600  d3.loss_bbox: 0.0493  d3.loss_iou: 0.3713  d4.loss_cls: 0.4535  d4.loss_bbox: 0.0497  d4.loss_iou: 0.3705  enc_loss_cls: 0.4617  enc_loss_bbox: 0.0600  enc_loss_iou: 0.4150  dn_loss_cls: 0.0205  dn_loss_bbox: 0.0357  dn_loss_iou: 0.2874  d0.dn_loss_cls: 0.0578  d0.dn_loss_bbox: 0.0550  d0.dn_loss_iou: 0.4087  d1.dn_loss_cls: 0.0311  d1.dn_loss_bbox: 0.0386  d1.dn_loss_iou: 0.3075  d2.dn_loss_cls: 0.0238  d2.dn_loss_bbox: 0.0363  d2.dn_loss_iou: 0.2923  d3.dn_loss_cls: 0.0214  d3.dn_loss_bbox: 0.0359  d3.dn_loss_iou: 0.2883  d4.dn_loss_cls: 0.0209  d4.dn_loss_bbox: 0.0357  d4.dn_loss_iou: 0.2870  loss_num: 0.0012  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0011  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 07:50:08 - mmengine - INFO - Epoch(train) [2][450/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:55:43  time: 1.1835  data_time: 0.0106  memory: 10621  grad_norm: 62.5044  loss: 8.6539  loss_cls: 0.4698  loss_bbox: 0.0538  loss_iou: 0.3880  d0.loss_cls: 0.4990  d0.loss_bbox: 0.0570  d0.loss_iou: 0.4042  d1.loss_cls: 0.4812  d1.loss_bbox: 0.0560  d1.loss_iou: 0.3986  d2.loss_cls: 0.4795  d2.loss_bbox: 0.0541  d2.loss_iou: 0.3915  d3.loss_cls: 0.4763  d3.loss_bbox: 0.0536  d3.loss_iou: 0.3885  d4.loss_cls: 0.4721  d4.loss_bbox: 0.0538  d4.loss_iou: 0.3876  enc_loss_cls: 0.4999  enc_loss_bbox: 0.0617  enc_loss_iou: 0.4329  dn_loss_cls: 0.0123  dn_loss_bbox: 0.0343  dn_loss_iou: 0.2730  d0.dn_loss_cls: 0.0460  d0.dn_loss_bbox: 0.0489  d0.dn_loss_iou: 0.3631  d1.dn_loss_cls: 0.0194  d1.dn_loss_bbox: 0.0364  d1.dn_loss_iou: 0.2873  d2.dn_loss_cls: 0.0144  d2.dn_loss_bbox: 0.0347  d2.dn_loss_iou: 0.2763  d3.dn_loss_cls: 0.0127  d3.dn_loss_bbox: 0.0343  d3.dn_loss_iou: 0.2734  d4.dn_loss_cls: 0.0120  d4.dn_loss_bbox: 0.0342  d4.dn_loss_iou: 0.2729  loss_num: 0.0015  d0.loss_num: 0.0015  d1.loss_num: 0.0015  d2.loss_num: 0.0015  d3.loss_num: 0.0015  d4.loss_num: 0.0015
2024/10/23 07:51:08 - mmengine - INFO - Epoch(train) [2][500/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:54:44  time: 1.1926  data_time: 0.0110  memory: 10621  grad_norm: 76.0886  loss: 8.7879  loss_cls: 0.4719  loss_bbox: 0.0541  loss_iou: 0.4027  d0.loss_cls: 0.5023  d0.loss_bbox: 0.0535  d0.loss_iou: 0.4160  d1.loss_cls: 0.4886  d1.loss_bbox: 0.0514  d1.loss_iou: 0.4052  d2.loss_cls: 0.4861  d2.loss_bbox: 0.0500  d2.loss_iou: 0.3959  d3.loss_cls: 0.4800  d3.loss_bbox: 0.0535  d3.loss_iou: 0.4025  d4.loss_cls: 0.4712  d4.loss_bbox: 0.0544  d4.loss_iou: 0.4035  enc_loss_cls: 0.5177  enc_loss_bbox: 0.0555  enc_loss_iou: 0.4378  dn_loss_cls: 0.0111  dn_loss_bbox: 0.0320  dn_loss_iou: 0.2813  d0.dn_loss_cls: 0.0427  d0.dn_loss_bbox: 0.0488  d0.dn_loss_iou: 0.3796  d1.dn_loss_cls: 0.0180  d1.dn_loss_bbox: 0.0349  d1.dn_loss_iou: 0.2983  d2.dn_loss_cls: 0.0135  d2.dn_loss_bbox: 0.0325  d2.dn_loss_iou: 0.2844  d3.dn_loss_cls: 0.0120  d3.dn_loss_bbox: 0.0320  d3.dn_loss_iou: 0.2806  d4.dn_loss_cls: 0.0112  d4.dn_loss_bbox: 0.0320  d4.dn_loss_iou: 0.2805  loss_num: 0.0014  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0014  d3.loss_num: 0.0015  d4.loss_num: 0.0014
2024/10/23 07:52:07 - mmengine - INFO - Epoch(train) [2][550/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:53:43  time: 1.1841  data_time: 0.0118  memory: 10630  grad_norm: 56.8077  loss: 8.2852  loss_cls: 0.4231  loss_bbox: 0.0549  loss_iou: 0.3777  d0.loss_cls: 0.4467  d0.loss_bbox: 0.0571  d0.loss_iou: 0.3868  d1.loss_cls: 0.4320  d1.loss_bbox: 0.0562  d1.loss_iou: 0.3842  d2.loss_cls: 0.4229  d2.loss_bbox: 0.0562  d2.loss_iou: 0.3828  d3.loss_cls: 0.4286  d3.loss_bbox: 0.0521  d3.loss_iou: 0.3737  d4.loss_cls: 0.4221  d4.loss_bbox: 0.0553  d4.loss_iou: 0.3791  enc_loss_cls: 0.4641  enc_loss_bbox: 0.0643  enc_loss_iou: 0.4105  dn_loss_cls: 0.0141  dn_loss_bbox: 0.0367  dn_loss_iou: 0.2743  d0.dn_loss_cls: 0.0411  d0.dn_loss_bbox: 0.0560  d0.dn_loss_iou: 0.3889  d1.dn_loss_cls: 0.0191  d1.dn_loss_bbox: 0.0398  d1.dn_loss_iou: 0.2938  d2.dn_loss_cls: 0.0161  d2.dn_loss_bbox: 0.0373  d2.dn_loss_iou: 0.2775  d3.dn_loss_cls: 0.0147  d3.dn_loss_bbox: 0.0367  d3.dn_loss_iou: 0.2738  d4.dn_loss_cls: 0.0142  d4.dn_loss_bbox: 0.0366  d4.dn_loss_iou: 0.2738  loss_num: 0.0017  d0.loss_num: 0.0017  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0017
2024/10/23 07:53:07 - mmengine - INFO - Epoch(train) [2][600/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:52:45  time: 1.2004  data_time: 0.0103  memory: 10618  grad_norm: 69.9143  loss: 8.6977  loss_cls: 0.4699  loss_bbox: 0.0498  loss_iou: 0.3866  d0.loss_cls: 0.4919  d0.loss_bbox: 0.0548  d0.loss_iou: 0.4071  d1.loss_cls: 0.4736  d1.loss_bbox: 0.0542  d1.loss_iou: 0.4060  d2.loss_cls: 0.4723  d2.loss_bbox: 0.0514  d2.loss_iou: 0.3961  d3.loss_cls: 0.4673  d3.loss_bbox: 0.0502  d3.loss_iou: 0.3901  d4.loss_cls: 0.4690  d4.loss_bbox: 0.0497  d4.loss_iou: 0.3843  enc_loss_cls: 0.4927  enc_loss_bbox: 0.0582  enc_loss_iou: 0.4299  dn_loss_cls: 0.0121  dn_loss_bbox: 0.0352  dn_loss_iou: 0.2873  d0.dn_loss_cls: 0.0442  d0.dn_loss_bbox: 0.0505  d0.dn_loss_iou: 0.3855  d1.dn_loss_cls: 0.0178  d1.dn_loss_bbox: 0.0375  d1.dn_loss_iou: 0.3036  d2.dn_loss_cls: 0.0137  d2.dn_loss_bbox: 0.0357  d2.dn_loss_iou: 0.2907  d3.dn_loss_cls: 0.0128  d3.dn_loss_bbox: 0.0353  d3.dn_loss_iou: 0.2875  d4.dn_loss_cls: 0.0123  d4.dn_loss_bbox: 0.0352  d4.dn_loss_iou: 0.2870  loss_num: 0.0015  d0.loss_num: 0.0015  d1.loss_num: 0.0015  d2.loss_num: 0.0014  d3.loss_num: 0.0015  d4.loss_num: 0.0015
2024/10/23 07:54:07 - mmengine - INFO - Epoch(train) [2][650/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:51:45  time: 1.1956  data_time: 0.0112  memory: 10612  grad_norm: 70.7313  loss: 9.5429  loss_cls: 0.4947  loss_bbox: 0.0512  loss_iou: 0.3744  d0.loss_cls: 0.5341  d0.loss_bbox: 0.0551  d0.loss_iou: 0.3936  d1.loss_cls: 0.5121  d1.loss_bbox: 0.0518  d1.loss_iou: 0.3789  d2.loss_cls: 0.5079  d2.loss_bbox: 0.0517  d2.loss_iou: 0.3726  d3.loss_cls: 0.5015  d3.loss_bbox: 0.0515  d3.loss_iou: 0.3716  d4.loss_cls: 0.4978  d4.loss_bbox: 0.0513  d4.loss_iou: 0.3737  enc_loss_cls: 0.5415  enc_loss_bbox: 0.0591  enc_loss_iou: 0.4147  dn_loss_cls: 0.0981  dn_loss_bbox: 0.0386  dn_loss_iou: 0.3089  d0.dn_loss_cls: 0.1337  d0.dn_loss_bbox: 0.0573  d0.dn_loss_iou: 0.4303  d1.dn_loss_cls: 0.1053  d1.dn_loss_bbox: 0.0414  d1.dn_loss_iou: 0.3307  d2.dn_loss_cls: 0.1012  d2.dn_loss_bbox: 0.0389  d2.dn_loss_iou: 0.3129  d3.dn_loss_cls: 0.1039  d3.dn_loss_bbox: 0.0386  d3.dn_loss_iou: 0.3086  d4.dn_loss_cls: 0.0983  d4.dn_loss_bbox: 0.0385  d4.dn_loss_iou: 0.3083  loss_num: 0.0014  d0.loss_num: 0.0015  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2024/10/23 07:55:06 - mmengine - INFO - Epoch(train) [2][700/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:50:46  time: 1.1942  data_time: 0.0109  memory: 10630  grad_norm: 57.2467  loss: 8.1161  loss_cls: 0.4487  loss_bbox: 0.0482  loss_iou: 0.3445  d0.loss_cls: 0.4616  d0.loss_bbox: 0.0578  d0.loss_iou: 0.3592  d1.loss_cls: 0.4532  d1.loss_bbox: 0.0512  d1.loss_iou: 0.3509  d2.loss_cls: 0.4551  d2.loss_bbox: 0.0504  d2.loss_iou: 0.3472  d3.loss_cls: 0.4581  d3.loss_bbox: 0.0449  d3.loss_iou: 0.3426  d4.loss_cls: 0.4514  d4.loss_bbox: 0.0442  d4.loss_iou: 0.3420  enc_loss_cls: 0.4769  enc_loss_bbox: 0.0551  enc_loss_iou: 0.3791  dn_loss_cls: 0.0268  dn_loss_bbox: 0.0309  dn_loss_iou: 0.2636  d0.dn_loss_cls: 0.0480  d0.dn_loss_bbox: 0.0468  d0.dn_loss_iou: 0.3632  d1.dn_loss_cls: 0.0308  d1.dn_loss_bbox: 0.0333  d1.dn_loss_iou: 0.2787  d2.dn_loss_cls: 0.0259  d2.dn_loss_bbox: 0.0314  d2.dn_loss_iou: 0.2662  d3.dn_loss_cls: 0.0264  d3.dn_loss_bbox: 0.0309  d3.dn_loss_iou: 0.2629  d4.dn_loss_cls: 0.0262  d4.dn_loss_bbox: 0.0308  d4.dn_loss_iou: 0.2629  loss_num: 0.0013  d0.loss_num: 0.0013  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0013  d4.loss_num: 0.0014
2024/10/23 07:56:06 - mmengine - INFO - Epoch(train) [2][750/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:49:46  time: 1.1883  data_time: 0.0115  memory: 10630  grad_norm: 64.8378  loss: 9.2751  loss_cls: 0.5126  loss_bbox: 0.0582  loss_iou: 0.4311  d0.loss_cls: 0.5502  d0.loss_bbox: 0.0575  d0.loss_iou: 0.4265  d1.loss_cls: 0.5359  d1.loss_bbox: 0.0533  d1.loss_iou: 0.4169  d2.loss_cls: 0.5254  d2.loss_bbox: 0.0541  d2.loss_iou: 0.4197  d3.loss_cls: 0.5173  d3.loss_bbox: 0.0607  d3.loss_iou: 0.4266  d4.loss_cls: 0.5104  d4.loss_bbox: 0.0612  d4.loss_iou: 0.4321  enc_loss_cls: 0.5738  enc_loss_bbox: 0.0579  enc_loss_iou: 0.4447  dn_loss_cls: 0.0163  dn_loss_bbox: 0.0341  dn_loss_iou: 0.2770  d0.dn_loss_cls: 0.0465  d0.dn_loss_bbox: 0.0511  d0.dn_loss_iou: 0.3789  d1.dn_loss_cls: 0.0217  d1.dn_loss_bbox: 0.0367  d1.dn_loss_iou: 0.2924  d2.dn_loss_cls: 0.0172  d2.dn_loss_bbox: 0.0343  d2.dn_loss_iou: 0.2784  d3.dn_loss_cls: 0.0167  d3.dn_loss_bbox: 0.0341  d3.dn_loss_iou: 0.2769  d4.dn_loss_cls: 0.0173  d4.dn_loss_bbox: 0.0341  d4.dn_loss_iou: 0.2765  loss_num: 0.0015  d0.loss_num: 0.0016  d1.loss_num: 0.0015  d2.loss_num: 0.0014  d3.loss_num: 0.0015  d4.loss_num: 0.0015
2024/10/23 07:57:05 - mmengine - INFO - Epoch(train) [2][800/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:48:46  time: 1.1826  data_time: 0.0108  memory: 10621  grad_norm: 63.9631  loss: 9.1713  loss_cls: 0.4454  loss_bbox: 0.0562  loss_iou: 0.4413  d0.loss_cls: 0.4718  d0.loss_bbox: 0.0596  d0.loss_iou: 0.4612  d1.loss_cls: 0.4604  d1.loss_bbox: 0.0561  d1.loss_iou: 0.4427  d2.loss_cls: 0.4479  d2.loss_bbox: 0.0567  d2.loss_iou: 0.4441  d3.loss_cls: 0.4445  d3.loss_bbox: 0.0561  d3.loss_iou: 0.4411  d4.loss_cls: 0.4416  d4.loss_bbox: 0.0569  d4.loss_iou: 0.4457  enc_loss_cls: 0.4753  enc_loss_bbox: 0.0622  enc_loss_iou: 0.4713  dn_loss_cls: 0.0304  dn_loss_bbox: 0.0361  dn_loss_iou: 0.3053  d0.dn_loss_cls: 0.0658  d0.dn_loss_bbox: 0.0552  d0.dn_loss_iou: 0.4161  d1.dn_loss_cls: 0.0359  d1.dn_loss_bbox: 0.0388  d1.dn_loss_iou: 0.3256  d2.dn_loss_cls: 0.0311  d2.dn_loss_bbox: 0.0364  d2.dn_loss_iou: 0.3085  d3.dn_loss_cls: 0.0296  d3.dn_loss_bbox: 0.0361  d3.dn_loss_iou: 0.3052  d4.dn_loss_cls: 0.0292  d4.dn_loss_bbox: 0.0361  d4.dn_loss_iou: 0.3046  loss_num: 0.0012  d0.loss_num: 0.0014  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 07:57:22 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 07:57:22 - mmengine - INFO - Saving checkpoint at 2 epochs
2024/10/23 07:57:34 - mmengine - INFO - Epoch(val) [2][ 50/858]    eta: 0:01:20  time: 0.0994  data_time: 0.0026  memory: 10618  
2024/10/23 07:57:39 - mmengine - INFO - Epoch(val) [2][100/858]    eta: 0:01:14  time: 0.0983  data_time: 0.0019  memory: 4267  
2024/10/23 07:57:44 - mmengine - INFO - Epoch(val) [2][150/858]    eta: 0:01:09  time: 0.0980  data_time: 0.0019  memory: 4267  
2024/10/23 07:57:49 - mmengine - INFO - Epoch(val) [2][200/858]    eta: 0:01:04  time: 0.0984  data_time: 0.0020  memory: 4267  
2024/10/23 07:57:54 - mmengine - INFO - Epoch(val) [2][250/858]    eta: 0:00:59  time: 0.0981  data_time: 0.0019  memory: 4267  
2024/10/23 07:57:59 - mmengine - INFO - Epoch(val) [2][300/858]    eta: 0:00:54  time: 0.0979  data_time: 0.0018  memory: 4267  
2024/10/23 07:58:04 - mmengine - INFO - Epoch(val) [2][350/858]    eta: 0:00:49  time: 0.0980  data_time: 0.0020  memory: 4267  
2024/10/23 07:58:09 - mmengine - INFO - Epoch(val) [2][400/858]    eta: 0:00:45  time: 0.0982  data_time: 0.0019  memory: 4267  
2024/10/23 07:58:14 - mmengine - INFO - Epoch(val) [2][450/858]    eta: 0:00:40  time: 0.0979  data_time: 0.0019  memory: 4267  
2024/10/23 07:58:19 - mmengine - INFO - Epoch(val) [2][500/858]    eta: 0:00:35  time: 0.0982  data_time: 0.0019  memory: 4267  
2024/10/23 07:58:24 - mmengine - INFO - Epoch(val) [2][550/858]    eta: 0:00:30  time: 0.0978  data_time: 0.0019  memory: 4267  
2024/10/23 07:58:28 - mmengine - INFO - Epoch(val) [2][600/858]    eta: 0:00:25  time: 0.0983  data_time: 0.0019  memory: 4267  
2024/10/23 07:58:33 - mmengine - INFO - Epoch(val) [2][650/858]    eta: 0:00:20  time: 0.0984  data_time: 0.0019  memory: 4267  
2024/10/23 07:58:38 - mmengine - INFO - Epoch(val) [2][700/858]    eta: 0:00:15  time: 0.0978  data_time: 0.0018  memory: 4267  
2024/10/23 07:58:43 - mmengine - INFO - Epoch(val) [2][750/858]    eta: 0:00:10  time: 0.0974  data_time: 0.0018  memory: 4267  
2024/10/23 07:58:48 - mmengine - INFO - Epoch(val) [2][800/858]    eta: 0:00:05  time: 0.0978  data_time: 0.0018  memory: 4267  
2024/10/23 07:58:53 - mmengine - INFO - Epoch(val) [2][850/858]    eta: 0:00:00  time: 0.0975  data_time: 0.0019  memory: 4267  
2024/10/23 07:58:55 - mmengine - INFO - {'instance_F1_score': 0.13830118232731797, 'instance_acc': 0.08597359735973598, 'image_F1_score': 0.21502449646162222, 'image_acc': 0.15967365967365968}
2024/10/23 07:58:55 - mmengine - INFO - Epoch(val) [2][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.1383  grefcoco_val/refdrone/instance_acc: 0.0860  grefcoco_val/refdrone/image_F1_score: 0.2150  grefcoco_val/refdrone/image_acc: 0.1597  data_time: 0.0019  time: 0.0981
2024/10/23 07:59:55 - mmengine - INFO - Epoch(train) [3][ 50/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:47:29  time: 1.1897  data_time: 0.0091  memory: 10621  grad_norm: 61.2263  loss: 8.7973  loss_cls: 0.4613  loss_bbox: 0.0522  loss_iou: 0.4052  d0.loss_cls: 0.4869  d0.loss_bbox: 0.0578  d0.loss_iou: 0.4352  d1.loss_cls: 0.4689  d1.loss_bbox: 0.0535  d1.loss_iou: 0.4181  d2.loss_cls: 0.4636  d2.loss_bbox: 0.0518  d2.loss_iou: 0.4099  d3.loss_cls: 0.4620  d3.loss_bbox: 0.0513  d3.loss_iou: 0.4068  d4.loss_cls: 0.4575  d4.loss_bbox: 0.0544  d4.loss_iou: 0.4089  enc_loss_cls: 0.4883  enc_loss_bbox: 0.0604  enc_loss_iou: 0.4527  dn_loss_cls: 0.0180  dn_loss_bbox: 0.0356  dn_loss_iou: 0.2808  d0.dn_loss_cls: 0.0489  d0.dn_loss_bbox: 0.0519  d0.dn_loss_iou: 0.3802  d1.dn_loss_cls: 0.0231  d1.dn_loss_bbox: 0.0382  d1.dn_loss_iou: 0.2988  d2.dn_loss_cls: 0.0197  d2.dn_loss_bbox: 0.0359  d2.dn_loss_iou: 0.2837  d3.dn_loss_cls: 0.0190  d3.dn_loss_bbox: 0.0355  d3.dn_loss_iou: 0.2802  d4.dn_loss_cls: 0.0179  d4.dn_loss_bbox: 0.0355  d4.dn_loss_iou: 0.2803  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0011  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 08:00:54 - mmengine - INFO - Epoch(train) [3][100/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:46:30  time: 1.1891  data_time: 0.0103  memory: 10630  grad_norm: 64.3636  loss: 7.4544  loss_cls: 0.3553  loss_bbox: 0.0466  loss_iou: 0.3351  d0.loss_cls: 0.3905  d0.loss_bbox: 0.0438  d0.loss_iou: 0.3285  d1.loss_cls: 0.3641  d1.loss_bbox: 0.0448  d1.loss_iou: 0.3321  d2.loss_cls: 0.3608  d2.loss_bbox: 0.0458  d2.loss_iou: 0.3304  d3.loss_cls: 0.3557  d3.loss_bbox: 0.0451  d3.loss_iou: 0.3351  d4.loss_cls: 0.3479  d4.loss_bbox: 0.0478  d4.loss_iou: 0.3411  enc_loss_cls: 0.3909  enc_loss_bbox: 0.0500  enc_loss_iou: 0.3579  dn_loss_cls: 0.0159  dn_loss_bbox: 0.0364  dn_loss_iou: 0.2849  d0.dn_loss_cls: 0.0474  d0.dn_loss_bbox: 0.0522  d0.dn_loss_iou: 0.3889  d1.dn_loss_cls: 0.0215  d1.dn_loss_bbox: 0.0390  d1.dn_loss_iou: 0.3010  d2.dn_loss_cls: 0.0166  d2.dn_loss_bbox: 0.0368  d2.dn_loss_iou: 0.2865  d3.dn_loss_cls: 0.0154  d3.dn_loss_bbox: 0.0363  d3.dn_loss_iou: 0.2835  d4.dn_loss_cls: 0.0161  d4.dn_loss_bbox: 0.0363  d4.dn_loss_iou: 0.2836  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 08:01:54 - mmengine - INFO - Epoch(train) [3][150/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:45:30  time: 1.1932  data_time: 0.0135  memory: 10624  grad_norm: 65.2872  loss: 8.5629  loss_cls: 0.4381  loss_bbox: 0.0515  loss_iou: 0.3993  d0.loss_cls: 0.4741  d0.loss_bbox: 0.0596  d0.loss_iou: 0.4277  d1.loss_cls: 0.4494  d1.loss_bbox: 0.0547  d1.loss_iou: 0.4145  d2.loss_cls: 0.4448  d2.loss_bbox: 0.0519  d2.loss_iou: 0.4053  d3.loss_cls: 0.4391  d3.loss_bbox: 0.0520  d3.loss_iou: 0.4032  d4.loss_cls: 0.4381  d4.loss_bbox: 0.0515  d4.loss_iou: 0.4003  enc_loss_cls: 0.4825  enc_loss_bbox: 0.0623  enc_loss_iou: 0.4495  dn_loss_cls: 0.0083  dn_loss_bbox: 0.0332  dn_loss_iou: 0.2769  d0.dn_loss_cls: 0.0410  d0.dn_loss_bbox: 0.0494  d0.dn_loss_iou: 0.3842  d1.dn_loss_cls: 0.0158  d1.dn_loss_bbox: 0.0365  d1.dn_loss_iou: 0.2990  d2.dn_loss_cls: 0.0107  d2.dn_loss_bbox: 0.0338  d2.dn_loss_iou: 0.2813  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.0333  d3.dn_loss_iou: 0.2768  d4.dn_loss_cls: 0.0085  d4.dn_loss_bbox: 0.0331  d4.dn_loss_iou: 0.2764  loss_num: 0.0010  d0.loss_num: 0.0012  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:02:54 - mmengine - INFO - Epoch(train) [3][200/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:44:31  time: 1.1924  data_time: 0.0112  memory: 10621  grad_norm: 67.7746  loss: 7.8691  loss_cls: 0.4211  loss_bbox: 0.0424  loss_iou: 0.3304  d0.loss_cls: 0.4529  d0.loss_bbox: 0.0489  d0.loss_iou: 0.3610  d1.loss_cls: 0.4306  d1.loss_bbox: 0.0448  d1.loss_iou: 0.3459  d2.loss_cls: 0.4208  d2.loss_bbox: 0.0479  d2.loss_iou: 0.3481  d3.loss_cls: 0.4227  d3.loss_bbox: 0.0433  d3.loss_iou: 0.3339  d4.loss_cls: 0.4219  d4.loss_bbox: 0.0423  d4.loss_iou: 0.3290  enc_loss_cls: 0.4591  enc_loss_bbox: 0.0514  enc_loss_iou: 0.3809  dn_loss_cls: 0.0125  dn_loss_bbox: 0.0344  dn_loss_iou: 0.2675  d0.dn_loss_cls: 0.0444  d0.dn_loss_bbox: 0.0528  d0.dn_loss_iou: 0.3755  d1.dn_loss_cls: 0.0186  d1.dn_loss_bbox: 0.0378  d1.dn_loss_iou: 0.2862  d2.dn_loss_cls: 0.0154  d2.dn_loss_bbox: 0.0354  d2.dn_loss_iou: 0.2719  d3.dn_loss_cls: 0.0135  d3.dn_loss_bbox: 0.0345  d3.dn_loss_iou: 0.2673  d4.dn_loss_cls: 0.0129  d4.dn_loss_bbox: 0.0343  d4.dn_loss_iou: 0.2667  loss_num: 0.0014  d0.loss_num: 0.0015  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0014
2024/10/23 08:03:53 - mmengine - INFO - Epoch(train) [3][250/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:43:31  time: 1.1849  data_time: 0.0103  memory: 10671  grad_norm: 59.0999  loss: 8.1673  loss_cls: 0.4373  loss_bbox: 0.0452  loss_iou: 0.3422  d0.loss_cls: 0.4622  d0.loss_bbox: 0.0473  d0.loss_iou: 0.3675  d1.loss_cls: 0.4395  d1.loss_bbox: 0.0478  d1.loss_iou: 0.3520  d2.loss_cls: 0.4412  d2.loss_bbox: 0.0420  d2.loss_iou: 0.3420  d3.loss_cls: 0.4430  d3.loss_bbox: 0.0413  d3.loss_iou: 0.3402  d4.loss_cls: 0.4416  d4.loss_bbox: 0.0412  d4.loss_iou: 0.3399  enc_loss_cls: 0.4675  enc_loss_bbox: 0.0510  enc_loss_iou: 0.3878  dn_loss_cls: 0.0122  dn_loss_bbox: 0.0344  dn_loss_iou: 0.2917  d0.dn_loss_cls: 0.0473  d0.dn_loss_bbox: 0.0522  d0.dn_loss_iou: 0.4069  d1.dn_loss_cls: 0.0198  d1.dn_loss_bbox: 0.0377  d1.dn_loss_iou: 0.3134  d2.dn_loss_cls: 0.0149  d2.dn_loss_bbox: 0.0353  d2.dn_loss_iou: 0.2972  d3.dn_loss_cls: 0.0123  d3.dn_loss_bbox: 0.0345  d3.dn_loss_iou: 0.2917  d4.dn_loss_cls: 0.0122  d4.dn_loss_bbox: 0.0344  d4.dn_loss_iou: 0.2912  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0014  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/23 08:04:52 - mmengine - INFO - Epoch(train) [3][300/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:42:31  time: 1.1873  data_time: 0.0106  memory: 10485  grad_norm: 59.8775  loss: 9.1665  loss_cls: 0.4560  loss_bbox: 0.0567  loss_iou: 0.4321  d0.loss_cls: 0.4939  d0.loss_bbox: 0.0603  d0.loss_iou: 0.4541  d1.loss_cls: 0.4767  d1.loss_bbox: 0.0573  d1.loss_iou: 0.4387  d2.loss_cls: 0.4713  d2.loss_bbox: 0.0561  d2.loss_iou: 0.4334  d3.loss_cls: 0.4589  d3.loss_bbox: 0.0561  d3.loss_iou: 0.4348  d4.loss_cls: 0.4557  d4.loss_bbox: 0.0567  d4.loss_iou: 0.4318  enc_loss_cls: 0.5019  enc_loss_bbox: 0.0675  enc_loss_iou: 0.4770  dn_loss_cls: 0.0232  dn_loss_bbox: 0.0365  dn_loss_iou: 0.2978  d0.dn_loss_cls: 0.0631  d0.dn_loss_bbox: 0.0528  d0.dn_loss_iou: 0.3976  d1.dn_loss_cls: 0.0344  d1.dn_loss_bbox: 0.0389  d1.dn_loss_iou: 0.3131  d2.dn_loss_cls: 0.0236  d2.dn_loss_bbox: 0.0369  d2.dn_loss_iou: 0.2995  d3.dn_loss_cls: 0.0232  d3.dn_loss_bbox: 0.0365  d3.dn_loss_iou: 0.2972  d4.dn_loss_cls: 0.0231  d4.dn_loss_bbox: 0.0364  d4.dn_loss_iou: 0.2971  loss_num: 0.0014  d0.loss_num: 0.0015  d1.loss_num: 0.0013  d2.loss_num: 0.0014  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2024/10/23 08:05:52 - mmengine - INFO - Epoch(train) [3][350/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:41:32  time: 1.2020  data_time: 0.0121  memory: 10627  grad_norm: 59.6153  loss: 8.6250  loss_cls: 0.4493  loss_bbox: 0.0522  loss_iou: 0.3893  d0.loss_cls: 0.4760  d0.loss_bbox: 0.0519  d0.loss_iou: 0.3951  d1.loss_cls: 0.4589  d1.loss_bbox: 0.0513  d1.loss_iou: 0.3894  d2.loss_cls: 0.4589  d2.loss_bbox: 0.0494  d2.loss_iou: 0.3853  d3.loss_cls: 0.4590  d3.loss_bbox: 0.0503  d3.loss_iou: 0.3849  d4.loss_cls: 0.4536  d4.loss_bbox: 0.0494  d4.loss_iou: 0.3838  enc_loss_cls: 0.4777  enc_loss_bbox: 0.0547  enc_loss_iou: 0.4208  dn_loss_cls: 0.0467  dn_loss_bbox: 0.0321  dn_loss_iou: 0.2730  d0.dn_loss_cls: 0.0737  d0.dn_loss_bbox: 0.0464  d0.dn_loss_iou: 0.3644  d1.dn_loss_cls: 0.0506  d1.dn_loss_bbox: 0.0340  d1.dn_loss_iou: 0.2869  d2.dn_loss_cls: 0.0524  d2.dn_loss_bbox: 0.0322  d2.dn_loss_iou: 0.2743  d3.dn_loss_cls: 0.0498  d3.dn_loss_bbox: 0.0321  d3.dn_loss_iou: 0.2733  d4.dn_loss_cls: 0.0499  d4.dn_loss_bbox: 0.0321  d4.dn_loss_iou: 0.2728  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/23 08:06:19 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 08:06:52 - mmengine - INFO - Epoch(train) [3][400/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:40:32  time: 1.1888  data_time: 0.0107  memory: 10621  grad_norm: 61.4802  loss: 7.6369  loss_cls: 0.3731  loss_bbox: 0.0456  loss_iou: 0.3465  d0.loss_cls: 0.4080  d0.loss_bbox: 0.0487  d0.loss_iou: 0.3669  d1.loss_cls: 0.3805  d1.loss_bbox: 0.0459  d1.loss_iou: 0.3524  d2.loss_cls: 0.3758  d2.loss_bbox: 0.0456  d2.loss_iou: 0.3499  d3.loss_cls: 0.3723  d3.loss_bbox: 0.0454  d3.loss_iou: 0.3460  d4.loss_cls: 0.3716  d4.loss_bbox: 0.0455  d4.loss_iou: 0.3461  enc_loss_cls: 0.4108  enc_loss_bbox: 0.0536  enc_loss_iou: 0.3980  dn_loss_cls: 0.0106  dn_loss_bbox: 0.0342  dn_loss_iou: 0.2767  d0.dn_loss_cls: 0.0398  d0.dn_loss_bbox: 0.0513  d0.dn_loss_iou: 0.3783  d1.dn_loss_cls: 0.0158  d1.dn_loss_bbox: 0.0366  d1.dn_loss_iou: 0.2931  d2.dn_loss_cls: 0.0111  d2.dn_loss_bbox: 0.0346  d2.dn_loss_iou: 0.2783  d3.dn_loss_cls: 0.0106  d3.dn_loss_bbox: 0.0342  d3.dn_loss_iou: 0.2763  d4.dn_loss_cls: 0.0104  d4.dn_loss_bbox: 0.0341  d4.dn_loss_iou: 0.2761  loss_num: 0.0011  d0.loss_num: 0.0011  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/23 08:07:51 - mmengine - INFO - Epoch(train) [3][450/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:39:33  time: 1.1891  data_time: 0.0112  memory: 10618  grad_norm: 64.4813  loss: 7.4966  loss_cls: 0.3781  loss_bbox: 0.0456  loss_iou: 0.3266  d0.loss_cls: 0.4131  d0.loss_bbox: 0.0491  d0.loss_iou: 0.3407  d1.loss_cls: 0.3859  d1.loss_bbox: 0.0492  d1.loss_iou: 0.3373  d2.loss_cls: 0.3820  d2.loss_bbox: 0.0470  d2.loss_iou: 0.3314  d3.loss_cls: 0.3827  d3.loss_bbox: 0.0462  d3.loss_iou: 0.3278  d4.loss_cls: 0.3805  d4.loss_bbox: 0.0455  d4.loss_iou: 0.3260  enc_loss_cls: 0.4235  enc_loss_bbox: 0.0518  enc_loss_iou: 0.3572  dn_loss_cls: 0.0082  dn_loss_bbox: 0.0348  dn_loss_iou: 0.2702  d0.dn_loss_cls: 0.0394  d0.dn_loss_bbox: 0.0521  d0.dn_loss_iou: 0.3751  d1.dn_loss_cls: 0.0133  d1.dn_loss_bbox: 0.0373  d1.dn_loss_iou: 0.2881  d2.dn_loss_cls: 0.0097  d2.dn_loss_bbox: 0.0352  d2.dn_loss_iou: 0.2729  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0347  d3.dn_loss_iou: 0.2697  d4.dn_loss_cls: 0.0084  d4.dn_loss_bbox: 0.0348  d4.dn_loss_iou: 0.2696  loss_num: 0.0012  d0.loss_num: 0.0012  d1.loss_num: 0.0011  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 08:08:51 - mmengine - INFO - Epoch(train) [3][500/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:38:33  time: 1.1935  data_time: 0.0107  memory: 10630  grad_norm: 60.5193  loss: 8.6168  loss_cls: 0.4362  loss_bbox: 0.0573  loss_iou: 0.4153  d0.loss_cls: 0.4605  d0.loss_bbox: 0.0661  d0.loss_iou: 0.4291  d1.loss_cls: 0.4461  d1.loss_bbox: 0.0598  d1.loss_iou: 0.4196  d2.loss_cls: 0.4482  d2.loss_bbox: 0.0552  d2.loss_iou: 0.4043  d3.loss_cls: 0.4399  d3.loss_bbox: 0.0570  d3.loss_iou: 0.4134  d4.loss_cls: 0.4440  d4.loss_bbox: 0.0549  d4.loss_iou: 0.4045  enc_loss_cls: 0.4548  enc_loss_bbox: 0.0769  enc_loss_iou: 0.4591  dn_loss_cls: 0.0091  dn_loss_bbox: 0.0348  dn_loss_iou: 0.2744  d0.dn_loss_cls: 0.0468  d0.dn_loss_bbox: 0.0530  d0.dn_loss_iou: 0.3821  d1.dn_loss_cls: 0.0169  d1.dn_loss_bbox: 0.0376  d1.dn_loss_iou: 0.2920  d2.dn_loss_cls: 0.0106  d2.dn_loss_bbox: 0.0353  d2.dn_loss_iou: 0.2775  d3.dn_loss_cls: 0.0095  d3.dn_loss_bbox: 0.0348  d3.dn_loss_iou: 0.2750  d4.dn_loss_cls: 0.0092  d4.dn_loss_bbox: 0.0347  d4.dn_loss_iou: 0.2739  loss_num: 0.0012  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 08:09:50 - mmengine - INFO - Epoch(train) [3][550/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:37:33  time: 1.1828  data_time: 0.0109  memory: 10615  grad_norm: 65.4650  loss: 7.7972  loss_cls: 0.4009  loss_bbox: 0.0471  loss_iou: 0.3608  d0.loss_cls: 0.4242  d0.loss_bbox: 0.0507  d0.loss_iou: 0.3802  d1.loss_cls: 0.4026  d1.loss_bbox: 0.0522  d1.loss_iou: 0.3732  d2.loss_cls: 0.4069  d2.loss_bbox: 0.0474  d2.loss_iou: 0.3652  d3.loss_cls: 0.3982  d3.loss_bbox: 0.0466  d3.loss_iou: 0.3603  d4.loss_cls: 0.4009  d4.loss_bbox: 0.0470  d4.loss_iou: 0.3605  enc_loss_cls: 0.4292  enc_loss_bbox: 0.0635  enc_loss_iou: 0.4283  dn_loss_cls: 0.0101  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2554  d0.dn_loss_cls: 0.0344  d0.dn_loss_bbox: 0.0477  d0.dn_loss_iou: 0.3523  d1.dn_loss_cls: 0.0151  d1.dn_loss_bbox: 0.0338  d1.dn_loss_iou: 0.2709  d2.dn_loss_cls: 0.0118  d2.dn_loss_bbox: 0.0318  d2.dn_loss_iou: 0.2573  d3.dn_loss_cls: 0.0103  d3.dn_loss_bbox: 0.0316  d3.dn_loss_iou: 0.2548  d4.dn_loss_cls: 0.0102  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2548  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:10:49 - mmengine - INFO - Epoch(train) [3][600/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:36:33  time: 1.1847  data_time: 0.0110  memory: 10621  grad_norm: 69.5683  loss: 8.0691  loss_cls: 0.4060  loss_bbox: 0.0567  loss_iou: 0.3744  d0.loss_cls: 0.4345  d0.loss_bbox: 0.0568  d0.loss_iou: 0.3853  d1.loss_cls: 0.4121  d1.loss_bbox: 0.0573  d1.loss_iou: 0.3811  d2.loss_cls: 0.4120  d2.loss_bbox: 0.0558  d2.loss_iou: 0.3699  d3.loss_cls: 0.4130  d3.loss_bbox: 0.0562  d3.loss_iou: 0.3712  d4.loss_cls: 0.4018  d4.loss_bbox: 0.0573  d4.loss_iou: 0.3767  enc_loss_cls: 0.4391  enc_loss_bbox: 0.0684  enc_loss_iou: 0.4248  dn_loss_cls: 0.0127  dn_loss_bbox: 0.0340  dn_loss_iou: 0.2681  d0.dn_loss_cls: 0.0370  d0.dn_loss_bbox: 0.0496  d0.dn_loss_iou: 0.3655  d1.dn_loss_cls: 0.0176  d1.dn_loss_bbox: 0.0364  d1.dn_loss_iou: 0.2828  d2.dn_loss_cls: 0.0142  d2.dn_loss_bbox: 0.0344  d2.dn_loss_iou: 0.2707  d3.dn_loss_cls: 0.0129  d3.dn_loss_bbox: 0.0341  d3.dn_loss_iou: 0.2677  d4.dn_loss_cls: 0.0126  d4.dn_loss_bbox: 0.0339  d4.dn_loss_iou: 0.2675  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/23 08:11:49 - mmengine - INFO - Epoch(train) [3][650/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:35:34  time: 1.1988  data_time: 0.0116  memory: 10352  grad_norm: 59.6032  loss: 8.5840  loss_cls: 0.4133  loss_bbox: 0.0599  loss_iou: 0.4149  d0.loss_cls: 0.4420  d0.loss_bbox: 0.0626  d0.loss_iou: 0.4283  d1.loss_cls: 0.4346  d1.loss_bbox: 0.0595  d1.loss_iou: 0.4141  d2.loss_cls: 0.4137  d2.loss_bbox: 0.0603  d2.loss_iou: 0.4143  d3.loss_cls: 0.4170  d3.loss_bbox: 0.0595  d3.loss_iou: 0.4097  d4.loss_cls: 0.4153  d4.loss_bbox: 0.0596  d4.loss_iou: 0.4117  enc_loss_cls: 0.4526  enc_loss_bbox: 0.0684  enc_loss_iou: 0.4520  dn_loss_cls: 0.0117  dn_loss_bbox: 0.0373  dn_loss_iou: 0.2921  d0.dn_loss_cls: 0.0395  d0.dn_loss_bbox: 0.0539  d0.dn_loss_iou: 0.3885  d1.dn_loss_cls: 0.0165  d1.dn_loss_bbox: 0.0397  d1.dn_loss_iou: 0.3076  d2.dn_loss_cls: 0.0129  d2.dn_loss_bbox: 0.0377  d2.dn_loss_iou: 0.2952  d3.dn_loss_cls: 0.0121  d3.dn_loss_bbox: 0.0373  d3.dn_loss_iou: 0.2920  d4.dn_loss_cls: 0.0118  d4.dn_loss_bbox: 0.0373  d4.dn_loss_iou: 0.2917  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:12:48 - mmengine - INFO - Epoch(train) [3][700/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:34:34  time: 1.1807  data_time: 0.0108  memory: 10618  grad_norm: 65.3135  loss: 7.5415  loss_cls: 0.3688  loss_bbox: 0.0404  loss_iou: 0.3365  d0.loss_cls: 0.3927  d0.loss_bbox: 0.0448  d0.loss_iou: 0.3541  d1.loss_cls: 0.3912  d1.loss_bbox: 0.0408  d1.loss_iou: 0.3385  d2.loss_cls: 0.3819  d2.loss_bbox: 0.0420  d2.loss_iou: 0.3401  d3.loss_cls: 0.3679  d3.loss_bbox: 0.0429  d3.loss_iou: 0.3434  d4.loss_cls: 0.3660  d4.loss_bbox: 0.0413  d4.loss_iou: 0.3407  enc_loss_cls: 0.4043  enc_loss_bbox: 0.0452  enc_loss_iou: 0.3697  dn_loss_cls: 0.0143  dn_loss_bbox: 0.0333  dn_loss_iou: 0.2784  d0.dn_loss_cls: 0.0449  d0.dn_loss_bbox: 0.0504  d0.dn_loss_iou: 0.3857  d1.dn_loss_cls: 0.0190  d1.dn_loss_bbox: 0.0360  d1.dn_loss_iou: 0.2977  d2.dn_loss_cls: 0.0156  d2.dn_loss_bbox: 0.0339  d2.dn_loss_iou: 0.2817  d3.dn_loss_cls: 0.0148  d3.dn_loss_bbox: 0.0334  d3.dn_loss_iou: 0.2783  d4.dn_loss_cls: 0.0140  d4.dn_loss_bbox: 0.0332  d4.dn_loss_iou: 0.2777  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0009  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0009
2024/10/23 08:13:48 - mmengine - INFO - Epoch(train) [3][750/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:33:34  time: 1.1834  data_time: 0.0108  memory: 10618  grad_norm: 58.2456  loss: 9.0665  loss_cls: 0.4650  loss_bbox: 0.0501  loss_iou: 0.4394  d0.loss_cls: 0.5111  d0.loss_bbox: 0.0549  d0.loss_iou: 0.4589  d1.loss_cls: 0.4846  d1.loss_bbox: 0.0524  d1.loss_iou: 0.4479  d2.loss_cls: 0.4746  d2.loss_bbox: 0.0512  d2.loss_iou: 0.4439  d3.loss_cls: 0.4706  d3.loss_bbox: 0.0506  d3.loss_iou: 0.4412  d4.loss_cls: 0.4639  d4.loss_bbox: 0.0502  d4.loss_iou: 0.4378  enc_loss_cls: 0.5098  enc_loss_bbox: 0.0580  enc_loss_iou: 0.4786  dn_loss_cls: 0.0127  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2852  d0.dn_loss_cls: 0.0458  d0.dn_loss_bbox: 0.0491  d0.dn_loss_iou: 0.3859  d1.dn_loss_cls: 0.0187  d1.dn_loss_bbox: 0.0348  d1.dn_loss_iou: 0.3055  d2.dn_loss_cls: 0.0141  d2.dn_loss_bbox: 0.0324  d2.dn_loss_iou: 0.2890  d3.dn_loss_cls: 0.0131  d3.dn_loss_bbox: 0.0318  d3.dn_loss_iou: 0.2858  d4.dn_loss_cls: 0.0127  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2848  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0011
2024/10/23 08:14:48 - mmengine - INFO - Epoch(train) [3][800/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:32:35  time: 1.2028  data_time: 0.0131  memory: 10618  grad_norm: 63.1184  loss: 7.6186  loss_cls: 0.3900  loss_bbox: 0.0435  loss_iou: 0.3422  d0.loss_cls: 0.4061  d0.loss_bbox: 0.0458  d0.loss_iou: 0.3479  d1.loss_cls: 0.4014  d1.loss_bbox: 0.0446  d1.loss_iou: 0.3439  d2.loss_cls: 0.3917  d2.loss_bbox: 0.0434  d2.loss_iou: 0.3410  d3.loss_cls: 0.3922  d3.loss_bbox: 0.0426  d3.loss_iou: 0.3372  d4.loss_cls: 0.3917  d4.loss_bbox: 0.0433  d4.loss_iou: 0.3398  enc_loss_cls: 0.4152  enc_loss_bbox: 0.0522  enc_loss_iou: 0.3738  dn_loss_cls: 0.0119  dn_loss_bbox: 0.0331  dn_loss_iou: 0.2749  d0.dn_loss_cls: 0.0404  d0.dn_loss_bbox: 0.0483  d0.dn_loss_iou: 0.3736  d1.dn_loss_cls: 0.0155  d1.dn_loss_bbox: 0.0352  d1.dn_loss_iou: 0.2893  d2.dn_loss_cls: 0.0122  d2.dn_loss_bbox: 0.0334  d2.dn_loss_iou: 0.2765  d3.dn_loss_cls: 0.0115  d3.dn_loss_bbox: 0.0331  d3.dn_loss_iou: 0.2741  d4.dn_loss_cls: 0.0119  d4.dn_loss_bbox: 0.0330  d4.dn_loss_iou: 0.2741  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/23 08:15:04 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 08:15:04 - mmengine - INFO - Saving checkpoint at 3 epochs
2024/10/23 08:15:17 - mmengine - INFO - Epoch(val) [3][ 50/858]    eta: 0:01:21  time: 0.1008  data_time: 0.0026  memory: 10219  
2024/10/23 08:15:22 - mmengine - INFO - Epoch(val) [3][100/858]    eta: 0:01:15  time: 0.0980  data_time: 0.0019  memory: 4267  
2024/10/23 08:15:26 - mmengine - INFO - Epoch(val) [3][150/858]    eta: 0:01:10  time: 0.0981  data_time: 0.0020  memory: 4267  
2024/10/23 08:15:31 - mmengine - INFO - Epoch(val) [3][200/858]    eta: 0:01:05  time: 0.0983  data_time: 0.0020  memory: 4267  
2024/10/23 08:15:36 - mmengine - INFO - Epoch(val) [3][250/858]    eta: 0:00:59  time: 0.0978  data_time: 0.0020  memory: 4267  
2024/10/23 08:15:41 - mmengine - INFO - Epoch(val) [3][300/858]    eta: 0:00:54  time: 0.0976  data_time: 0.0019  memory: 4267  
2024/10/23 08:15:46 - mmengine - INFO - Epoch(val) [3][350/858]    eta: 0:00:49  time: 0.0976  data_time: 0.0020  memory: 4267  
2024/10/23 08:15:51 - mmengine - INFO - Epoch(val) [3][400/858]    eta: 0:00:45  time: 0.0979  data_time: 0.0019  memory: 4267  
2024/10/23 08:15:56 - mmengine - INFO - Epoch(val) [3][450/858]    eta: 0:00:40  time: 0.0977  data_time: 0.0019  memory: 4267  
2024/10/23 08:16:01 - mmengine - INFO - Epoch(val) [3][500/858]    eta: 0:00:35  time: 0.0977  data_time: 0.0019  memory: 4267  
2024/10/23 08:16:06 - mmengine - INFO - Epoch(val) [3][550/858]    eta: 0:00:30  time: 0.0977  data_time: 0.0019  memory: 4267  
2024/10/23 08:16:10 - mmengine - INFO - Epoch(val) [3][600/858]    eta: 0:00:25  time: 0.0978  data_time: 0.0019  memory: 4267  
2024/10/23 08:16:15 - mmengine - INFO - Epoch(val) [3][650/858]    eta: 0:00:20  time: 0.0978  data_time: 0.0019  memory: 4267  
2024/10/23 08:16:20 - mmengine - INFO - Epoch(val) [3][700/858]    eta: 0:00:15  time: 0.0982  data_time: 0.0019  memory: 4267  
2024/10/23 08:16:25 - mmengine - INFO - Epoch(val) [3][750/858]    eta: 0:00:10  time: 0.0980  data_time: 0.0018  memory: 4267  
2024/10/23 08:16:30 - mmengine - INFO - Epoch(val) [3][800/858]    eta: 0:00:05  time: 0.0987  data_time: 0.0019  memory: 4267  
2024/10/23 08:16:35 - mmengine - INFO - Epoch(val) [3][850/858]    eta: 0:00:00  time: 0.0975  data_time: 0.0018  memory: 4267  
2024/10/23 08:16:37 - mmengine - INFO - {'instance_F1_score': 0.07767770987455773, 'instance_acc': 0.05245766212308963, 'image_F1_score': 0.13795061027533354, 'image_acc': 0.1150932400932401}
2024/10/23 08:16:37 - mmengine - INFO - Epoch(val) [3][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.0777  grefcoco_val/refdrone/instance_acc: 0.0525  grefcoco_val/refdrone/image_F1_score: 0.1380  grefcoco_val/refdrone/image_acc: 0.1151  data_time: 0.0019  time: 0.0980
2024/10/23 08:17:36 - mmengine - INFO - Epoch(train) [4][ 50/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:31:19  time: 1.1833  data_time: 0.0096  memory: 10621  grad_norm: 55.0761  loss: 7.4782  loss_cls: 0.3755  loss_bbox: 0.0516  loss_iou: 0.3699  d0.loss_cls: 0.4086  d0.loss_bbox: 0.0538  d0.loss_iou: 0.3788  d1.loss_cls: 0.3803  d1.loss_bbox: 0.0537  d1.loss_iou: 0.3775  d2.loss_cls: 0.3825  d2.loss_bbox: 0.0515  d2.loss_iou: 0.3693  d3.loss_cls: 0.3757  d3.loss_bbox: 0.0518  d3.loss_iou: 0.3716  d4.loss_cls: 0.3777  d4.loss_bbox: 0.0509  d4.loss_iou: 0.3662  enc_loss_cls: 0.4260  enc_loss_bbox: 0.0525  enc_loss_iou: 0.3888  dn_loss_cls: 0.0082  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2312  d0.dn_loss_cls: 0.0343  d0.dn_loss_bbox: 0.0430  d0.dn_loss_iou: 0.3081  d1.dn_loss_cls: 0.0111  d1.dn_loss_bbox: 0.0323  d1.dn_loss_iou: 0.2451  d2.dn_loss_cls: 0.0090  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2344  d3.dn_loss_cls: 0.0083  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2320  d4.dn_loss_cls: 0.0082  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2312  loss_num: 0.0011  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0010
2024/10/23 08:18:36 - mmengine - INFO - Epoch(train) [4][100/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:30:19  time: 1.1868  data_time: 0.0104  memory: 10624  grad_norm: 45.8371  loss: 7.5328  loss_cls: 0.3555  loss_bbox: 0.0480  loss_iou: 0.3595  d0.loss_cls: 0.3840  d0.loss_bbox: 0.0503  d0.loss_iou: 0.3751  d1.loss_cls: 0.3664  d1.loss_bbox: 0.0498  d1.loss_iou: 0.3712  d2.loss_cls: 0.3688  d2.loss_bbox: 0.0486  d2.loss_iou: 0.3596  d3.loss_cls: 0.3630  d3.loss_bbox: 0.0481  d3.loss_iou: 0.3585  d4.loss_cls: 0.3552  d4.loss_bbox: 0.0487  d4.loss_iou: 0.3625  enc_loss_cls: 0.3946  enc_loss_bbox: 0.0543  enc_loss_iou: 0.3980  dn_loss_cls: 0.0081  dn_loss_bbox: 0.0345  dn_loss_iou: 0.2665  d0.dn_loss_cls: 0.0362  d0.dn_loss_bbox: 0.0471  d0.dn_loss_iou: 0.3481  d1.dn_loss_cls: 0.0139  d1.dn_loss_bbox: 0.0365  d1.dn_loss_iou: 0.2814  d2.dn_loss_cls: 0.0102  d2.dn_loss_bbox: 0.0351  d2.dn_loss_iou: 0.2699  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0347  d3.dn_loss_iou: 0.2674  d4.dn_loss_cls: 0.0082  d4.dn_loss_bbox: 0.0345  d4.dn_loss_iou: 0.2665  loss_num: 0.0009  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:19:35 - mmengine - INFO - Epoch(train) [4][150/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:29:19  time: 1.1928  data_time: 0.0113  memory: 10618  grad_norm: 51.2350  loss: 7.1394  loss_cls: 0.3385  loss_bbox: 0.0453  loss_iou: 0.3473  d0.loss_cls: 0.3586  d0.loss_bbox: 0.0450  d0.loss_iou: 0.3571  d1.loss_cls: 0.3415  d1.loss_bbox: 0.0460  d1.loss_iou: 0.3532  d2.loss_cls: 0.3422  d2.loss_bbox: 0.0446  d2.loss_iou: 0.3485  d3.loss_cls: 0.3429  d3.loss_bbox: 0.0429  d3.loss_iou: 0.3431  d4.loss_cls: 0.3410  d4.loss_bbox: 0.0440  d4.loss_iou: 0.3441  enc_loss_cls: 0.3633  enc_loss_bbox: 0.0474  enc_loss_iou: 0.3698  dn_loss_cls: 0.0080  dn_loss_bbox: 0.0311  dn_loss_iou: 0.2571  d0.dn_loss_cls: 0.0363  d0.dn_loss_bbox: 0.0434  d0.dn_loss_iou: 0.3380  d1.dn_loss_cls: 0.0131  d1.dn_loss_bbox: 0.0330  d1.dn_loss_iou: 0.2723  d2.dn_loss_cls: 0.0102  d2.dn_loss_bbox: 0.0313  d2.dn_loss_iou: 0.2596  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0311  d3.dn_loss_iou: 0.2575  d4.dn_loss_cls: 0.0084  d4.dn_loss_bbox: 0.0311  d4.dn_loss_iou: 0.2570  loss_num: 0.0009  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0009
2024/10/23 08:20:35 - mmengine - INFO - Epoch(train) [4][200/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:28:20  time: 1.1954  data_time: 0.0109  memory: 10627  grad_norm: 47.3657  loss: 7.6624  loss_cls: 0.3308  loss_bbox: 0.0502  loss_iou: 0.3978  d0.loss_cls: 0.3646  d0.loss_bbox: 0.0562  d0.loss_iou: 0.4277  d1.loss_cls: 0.3419  d1.loss_bbox: 0.0499  d1.loss_iou: 0.4116  d2.loss_cls: 0.3349  d2.loss_bbox: 0.0476  d2.loss_iou: 0.3978  d3.loss_cls: 0.3330  d3.loss_bbox: 0.0477  d3.loss_iou: 0.3971  d4.loss_cls: 0.3287  d4.loss_bbox: 0.0505  d4.loss_iou: 0.4022  enc_loss_cls: 0.3808  enc_loss_bbox: 0.0548  enc_loss_iou: 0.4320  dn_loss_cls: 0.0074  dn_loss_bbox: 0.0337  dn_loss_iou: 0.2704  d0.dn_loss_cls: 0.0358  d0.dn_loss_bbox: 0.0455  d0.dn_loss_iou: 0.3547  d1.dn_loss_cls: 0.0124  d1.dn_loss_bbox: 0.0354  d1.dn_loss_iou: 0.2832  d2.dn_loss_cls: 0.0088  d2.dn_loss_bbox: 0.0341  d2.dn_loss_iou: 0.2734  d3.dn_loss_cls: 0.0079  d3.dn_loss_bbox: 0.0338  d3.dn_loss_iou: 0.2711  d4.dn_loss_cls: 0.0076  d4.dn_loss_bbox: 0.0337  d4.dn_loss_iou: 0.2703  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:21:34 - mmengine - INFO - Epoch(train) [4][250/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:27:20  time: 1.1834  data_time: 0.0104  memory: 10630  grad_norm: 52.9129  loss: 7.1621  loss_cls: 0.3333  loss_bbox: 0.0462  loss_iou: 0.3531  d0.loss_cls: 0.3638  d0.loss_bbox: 0.0483  d0.loss_iou: 0.3716  d1.loss_cls: 0.3445  d1.loss_bbox: 0.0471  d1.loss_iou: 0.3610  d2.loss_cls: 0.3357  d2.loss_bbox: 0.0464  d2.loss_iou: 0.3559  d3.loss_cls: 0.3338  d3.loss_bbox: 0.0464  d3.loss_iou: 0.3549  d4.loss_cls: 0.3319  d4.loss_bbox: 0.0480  d4.loss_iou: 0.3580  enc_loss_cls: 0.3647  enc_loss_bbox: 0.0522  enc_loss_iou: 0.3927  dn_loss_cls: 0.0088  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2482  d0.dn_loss_cls: 0.0360  d0.dn_loss_bbox: 0.0433  d0.dn_loss_iou: 0.3220  d1.dn_loss_cls: 0.0124  d1.dn_loss_bbox: 0.0334  d1.dn_loss_iou: 0.2602  d2.dn_loss_cls: 0.0098  d2.dn_loss_bbox: 0.0321  d2.dn_loss_iou: 0.2520  d3.dn_loss_cls: 0.0090  d3.dn_loss_bbox: 0.0317  d3.dn_loss_iou: 0.2486  d4.dn_loss_cls: 0.0090  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2481  loss_num: 0.0008  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/23 08:22:34 - mmengine - INFO - Epoch(train) [4][300/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:26:21  time: 1.1881  data_time: 0.0104  memory: 10615  grad_norm: 49.2192  loss: 6.8910  loss_cls: 0.3223  loss_bbox: 0.0441  loss_iou: 0.3512  d0.loss_cls: 0.3475  d0.loss_bbox: 0.0464  d0.loss_iou: 0.3558  d1.loss_cls: 0.3360  d1.loss_bbox: 0.0424  d1.loss_iou: 0.3444  d2.loss_cls: 0.3311  d2.loss_bbox: 0.0420  d2.loss_iou: 0.3470  d3.loss_cls: 0.3318  d3.loss_bbox: 0.0421  d3.loss_iou: 0.3393  d4.loss_cls: 0.3259  d4.loss_bbox: 0.0441  d4.loss_iou: 0.3463  enc_loss_cls: 0.3673  enc_loss_bbox: 0.0485  enc_loss_iou: 0.3663  dn_loss_cls: 0.0097  dn_loss_bbox: 0.0293  dn_loss_iou: 0.2356  d0.dn_loss_cls: 0.0288  d0.dn_loss_bbox: 0.0390  d0.dn_loss_iou: 0.3031  d1.dn_loss_cls: 0.0122  d1.dn_loss_bbox: 0.0307  d1.dn_loss_iou: 0.2473  d2.dn_loss_cls: 0.0105  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2379  d3.dn_loss_cls: 0.0099  d3.dn_loss_bbox: 0.0294  d3.dn_loss_iou: 0.2360  d4.dn_loss_cls: 0.0098  d4.dn_loss_bbox: 0.0293  d4.dn_loss_iou: 0.2355  loss_num: 0.0009  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:23:33 - mmengine - INFO - Epoch(train) [4][350/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:25:21  time: 1.1819  data_time: 0.0105  memory: 10618  grad_norm: 48.3762  loss: 7.1225  loss_cls: 0.3463  loss_bbox: 0.0424  loss_iou: 0.3543  d0.loss_cls: 0.3662  d0.loss_bbox: 0.0443  d0.loss_iou: 0.3636  d1.loss_cls: 0.3414  d1.loss_bbox: 0.0445  d1.loss_iou: 0.3698  d2.loss_cls: 0.3439  d2.loss_bbox: 0.0423  d2.loss_iou: 0.3584  d3.loss_cls: 0.3442  d3.loss_bbox: 0.0421  d3.loss_iou: 0.3535  d4.loss_cls: 0.3454  d4.loss_bbox: 0.0423  d4.loss_iou: 0.3555  enc_loss_cls: 0.3727  enc_loss_bbox: 0.0481  enc_loss_iou: 0.3826  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2445  d0.dn_loss_cls: 0.0323  d0.dn_loss_bbox: 0.0411  d0.dn_loss_iou: 0.3161  d1.dn_loss_cls: 0.0105  d1.dn_loss_bbox: 0.0318  d1.dn_loss_iou: 0.2554  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2461  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0304  d3.dn_loss_iou: 0.2447  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2444  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0008
2024/10/23 08:24:33 - mmengine - INFO - Epoch(train) [4][400/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:24:22  time: 1.1951  data_time: 0.0106  memory: 10630  grad_norm: 46.0964  loss: 7.2330  loss_cls: 0.3334  loss_bbox: 0.0408  loss_iou: 0.3554  d0.loss_cls: 0.3633  d0.loss_bbox: 0.0447  d0.loss_iou: 0.3797  d1.loss_cls: 0.3473  d1.loss_bbox: 0.0429  d1.loss_iou: 0.3673  d2.loss_cls: 0.3354  d2.loss_bbox: 0.0424  d2.loss_iou: 0.3620  d3.loss_cls: 0.3330  d3.loss_bbox: 0.0408  d3.loss_iou: 0.3553  d4.loss_cls: 0.3336  d4.loss_bbox: 0.0408  d4.loss_iou: 0.3554  enc_loss_cls: 0.3873  enc_loss_bbox: 0.0450  enc_loss_iou: 0.3872  dn_loss_cls: 0.0145  dn_loss_bbox: 0.0309  dn_loss_iou: 0.2544  d0.dn_loss_cls: 0.0421  d0.dn_loss_bbox: 0.0416  d0.dn_loss_iou: 0.3273  d1.dn_loss_cls: 0.0205  d1.dn_loss_bbox: 0.0324  d1.dn_loss_iou: 0.2660  d2.dn_loss_cls: 0.0165  d2.dn_loss_bbox: 0.0312  d2.dn_loss_iou: 0.2578  d3.dn_loss_cls: 0.0146  d3.dn_loss_bbox: 0.0310  d3.dn_loss_iou: 0.2548  d4.dn_loss_cls: 0.0143  d4.dn_loss_bbox: 0.0309  d4.dn_loss_iou: 0.2543  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/23 08:25:32 - mmengine - INFO - Epoch(train) [4][450/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:23:22  time: 1.1910  data_time: 0.0110  memory: 10621  grad_norm: 52.2474  loss: 6.7205  loss_cls: 0.3223  loss_bbox: 0.0443  loss_iou: 0.3055  d0.loss_cls: 0.3444  d0.loss_bbox: 0.0472  d0.loss_iou: 0.3255  d1.loss_cls: 0.3250  d1.loss_bbox: 0.0473  d1.loss_iou: 0.3183  d2.loss_cls: 0.3226  d2.loss_bbox: 0.0459  d2.loss_iou: 0.3071  d3.loss_cls: 0.3191  d3.loss_bbox: 0.0455  d3.loss_iou: 0.3070  d4.loss_cls: 0.3189  d4.loss_bbox: 0.0457  d4.loss_iou: 0.3086  enc_loss_cls: 0.3534  enc_loss_bbox: 0.0514  enc_loss_iou: 0.3454  dn_loss_cls: 0.0096  dn_loss_bbox: 0.0335  dn_loss_iou: 0.2441  d0.dn_loss_cls: 0.0371  d0.dn_loss_bbox: 0.0458  d0.dn_loss_iou: 0.3185  d1.dn_loss_cls: 0.0146  d1.dn_loss_bbox: 0.0353  d1.dn_loss_iou: 0.2575  d2.dn_loss_cls: 0.0109  d2.dn_loss_bbox: 0.0341  d2.dn_loss_iou: 0.2486  d3.dn_loss_cls: 0.0100  d3.dn_loss_bbox: 0.0336  d3.dn_loss_iou: 0.2449  d4.dn_loss_cls: 0.0091  d4.dn_loss_bbox: 0.0335  d4.dn_loss_iou: 0.2441  loss_num: 0.0009  d0.loss_num: 0.0010  d1.loss_num: 0.0008  d2.loss_num: 0.0010  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:26:32 - mmengine - INFO - Epoch(train) [4][500/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:22:23  time: 1.1925  data_time: 0.0108  memory: 10627  grad_norm: 51.1555  loss: 7.4745  loss_cls: 0.3587  loss_bbox: 0.0451  loss_iou: 0.3653  d0.loss_cls: 0.3806  d0.loss_bbox: 0.0468  d0.loss_iou: 0.3792  d1.loss_cls: 0.3705  d1.loss_bbox: 0.0457  d1.loss_iou: 0.3677  d2.loss_cls: 0.3629  d2.loss_bbox: 0.0446  d2.loss_iou: 0.3684  d3.loss_cls: 0.3606  d3.loss_bbox: 0.0437  d3.loss_iou: 0.3653  d4.loss_cls: 0.3629  d4.loss_bbox: 0.0435  d4.loss_iou: 0.3623  enc_loss_cls: 0.3941  enc_loss_bbox: 0.0486  enc_loss_iou: 0.3886  dn_loss_cls: 0.0111  dn_loss_bbox: 0.0311  dn_loss_iou: 0.2578  d0.dn_loss_cls: 0.0430  d0.dn_loss_bbox: 0.0440  d0.dn_loss_iou: 0.3407  d1.dn_loss_cls: 0.0195  d1.dn_loss_bbox: 0.0333  d1.dn_loss_iou: 0.2739  d2.dn_loss_cls: 0.0130  d2.dn_loss_bbox: 0.0317  d2.dn_loss_iou: 0.2617  d3.dn_loss_cls: 0.0115  d3.dn_loss_bbox: 0.0312  d3.dn_loss_iou: 0.2586  d4.dn_loss_cls: 0.0111  d4.dn_loss_bbox: 0.0311  d4.dn_loss_iou: 0.2578  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/23 08:27:31 - mmengine - INFO - Epoch(train) [4][550/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:21:23  time: 1.1879  data_time: 0.0108  memory: 10621  grad_norm: 52.0100  loss: 7.4778  loss_cls: 0.3615  loss_bbox: 0.0398  loss_iou: 0.3639  d0.loss_cls: 0.4027  d0.loss_bbox: 0.0411  d0.loss_iou: 0.3750  d1.loss_cls: 0.3815  d1.loss_bbox: 0.0401  d1.loss_iou: 0.3681  d2.loss_cls: 0.3742  d2.loss_bbox: 0.0399  d2.loss_iou: 0.3650  d3.loss_cls: 0.3691  d3.loss_bbox: 0.0396  d3.loss_iou: 0.3630  d4.loss_cls: 0.3639  d4.loss_bbox: 0.0398  d4.loss_iou: 0.3654  enc_loss_cls: 0.4249  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3921  dn_loss_cls: 0.0089  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2600  d0.dn_loss_cls: 0.0310  d0.dn_loss_bbox: 0.0396  d0.dn_loss_iou: 0.3334  d1.dn_loss_cls: 0.0131  d1.dn_loss_bbox: 0.0307  d1.dn_loss_iou: 0.2724  d2.dn_loss_cls: 0.0095  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2631  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.0291  d3.dn_loss_iou: 0.2607  d4.dn_loss_cls: 0.0089  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2601  loss_num: 0.0011  d0.loss_num: 0.0013  d1.loss_num: 0.0011  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/23 08:27:41 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 08:28:31 - mmengine - INFO - Epoch(train) [4][600/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:20:23  time: 1.1873  data_time: 0.0107  memory: 10627  grad_norm: 47.9401  loss: 7.2593  loss_cls: 0.3027  loss_bbox: 0.0523  loss_iou: 0.3737  d0.loss_cls: 0.3377  d0.loss_bbox: 0.0541  d0.loss_iou: 0.3867  d1.loss_cls: 0.3141  d1.loss_bbox: 0.0530  d1.loss_iou: 0.3782  d2.loss_cls: 0.3112  d2.loss_bbox: 0.0532  d2.loss_iou: 0.3784  d3.loss_cls: 0.3054  d3.loss_bbox: 0.0524  d3.loss_iou: 0.3729  d4.loss_cls: 0.3038  d4.loss_bbox: 0.0523  d4.loss_iou: 0.3740  enc_loss_cls: 0.3374  enc_loss_bbox: 0.0588  enc_loss_iou: 0.4085  dn_loss_cls: 0.0098  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2646  d0.dn_loss_cls: 0.0415  d0.dn_loss_bbox: 0.0435  d0.dn_loss_iou: 0.3471  d1.dn_loss_cls: 0.0145  d1.dn_loss_bbox: 0.0342  d1.dn_loss_iou: 0.2820  d2.dn_loss_cls: 0.0110  d2.dn_loss_bbox: 0.0322  d2.dn_loss_iou: 0.2683  d3.dn_loss_cls: 0.0099  d3.dn_loss_bbox: 0.0317  d3.dn_loss_iou: 0.2652  d4.dn_loss_cls: 0.0098  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2646  loss_num: 0.0009  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0010  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:29:30 - mmengine - INFO - Epoch(train) [4][650/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:19:24  time: 1.1960  data_time: 0.0176  memory: 10488  grad_norm: 50.6739  loss: 6.7175  loss_cls: 0.2900  loss_bbox: 0.0384  loss_iou: 0.3423  d0.loss_cls: 0.3196  d0.loss_bbox: 0.0410  d0.loss_iou: 0.3577  d1.loss_cls: 0.3015  d1.loss_bbox: 0.0397  d1.loss_iou: 0.3490  d2.loss_cls: 0.2987  d2.loss_bbox: 0.0386  d2.loss_iou: 0.3421  d3.loss_cls: 0.2942  d3.loss_bbox: 0.0385  d3.loss_iou: 0.3449  d4.loss_cls: 0.2907  d4.loss_bbox: 0.0383  d4.loss_iou: 0.3413  enc_loss_cls: 0.3313  enc_loss_bbox: 0.0438  enc_loss_iou: 0.3748  dn_loss_cls: 0.0080  dn_loss_bbox: 0.0310  dn_loss_iou: 0.2483  d0.dn_loss_cls: 0.0303  d0.dn_loss_bbox: 0.0426  d0.dn_loss_iou: 0.3244  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0328  d1.dn_loss_iou: 0.2606  d2.dn_loss_cls: 0.0091  d2.dn_loss_bbox: 0.0314  d2.dn_loss_iou: 0.2509  d3.dn_loss_cls: 0.0083  d3.dn_loss_bbox: 0.0310  d3.dn_loss_iou: 0.2484  d4.dn_loss_cls: 0.0081  d4.dn_loss_bbox: 0.0310  d4.dn_loss_iou: 0.2481  loss_num: 0.0009  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:30:30 - mmengine - INFO - Epoch(train) [4][700/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:18:24  time: 1.1841  data_time: 0.0111  memory: 10358  grad_norm: 52.1131  loss: 7.6783  loss_cls: 0.3410  loss_bbox: 0.0552  loss_iou: 0.3975  d0.loss_cls: 0.3765  d0.loss_bbox: 0.0532  d0.loss_iou: 0.3952  d1.loss_cls: 0.3539  d1.loss_bbox: 0.0527  d1.loss_iou: 0.3962  d2.loss_cls: 0.3543  d2.loss_bbox: 0.0519  d2.loss_iou: 0.3876  d3.loss_cls: 0.3491  d3.loss_bbox: 0.0514  d3.loss_iou: 0.3870  d4.loss_cls: 0.3433  d4.loss_bbox: 0.0516  d4.loss_iou: 0.3883  enc_loss_cls: 0.3879  enc_loss_bbox: 0.0553  enc_loss_iou: 0.4126  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0312  dn_loss_iou: 0.2751  d0.dn_loss_cls: 0.0304  d0.dn_loss_bbox: 0.0440  d0.dn_loss_iou: 0.3667  d1.dn_loss_cls: 0.0100  d1.dn_loss_bbox: 0.0332  d1.dn_loss_iou: 0.2906  d2.dn_loss_cls: 0.0071  d2.dn_loss_bbox: 0.0317  d2.dn_loss_iou: 0.2782  d3.dn_loss_cls: 0.0064  d3.dn_loss_bbox: 0.0312  d3.dn_loss_iou: 0.2756  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0312  d4.dn_loss_iou: 0.2750  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:31:29 - mmengine - INFO - Epoch(train) [4][750/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:17:25  time: 1.1920  data_time: 0.0115  memory: 10627  grad_norm: 53.6233  loss: 6.7490  loss_cls: 0.3053  loss_bbox: 0.0436  loss_iou: 0.3360  d0.loss_cls: 0.3260  d0.loss_bbox: 0.0466  d0.loss_iou: 0.3535  d1.loss_cls: 0.3107  d1.loss_bbox: 0.0451  d1.loss_iou: 0.3441  d2.loss_cls: 0.3101  d2.loss_bbox: 0.0438  d2.loss_iou: 0.3366  d3.loss_cls: 0.3089  d3.loss_bbox: 0.0436  d3.loss_iou: 0.3340  d4.loss_cls: 0.3079  d4.loss_bbox: 0.0436  d4.loss_iou: 0.3359  enc_loss_cls: 0.3445  enc_loss_bbox: 0.0494  enc_loss_iou: 0.3713  dn_loss_cls: 0.0084  dn_loss_bbox: 0.0315  dn_loss_iou: 0.2397  d0.dn_loss_cls: 0.0335  d0.dn_loss_bbox: 0.0418  d0.dn_loss_iou: 0.3088  d1.dn_loss_cls: 0.0121  d1.dn_loss_bbox: 0.0332  d1.dn_loss_iou: 0.2499  d2.dn_loss_cls: 0.0098  d2.dn_loss_bbox: 0.0318  d2.dn_loss_iou: 0.2416  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0316  d3.dn_loss_iou: 0.2401  d4.dn_loss_cls: 0.0088  d4.dn_loss_bbox: 0.0315  d4.dn_loss_iou: 0.2396  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:32:28 - mmengine - INFO - Epoch(train) [4][800/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:16:25  time: 1.1874  data_time: 0.0110  memory: 10621  grad_norm: 48.8739  loss: 6.1152  loss_cls: 0.2744  loss_bbox: 0.0418  loss_iou: 0.2747  d0.loss_cls: 0.2985  d0.loss_bbox: 0.0441  d0.loss_iou: 0.2937  d1.loss_cls: 0.2867  d1.loss_bbox: 0.0432  d1.loss_iou: 0.2827  d2.loss_cls: 0.2828  d2.loss_bbox: 0.0418  d2.loss_iou: 0.2758  d3.loss_cls: 0.2810  d3.loss_bbox: 0.0424  d3.loss_iou: 0.2773  d4.loss_cls: 0.2748  d4.loss_bbox: 0.0422  d4.loss_iou: 0.2764  enc_loss_cls: 0.3099  enc_loss_bbox: 0.0469  enc_loss_iou: 0.3015  dn_loss_cls: 0.0073  dn_loss_bbox: 0.0323  dn_loss_iou: 0.2412  d0.dn_loss_cls: 0.0376  d0.dn_loss_bbox: 0.0438  d0.dn_loss_iou: 0.3111  d1.dn_loss_cls: 0.0128  d1.dn_loss_bbox: 0.0337  d1.dn_loss_iou: 0.2513  d2.dn_loss_cls: 0.0085  d2.dn_loss_bbox: 0.0325  d2.dn_loss_iou: 0.2425  d3.dn_loss_cls: 0.0079  d3.dn_loss_bbox: 0.0323  d3.dn_loss_iou: 0.2412  d4.dn_loss_cls: 0.0077  d4.dn_loss_bbox: 0.0323  d4.dn_loss_iou: 0.2410  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0010  d4.loss_num: 0.0009
2024/10/23 08:32:45 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 08:32:45 - mmengine - INFO - Saving checkpoint at 4 epochs
2024/10/23 08:32:58 - mmengine - INFO - Epoch(val) [4][ 50/858]    eta: 0:01:22  time: 0.1018  data_time: 0.0027  memory: 10228  
2024/10/23 08:33:03 - mmengine - INFO - Epoch(val) [4][100/858]    eta: 0:01:16  time: 0.0991  data_time: 0.0020  memory: 4267  
2024/10/23 08:33:08 - mmengine - INFO - Epoch(val) [4][150/858]    eta: 0:01:10  time: 0.0988  data_time: 0.0020  memory: 4267  
2024/10/23 08:33:12 - mmengine - INFO - Epoch(val) [4][200/858]    eta: 0:01:05  time: 0.0990  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:17 - mmengine - INFO - Epoch(val) [4][250/858]    eta: 0:01:00  time: 0.0987  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:22 - mmengine - INFO - Epoch(val) [4][300/858]    eta: 0:00:55  time: 0.0981  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:27 - mmengine - INFO - Epoch(val) [4][350/858]    eta: 0:00:50  time: 0.0979  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:32 - mmengine - INFO - Epoch(val) [4][400/858]    eta: 0:00:45  time: 0.0983  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:37 - mmengine - INFO - Epoch(val) [4][450/858]    eta: 0:00:40  time: 0.0975  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:42 - mmengine - INFO - Epoch(val) [4][500/858]    eta: 0:00:35  time: 0.0975  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:47 - mmengine - INFO - Epoch(val) [4][550/858]    eta: 0:00:30  time: 0.0975  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:52 - mmengine - INFO - Epoch(val) [4][600/858]    eta: 0:00:25  time: 0.0977  data_time: 0.0019  memory: 4267  
2024/10/23 08:33:57 - mmengine - INFO - Epoch(val) [4][650/858]    eta: 0:00:20  time: 0.0986  data_time: 0.0021  memory: 4267  
2024/10/23 08:34:02 - mmengine - INFO - Epoch(val) [4][700/858]    eta: 0:00:15  time: 0.0979  data_time: 0.0020  memory: 4267  
2024/10/23 08:34:06 - mmengine - INFO - Epoch(val) [4][750/858]    eta: 0:00:10  time: 0.0980  data_time: 0.0020  memory: 4267  
2024/10/23 08:34:11 - mmengine - INFO - Epoch(val) [4][800/858]    eta: 0:00:05  time: 0.0984  data_time: 0.0019  memory: 4267  
2024/10/23 08:34:16 - mmengine - INFO - Epoch(val) [4][850/858]    eta: 0:00:00  time: 0.0970  data_time: 0.0019  memory: 4267  
2024/10/23 08:34:19 - mmengine - INFO - {'instance_F1_score': 0.32970235204329423, 'instance_acc': 0.20700976770910284, 'image_F1_score': 0.34040404040404043, 'image_acc': 0.23892773892773891}
2024/10/23 08:34:19 - mmengine - INFO - Epoch(val) [4][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.3297  grefcoco_val/refdrone/instance_acc: 0.2070  grefcoco_val/refdrone/image_F1_score: 0.3404  grefcoco_val/refdrone/image_acc: 0.2389  data_time: 0.0020  time: 0.0983
2024/10/23 08:35:18 - mmengine - INFO - Epoch(train) [5][ 50/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:15:09  time: 1.1954  data_time: 0.0097  memory: 10627  grad_norm: 50.3348  loss: 6.7394  loss_cls: 0.3194  loss_bbox: 0.0427  loss_iou: 0.3049  d0.loss_cls: 0.3450  d0.loss_bbox: 0.0483  d0.loss_iou: 0.3232  d1.loss_cls: 0.3283  d1.loss_bbox: 0.0436  d1.loss_iou: 0.3103  d2.loss_cls: 0.3226  d2.loss_bbox: 0.0427  d2.loss_iou: 0.3077  d3.loss_cls: 0.3217  d3.loss_bbox: 0.0428  d3.loss_iou: 0.3053  d4.loss_cls: 0.3187  d4.loss_bbox: 0.0430  d4.loss_iou: 0.3057  enc_loss_cls: 0.3668  enc_loss_bbox: 0.0472  enc_loss_iou: 0.3303  dn_loss_cls: 0.0118  dn_loss_bbox: 0.0324  dn_loss_iou: 0.2502  d0.dn_loss_cls: 0.0422  d0.dn_loss_bbox: 0.0453  d0.dn_loss_iou: 0.3273  d1.dn_loss_cls: 0.0193  d1.dn_loss_bbox: 0.0345  d1.dn_loss_iou: 0.2636  d2.dn_loss_cls: 0.0132  d2.dn_loss_bbox: 0.0327  d2.dn_loss_iou: 0.2522  d3.dn_loss_cls: 0.0122  d3.dn_loss_bbox: 0.0325  d3.dn_loss_iou: 0.2505  d4.dn_loss_cls: 0.0115  d4.dn_loss_bbox: 0.0324  d4.dn_loss_iou: 0.2502  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/23 08:36:18 - mmengine - INFO - Epoch(train) [5][100/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:14:09  time: 1.1846  data_time: 0.0113  memory: 10618  grad_norm: 57.3246  loss: 7.2246  loss_cls: 0.3350  loss_bbox: 0.0496  loss_iou: 0.3370  d0.loss_cls: 0.3527  d0.loss_bbox: 0.0560  d0.loss_iou: 0.3543  d1.loss_cls: 0.3407  d1.loss_bbox: 0.0508  d1.loss_iou: 0.3395  d2.loss_cls: 0.3342  d2.loss_bbox: 0.0502  d2.loss_iou: 0.3387  d3.loss_cls: 0.3347  d3.loss_bbox: 0.0523  d3.loss_iou: 0.3433  d4.loss_cls: 0.3370  d4.loss_bbox: 0.0497  d4.loss_iou: 0.3378  enc_loss_cls: 0.3676  enc_loss_bbox: 0.0607  enc_loss_iou: 0.3752  dn_loss_cls: 0.0113  dn_loss_bbox: 0.0349  dn_loss_iou: 0.2661  d0.dn_loss_cls: 0.0427  d0.dn_loss_bbox: 0.0480  d0.dn_loss_iou: 0.3468  d1.dn_loss_cls: 0.0164  d1.dn_loss_bbox: 0.0365  d1.dn_loss_iou: 0.2788  d2.dn_loss_cls: 0.0123  d2.dn_loss_bbox: 0.0352  d2.dn_loss_iou: 0.2682  d3.dn_loss_cls: 0.0112  d3.dn_loss_bbox: 0.0348  d3.dn_loss_iou: 0.2662  d4.dn_loss_cls: 0.0114  d4.dn_loss_bbox: 0.0349  d4.dn_loss_iou: 0.2661  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:37:17 - mmengine - INFO - Epoch(train) [5][150/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:13:10  time: 1.1911  data_time: 0.0110  memory: 10618  grad_norm: 47.9999  loss: 7.6627  loss_cls: 0.3541  loss_bbox: 0.0453  loss_iou: 0.4148  d0.loss_cls: 0.3927  d0.loss_bbox: 0.0490  d0.loss_iou: 0.4279  d1.loss_cls: 0.3669  d1.loss_bbox: 0.0481  d1.loss_iou: 0.4220  d2.loss_cls: 0.3597  d2.loss_bbox: 0.0469  d2.loss_iou: 0.4182  d3.loss_cls: 0.3566  d3.loss_bbox: 0.0455  d3.loss_iou: 0.4154  d4.loss_cls: 0.3546  d4.loss_bbox: 0.0457  d4.loss_iou: 0.4161  enc_loss_cls: 0.3968  enc_loss_bbox: 0.0500  enc_loss_iou: 0.4366  dn_loss_cls: 0.0062  dn_loss_bbox: 0.0287  dn_loss_iou: 0.2430  d0.dn_loss_cls: 0.0296  d0.dn_loss_bbox: 0.0386  d0.dn_loss_iou: 0.3136  d1.dn_loss_cls: 0.0111  d1.dn_loss_bbox: 0.0300  d1.dn_loss_iou: 0.2540  d2.dn_loss_cls: 0.0077  d2.dn_loss_bbox: 0.0290  d2.dn_loss_iou: 0.2452  d3.dn_loss_cls: 0.0066  d3.dn_loss_bbox: 0.0287  d3.dn_loss_iou: 0.2431  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0287  d4.dn_loss_iou: 0.2430  loss_num: 0.0011  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/23 08:38:17 - mmengine - INFO - Epoch(train) [5][200/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:12:11  time: 1.1999  data_time: 0.0113  memory: 10627  grad_norm: 48.7949  loss: 7.5275  loss_cls: 0.3393  loss_bbox: 0.0428  loss_iou: 0.3757  d0.loss_cls: 0.3688  d0.loss_bbox: 0.0449  d0.loss_iou: 0.3900  d1.loss_cls: 0.3490  d1.loss_bbox: 0.0442  d1.loss_iou: 0.3835  d2.loss_cls: 0.3412  d2.loss_bbox: 0.0435  d2.loss_iou: 0.3800  d3.loss_cls: 0.3407  d3.loss_bbox: 0.0433  d3.loss_iou: 0.3782  d4.loss_cls: 0.3379  d4.loss_bbox: 0.0433  d4.loss_iou: 0.3782  enc_loss_cls: 0.3717  enc_loss_bbox: 0.0493  enc_loss_iou: 0.4069  dn_loss_cls: 0.0096  dn_loss_bbox: 0.0304  dn_loss_iou: 0.2780  d0.dn_loss_cls: 0.0452  d0.dn_loss_bbox: 0.0421  d0.dn_loss_iou: 0.3642  d1.dn_loss_cls: 0.0143  d1.dn_loss_bbox: 0.0322  d1.dn_loss_iou: 0.2941  d2.dn_loss_cls: 0.0105  d2.dn_loss_bbox: 0.0309  d2.dn_loss_iou: 0.2826  d3.dn_loss_cls: 0.0099  d3.dn_loss_bbox: 0.0305  d3.dn_loss_iou: 0.2787  d4.dn_loss_cls: 0.0094  d4.dn_loss_bbox: 0.0304  d4.dn_loss_iou: 0.2779  loss_num: 0.0007  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/23 08:39:18 - mmengine - INFO - Epoch(train) [5][250/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:11:11  time: 1.2115  data_time: 0.0106  memory: 10627  grad_norm: 52.6658  loss: 8.1348  loss_cls: 0.3746  loss_bbox: 0.0479  loss_iou: 0.4312  d0.loss_cls: 0.4220  d0.loss_bbox: 0.0506  d0.loss_iou: 0.4451  d1.loss_cls: 0.3907  d1.loss_bbox: 0.0513  d1.loss_iou: 0.4436  d2.loss_cls: 0.3735  d2.loss_bbox: 0.0505  d2.loss_iou: 0.4426  d3.loss_cls: 0.3716  d3.loss_bbox: 0.0496  d3.loss_iou: 0.4378  d4.loss_cls: 0.3704  d4.loss_bbox: 0.0483  d4.loss_iou: 0.4343  enc_loss_cls: 0.4262  enc_loss_bbox: 0.0545  enc_loss_iou: 0.4630  dn_loss_cls: 0.0156  dn_loss_bbox: 0.0283  dn_loss_iou: 0.2575  d0.dn_loss_cls: 0.0460  d0.dn_loss_bbox: 0.0388  d0.dn_loss_iou: 0.3330  d1.dn_loss_cls: 0.0220  d1.dn_loss_bbox: 0.0299  d1.dn_loss_iou: 0.2701  d2.dn_loss_cls: 0.0171  d2.dn_loss_bbox: 0.0286  d2.dn_loss_iou: 0.2597  d3.dn_loss_cls: 0.0166  d3.dn_loss_bbox: 0.0283  d3.dn_loss_iou: 0.2579  d4.dn_loss_cls: 0.0161  d4.dn_loss_bbox: 0.0283  d4.dn_loss_iou: 0.2574  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/23 08:40:17 - mmengine - INFO - Epoch(train) [5][300/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:10:12  time: 1.1924  data_time: 0.0113  memory: 10624  grad_norm: 50.9736  loss: 7.1111  loss_cls: 0.3215  loss_bbox: 0.0450  loss_iou: 0.3696  d0.loss_cls: 0.3518  d0.loss_bbox: 0.0446  d0.loss_iou: 0.3854  d1.loss_cls: 0.3309  d1.loss_bbox: 0.0456  d1.loss_iou: 0.3795  d2.loss_cls: 0.3243  d2.loss_bbox: 0.0451  d2.loss_iou: 0.3745  d3.loss_cls: 0.3236  d3.loss_bbox: 0.0431  d3.loss_iou: 0.3652  d4.loss_cls: 0.3159  d4.loss_bbox: 0.0454  d4.loss_iou: 0.3727  enc_loss_cls: 0.3722  enc_loss_bbox: 0.0475  enc_loss_iou: 0.3942  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2496  d0.dn_loss_cls: 0.0272  d0.dn_loss_bbox: 0.0361  d0.dn_loss_iou: 0.3166  d1.dn_loss_cls: 0.0089  d1.dn_loss_bbox: 0.0285  d1.dn_loss_iou: 0.2589  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0276  d2.dn_loss_iou: 0.2514  d3.dn_loss_cls: 0.0056  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2499  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2495  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/23 08:41:17 - mmengine - INFO - Epoch(train) [5][350/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:09:12  time: 1.1833  data_time: 0.0107  memory: 10627  grad_norm: 45.8688  loss: 6.5769  loss_cls: 0.2872  loss_bbox: 0.0418  loss_iou: 0.3242  d0.loss_cls: 0.3127  d0.loss_bbox: 0.0431  d0.loss_iou: 0.3352  d1.loss_cls: 0.2958  d1.loss_bbox: 0.0418  d1.loss_iou: 0.3227  d2.loss_cls: 0.2919  d2.loss_bbox: 0.0416  d2.loss_iou: 0.3215  d3.loss_cls: 0.2889  d3.loss_bbox: 0.0417  d3.loss_iou: 0.3228  d4.loss_cls: 0.2893  d4.loss_bbox: 0.0419  d4.loss_iou: 0.3237  enc_loss_cls: 0.3296  enc_loss_bbox: 0.0454  enc_loss_iou: 0.3464  dn_loss_cls: 0.0091  dn_loss_bbox: 0.0323  dn_loss_iou: 0.2510  d0.dn_loss_cls: 0.0350  d0.dn_loss_bbox: 0.0432  d0.dn_loss_iou: 0.3208  d1.dn_loss_cls: 0.0133  d1.dn_loss_bbox: 0.0339  d1.dn_loss_iou: 0.2628  d2.dn_loss_cls: 0.0103  d2.dn_loss_bbox: 0.0326  d2.dn_loss_iou: 0.2532  d3.dn_loss_cls: 0.0094  d3.dn_loss_bbox: 0.0323  d3.dn_loss_iou: 0.2513  d4.dn_loss_cls: 0.0089  d4.dn_loss_bbox: 0.0323  d4.dn_loss_iou: 0.2510  loss_num: 0.0008  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0007
2024/10/23 08:42:16 - mmengine - INFO - Epoch(train) [5][400/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:08:12  time: 1.1863  data_time: 0.0109  memory: 10621  grad_norm: 54.9486  loss: 7.1527  loss_cls: 0.3339  loss_bbox: 0.0490  loss_iou: 0.3675  d0.loss_cls: 0.3610  d0.loss_bbox: 0.0524  d0.loss_iou: 0.3851  d1.loss_cls: 0.3442  d1.loss_bbox: 0.0501  d1.loss_iou: 0.3738  d2.loss_cls: 0.3426  d2.loss_bbox: 0.0493  d2.loss_iou: 0.3686  d3.loss_cls: 0.3404  d3.loss_bbox: 0.0491  d3.loss_iou: 0.3667  d4.loss_cls: 0.3293  d4.loss_bbox: 0.0492  d4.loss_iou: 0.3685  enc_loss_cls: 0.3737  enc_loss_bbox: 0.0559  enc_loss_iou: 0.3997  dn_loss_cls: 0.0085  dn_loss_bbox: 0.0305  dn_loss_iou: 0.2299  d0.dn_loss_cls: 0.0315  d0.dn_loss_bbox: 0.0416  d0.dn_loss_iou: 0.2953  d1.dn_loss_cls: 0.0140  d1.dn_loss_bbox: 0.0324  d1.dn_loss_iou: 0.2424  d2.dn_loss_cls: 0.0098  d2.dn_loss_bbox: 0.0311  d2.dn_loss_iou: 0.2320  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0307  d3.dn_loss_iou: 0.2301  d4.dn_loss_cls: 0.0085  d4.dn_loss_bbox: 0.0305  d4.dn_loss_iou: 0.2297  loss_num: 0.0009  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:43:16 - mmengine - INFO - Epoch(train) [5][450/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:07:13  time: 1.1929  data_time: 0.0106  memory: 10627  grad_norm: 55.6177  loss: 7.4276  loss_cls: 0.3406  loss_bbox: 0.0471  loss_iou: 0.3798  d0.loss_cls: 0.3573  d0.loss_bbox: 0.0494  d0.loss_iou: 0.3916  d1.loss_cls: 0.3506  d1.loss_bbox: 0.0463  d1.loss_iou: 0.3801  d2.loss_cls: 0.3509  d2.loss_bbox: 0.0456  d2.loss_iou: 0.3747  d3.loss_cls: 0.3456  d3.loss_bbox: 0.0462  d3.loss_iou: 0.3785  d4.loss_cls: 0.3445  d4.loss_bbox: 0.0467  d4.loss_iou: 0.3765  enc_loss_cls: 0.3641  enc_loss_bbox: 0.0496  enc_loss_iou: 0.3991  dn_loss_cls: 0.0079  dn_loss_bbox: 0.0324  dn_loss_iou: 0.2631  d0.dn_loss_cls: 0.0342  d0.dn_loss_bbox: 0.0440  d0.dn_loss_iou: 0.3403  d1.dn_loss_cls: 0.0111  d1.dn_loss_bbox: 0.0340  d1.dn_loss_iou: 0.2746  d2.dn_loss_cls: 0.0093  d2.dn_loss_bbox: 0.0328  d2.dn_loss_iou: 0.2654  d3.dn_loss_cls: 0.0085  d3.dn_loss_bbox: 0.0325  d3.dn_loss_iou: 0.2632  d4.dn_loss_cls: 0.0080  d4.dn_loss_bbox: 0.0324  d4.dn_loss_iou: 0.2630  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:44:18 - mmengine - INFO - Epoch(train) [5][500/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:06:14  time: 1.2455  data_time: 0.0746  memory: 10610  grad_norm: 49.0188  loss: 7.1385  loss_cls: 0.3232  loss_bbox: 0.0414  loss_iou: 0.3640  d0.loss_cls: 0.3446  d0.loss_bbox: 0.0457  d0.loss_iou: 0.3736  d1.loss_cls: 0.3275  d1.loss_bbox: 0.0454  d1.loss_iou: 0.3709  d2.loss_cls: 0.3232  d2.loss_bbox: 0.0436  d2.loss_iou: 0.3637  d3.loss_cls: 0.3225  d3.loss_bbox: 0.0410  d3.loss_iou: 0.3613  d4.loss_cls: 0.3214  d4.loss_bbox: 0.0413  d4.loss_iou: 0.3628  enc_loss_cls: 0.3539  enc_loss_bbox: 0.0490  enc_loss_iou: 0.3934  dn_loss_cls: 0.0069  dn_loss_bbox: 0.0298  dn_loss_iou: 0.2602  d0.dn_loss_cls: 0.0348  d0.dn_loss_bbox: 0.0408  d0.dn_loss_iou: 0.3361  d1.dn_loss_cls: 0.0120  d1.dn_loss_bbox: 0.0313  d1.dn_loss_iou: 0.2728  d2.dn_loss_cls: 0.0086  d2.dn_loss_bbox: 0.0300  d2.dn_loss_iou: 0.2624  d3.dn_loss_cls: 0.0072  d3.dn_loss_bbox: 0.0298  d3.dn_loss_iou: 0.2601  d4.dn_loss_cls: 0.0069  d4.dn_loss_bbox: 0.0298  d4.dn_loss_iou: 0.2601  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0009
2024/10/23 08:45:18 - mmengine - INFO - Epoch(train) [5][550/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:05:14  time: 1.1954  data_time: 0.0106  memory: 10621  grad_norm: 50.5647  loss: 6.9505  loss_cls: 0.3347  loss_bbox: 0.0379  loss_iou: 0.3245  d0.loss_cls: 0.3501  d0.loss_bbox: 0.0402  d0.loss_iou: 0.3395  d1.loss_cls: 0.3340  d1.loss_bbox: 0.0399  d1.loss_iou: 0.3353  d2.loss_cls: 0.3404  d2.loss_bbox: 0.0382  d2.loss_iou: 0.3265  d3.loss_cls: 0.3339  d3.loss_bbox: 0.0380  d3.loss_iou: 0.3251  d4.loss_cls: 0.3338  d4.loss_bbox: 0.0380  d4.loss_iou: 0.3244  enc_loss_cls: 0.3590  enc_loss_bbox: 0.0426  enc_loss_iou: 0.3506  dn_loss_cls: 0.0329  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2450  d0.dn_loss_cls: 0.0577  d0.dn_loss_bbox: 0.0374  d0.dn_loss_iou: 0.3183  d1.dn_loss_cls: 0.0375  d1.dn_loss_bbox: 0.0285  d1.dn_loss_iou: 0.2560  d2.dn_loss_cls: 0.0341  d2.dn_loss_bbox: 0.0275  d2.dn_loss_iou: 0.2464  d3.dn_loss_cls: 0.0342  d3.dn_loss_bbox: 0.0273  d3.dn_loss_iou: 0.2450  d4.dn_loss_cls: 0.0315  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2448  loss_num: 0.0009  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:46:17 - mmengine - INFO - Epoch(train) [5][600/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:04:14  time: 1.1924  data_time: 0.0119  memory: 10630  grad_norm: 50.2859  loss: 6.5199  loss_cls: 0.3159  loss_bbox: 0.0408  loss_iou: 0.3011  d0.loss_cls: 0.3490  d0.loss_bbox: 0.0438  d0.loss_iou: 0.3135  d1.loss_cls: 0.3252  d1.loss_bbox: 0.0424  d1.loss_iou: 0.3114  d2.loss_cls: 0.3176  d2.loss_bbox: 0.0419  d2.loss_iou: 0.3085  d3.loss_cls: 0.3192  d3.loss_bbox: 0.0408  d3.loss_iou: 0.3012  d4.loss_cls: 0.3179  d4.loss_bbox: 0.0408  d4.loss_iou: 0.3011  enc_loss_cls: 0.3538  enc_loss_bbox: 0.0469  enc_loss_iou: 0.3326  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0281  dn_loss_iou: 0.2345  d0.dn_loss_cls: 0.0363  d0.dn_loss_bbox: 0.0385  d0.dn_loss_iou: 0.3081  d1.dn_loss_cls: 0.0112  d1.dn_loss_bbox: 0.0296  d1.dn_loss_iou: 0.2458  d2.dn_loss_cls: 0.0072  d2.dn_loss_bbox: 0.0284  d2.dn_loss_iou: 0.2372  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0282  d3.dn_loss_iou: 0.2354  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0281  d4.dn_loss_iou: 0.2345  loss_num: 0.0009  d0.loss_num: 0.0010  d1.loss_num: 0.0008  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:47:17 - mmengine - INFO - Epoch(train) [5][650/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:03:15  time: 1.1871  data_time: 0.0109  memory: 10621  grad_norm: 53.7548  loss: 6.6440  loss_cls: 0.3253  loss_bbox: 0.0402  loss_iou: 0.3084  d0.loss_cls: 0.3546  d0.loss_bbox: 0.0380  d0.loss_iou: 0.3029  d1.loss_cls: 0.3385  d1.loss_bbox: 0.0380  d1.loss_iou: 0.3065  d2.loss_cls: 0.3338  d2.loss_bbox: 0.0390  d2.loss_iou: 0.3075  d3.loss_cls: 0.3297  d3.loss_bbox: 0.0389  d3.loss_iou: 0.3072  d4.loss_cls: 0.3255  d4.loss_bbox: 0.0390  d4.loss_iou: 0.3072  enc_loss_cls: 0.3657  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3266  dn_loss_cls: 0.0082  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2427  d0.dn_loss_cls: 0.0354  d0.dn_loss_bbox: 0.0411  d0.dn_loss_iou: 0.3214  d1.dn_loss_cls: 0.0118  d1.dn_loss_bbox: 0.0312  d1.dn_loss_iou: 0.2559  d2.dn_loss_cls: 0.0089  d2.dn_loss_bbox: 0.0298  d2.dn_loss_iou: 0.2451  d3.dn_loss_cls: 0.0085  d3.dn_loss_bbox: 0.0296  d3.dn_loss_iou: 0.2430  d4.dn_loss_cls: 0.0082  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2427  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0009  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2024/10/23 08:48:16 - mmengine - INFO - Epoch(train) [5][700/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:02:15  time: 1.1852  data_time: 0.0110  memory: 10621  grad_norm: 53.8753  loss: 6.9279  loss_cls: 0.3006  loss_bbox: 0.0437  loss_iou: 0.3204  d0.loss_cls: 0.3161  d0.loss_bbox: 0.0426  d0.loss_iou: 0.3313  d1.loss_cls: 0.3036  d1.loss_bbox: 0.0436  d1.loss_iou: 0.3248  d2.loss_cls: 0.3025  d2.loss_bbox: 0.0430  d2.loss_iou: 0.3206  d3.loss_cls: 0.3010  d3.loss_bbox: 0.0428  d3.loss_iou: 0.3212  d4.loss_cls: 0.3015  d4.loss_bbox: 0.0437  d4.loss_iou: 0.3204  enc_loss_cls: 0.3317  enc_loss_bbox: 0.0442  enc_loss_iou: 0.3423  dn_loss_cls: 0.0567  dn_loss_bbox: 0.0317  dn_loss_iou: 0.2502  d0.dn_loss_cls: 0.0875  d0.dn_loss_bbox: 0.0431  d0.dn_loss_iou: 0.3209  d1.dn_loss_cls: 0.0633  d1.dn_loss_bbox: 0.0335  d1.dn_loss_iou: 0.2633  d2.dn_loss_cls: 0.0647  d2.dn_loss_bbox: 0.0322  d2.dn_loss_iou: 0.2528  d3.dn_loss_cls: 0.0625  d3.dn_loss_bbox: 0.0318  d3.dn_loss_iou: 0.2504  d4.dn_loss_cls: 0.0551  d4.dn_loss_bbox: 0.0317  d4.dn_loss_iou: 0.2501  loss_num: 0.0008  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/23 08:49:08 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 08:49:15 - mmengine - INFO - Epoch(train) [5][750/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:01:16  time: 1.1910  data_time: 0.0105  memory: 10497  grad_norm: 48.0576  loss: 6.7811  loss_cls: 0.2890  loss_bbox: 0.0419  loss_iou: 0.3290  d0.loss_cls: 0.3111  d0.loss_bbox: 0.0428  d0.loss_iou: 0.3362  d1.loss_cls: 0.2964  d1.loss_bbox: 0.0422  d1.loss_iou: 0.3343  d2.loss_cls: 0.2974  d2.loss_bbox: 0.0418  d2.loss_iou: 0.3285  d3.loss_cls: 0.2936  d3.loss_bbox: 0.0420  d3.loss_iou: 0.3295  d4.loss_cls: 0.2917  d4.loss_bbox: 0.0420  d4.loss_iou: 0.3293  enc_loss_cls: 0.3238  enc_loss_bbox: 0.0438  enc_loss_iou: 0.3496  dn_loss_cls: 0.0089  dn_loss_bbox: 0.0339  dn_loss_iou: 0.2742  d0.dn_loss_cls: 0.0323  d0.dn_loss_bbox: 0.0456  d0.dn_loss_iou: 0.3509  d1.dn_loss_cls: 0.0136  d1.dn_loss_bbox: 0.0358  d1.dn_loss_iou: 0.2881  d2.dn_loss_cls: 0.0101  d2.dn_loss_bbox: 0.0343  d2.dn_loss_iou: 0.2766  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.0340  d3.dn_loss_iou: 0.2748  d4.dn_loss_cls: 0.0090  d4.dn_loss_bbox: 0.0339  d4.dn_loss_iou: 0.2742  loss_num: 0.0009  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/23 08:50:15 - mmengine - INFO - Epoch(train) [5][800/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:00:16  time: 1.1936  data_time: 0.0113  memory: 10633  grad_norm: 47.5406  loss: 7.3188  loss_cls: 0.3533  loss_bbox: 0.0424  loss_iou: 0.3609  d0.loss_cls: 0.3807  d0.loss_bbox: 0.0442  d0.loss_iou: 0.3753  d1.loss_cls: 0.3576  d1.loss_bbox: 0.0429  d1.loss_iou: 0.3678  d2.loss_cls: 0.3516  d2.loss_bbox: 0.0418  d2.loss_iou: 0.3643  d3.loss_cls: 0.3531  d3.loss_bbox: 0.0411  d3.loss_iou: 0.3566  d4.loss_cls: 0.3551  d4.loss_bbox: 0.0406  d4.loss_iou: 0.3550  enc_loss_cls: 0.3927  enc_loss_bbox: 0.0474  enc_loss_iou: 0.3950  dn_loss_cls: 0.0085  dn_loss_bbox: 0.0272  dn_loss_iou: 0.2545  d0.dn_loss_cls: 0.0412  d0.dn_loss_bbox: 0.0384  d0.dn_loss_iou: 0.3341  d1.dn_loss_cls: 0.0143  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2686  d2.dn_loss_cls: 0.0103  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2576  d3.dn_loss_cls: 0.0088  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2552  d4.dn_loss_cls: 0.0086  d4.dn_loss_bbox: 0.0272  d4.dn_loss_iou: 0.2544  loss_num: 0.0011  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/23 08:50:32 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_20241023_072251
2024/10/23 08:50:32 - mmengine - INFO - Saving checkpoint at 5 epochs
2024/10/23 08:50:44 - mmengine - INFO - Epoch(val) [5][ 50/858]    eta: 0:01:21  time: 0.1013  data_time: 0.0026  memory: 10618  
2024/10/23 08:50:49 - mmengine - INFO - Epoch(val) [5][100/858]    eta: 0:01:15  time: 0.0991  data_time: 0.0019  memory: 4267  
2024/10/23 08:50:54 - mmengine - INFO - Epoch(val) [5][150/858]    eta: 0:01:10  time: 0.0983  data_time: 0.0019  memory: 4267  
2024/10/23 08:50:59 - mmengine - INFO - Epoch(val) [5][200/858]    eta: 0:01:05  time: 0.0984  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:04 - mmengine - INFO - Epoch(val) [5][250/858]    eta: 0:01:00  time: 0.0980  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:09 - mmengine - INFO - Epoch(val) [5][300/858]    eta: 0:00:55  time: 0.0975  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:14 - mmengine - INFO - Epoch(val) [5][350/858]    eta: 0:00:50  time: 0.0974  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:18 - mmengine - INFO - Epoch(val) [5][400/858]    eta: 0:00:45  time: 0.0980  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:23 - mmengine - INFO - Epoch(val) [5][450/858]    eta: 0:00:40  time: 0.0976  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:28 - mmengine - INFO - Epoch(val) [5][500/858]    eta: 0:00:35  time: 0.0979  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:33 - mmengine - INFO - Epoch(val) [5][550/858]    eta: 0:00:30  time: 0.0975  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:38 - mmengine - INFO - Epoch(val) [5][600/858]    eta: 0:00:25  time: 0.0977  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:43 - mmengine - INFO - Epoch(val) [5][650/858]    eta: 0:00:20  time: 0.0979  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:48 - mmengine - INFO - Epoch(val) [5][700/858]    eta: 0:00:15  time: 0.0977  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:53 - mmengine - INFO - Epoch(val) [5][750/858]    eta: 0:00:10  time: 0.0974  data_time: 0.0019  memory: 4267  
2024/10/23 08:51:58 - mmengine - INFO - Epoch(val) [5][800/858]    eta: 0:00:05  time: 0.0974  data_time: 0.0019  memory: 4267  
2024/10/23 08:52:02 - mmengine - INFO - Epoch(val) [5][850/858]    eta: 0:00:00  time: 0.0974  data_time: 0.0022  memory: 4267  
2024/10/23 08:52:05 - mmengine - INFO - {'instance_F1_score': 0.34444826400825024, 'instance_acc': 0.21760892754574546, 'image_F1_score': 0.34007074279939364, 'image_acc': 0.23892773892773891}
2024/10/23 08:52:05 - mmengine - INFO - Epoch(val) [5][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.3444  grefcoco_val/refdrone/instance_acc: 0.2176  grefcoco_val/refdrone/image_F1_score: 0.3401  grefcoco_val/refdrone/image_acc: 0.2389  data_time: 0.0020  time: 0.0980
