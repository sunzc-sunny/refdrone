2025/10/29 08:25:25 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1848661858
    GPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-40GB
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1848661858
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 8
------------------------------------------------------------

2025/10/29 08:25:27 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=32, enable=True)
backend_args = None
coco_od_dataset = dict(
    ann_file='o365v1_train_odvg.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='data/objects365v1/',
    filter_cfg=dict(filter_empty_gt=False),
    label_map_file='o365v1_label_map.json',
    pipeline=[
        dict(backend_args=None, type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            transforms=[
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                400,
                                4200,
                            ),
                            (
                                500,
                                4200,
                            ),
                            (
                                600,
                                4200,
                            ),
                        ],
                        type='RandomChoiceResize'),
                    dict(
                        allow_negative_crop=True,
                        crop_size=(
                            384,
                            600,
                        ),
                        crop_type='absolute_range',
                        type='RandomCrop'),
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
            ],
            type='RandomChoice'),
        dict(min_gt_bbox_wh=(
            0.01,
            0.01,
        ), type='FilterAnnotations'),
        dict(
            max_tokens=256,
            num_sample_negative=85,
            tokenizer_name=
            '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
            type='RandomSamplingNegPos'),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'flip',
                'flip_direction',
                'text',
                'custom_entities',
                'tokens_positive',
                'dataset_mode',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    type='ODVGDataset')
data_root = 'data/objects365v1/'
dataset_prefixes = [
    'grefcoco_val',
]
dataset_type = 'ODVGDataset'
datasets = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        backend_args=None,
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        test_mode=True,
        type='MDETRStyleRefCocoDataset'),
]
default_hooks = dict(
    checkpoint=dict(interval=1, type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='GroundingVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
lang_model_name = '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased'
launcher = 'pytorch'
load_from = '/mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-t_numbranch_pretrain.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
max_epochs = 5
metrics = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        iou_thrs=0.5,
        metric='bbox',
        thresh_f1=1.0,
        thresh_score=0.7,
        type='RefDroneMetric'),
]
model = dict(
    as_two_stage=True,
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            6,
            2,
        ],
        drop_path_rate=0.2,
        drop_rate=0.0,
        embed_dims=96,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            '/mnt/public/usr/sunzhichao/mmdetection/swin_tiny_patch4_window7_224.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=(
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=7,
        with_cp=True),
    bbox_head=dict(
        contrastive_cfg=dict(bias=True, log_scale='auto', max_text_len=256),
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            type='FocalLoss',
            use_sigmoid=True),
        num_classes=256,
        sync_cls_avg_factor=True,
        type='GroundingDINOHeadNumv11'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    decoder=dict(
        layer_cfg=dict(
            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            cross_attn_text_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),
        num_layers=6,
        post_norm_cfg=None,
        return_intermediate=True),
    dn_cfg=dict(
        box_noise_scale=1.0,
        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
        label_noise_scale=0.5),
    encoder=dict(
        fusion_layer_cfg=dict(
            embed_dim=1024,
            init_values=0.0001,
            l_dim=256,
            num_heads=4,
            v_dim=256),
        layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=4)),
        num_cp=6,
        num_layers=6,
        text_layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=1024, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=4))),
    language_model=dict(
        add_pooling_layer=False,
        max_tokens=256,
        name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        pad_to_max=False,
        special_tokens_list=[
            '[CLS]',
            '[SEP]',
            '.',
            '?',
        ],
        type='BertModel',
        use_sub_sentence_represent=True),
    neck=dict(
        act_cfg=None,
        bias=True,
        in_channels=[
            192,
            384,
            768,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=4,
        out_channels=256,
        type='ChannelMapper'),
    num_queries=900,
    positional_encoding=dict(
        normalize=True, num_feats=128, offset=0.0, temperature=20),
    test_cfg=dict(max_per_img=300),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='BinaryFocalLossCost', weight=2.0),
                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                dict(iou_mode='giou', type='IoUCost', weight=2.0),
            ],
            type='HungarianAssigner')),
    type='NumGroundingDINO',
    with_box_refine=True)
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0001, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.1))),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=5,
        gamma=0.1,
        milestones=[
            4,
        ],
        type='MultiStepLR'),
]
pretrained = '/mnt/public/usr/sunzhichao/mmdetection/swin_tiny_patch4_window7_224.pth'
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        backend_args=None,
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        data_root='data/coco/',
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        test_mode=True,
        type='MDETRStyleRefCocoDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'tokens_positive',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=5, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=8,
    dataset=dict(
        dataset=dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_train7_vg.json',
            data_prefix=dict(
                img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
            filter_cfg=dict(filter_empty_gt=False),
            pipeline=[
                dict(backend_args=None, type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True),
                dict(prob=0.0, type='RandomFlip'),
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
                dict(
                    max_tokens=256,
                    num_sample_negative=85,
                    tokenizer_name=
                    '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
                    type='RandomSamplingNegPos'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'text',
                        'custom_entities',
                        'tokens_positive',
                        'dataset_mode',
                    ),
                    type='PackDetInputs'),
            ],
            return_classes=True,
            type='ODVGDataset'),
        times=10,
        type='RepeatDataset'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.0, type='RandomFlip'),
    dict(
        keep_ratio=True,
        scales=[
            (
                480,
                1333,
            ),
            (
                512,
                1333,
            ),
            (
                544,
                1333,
            ),
            (
                576,
                1333,
            ),
            (
                608,
                1333,
            ),
            (
                640,
                1333,
            ),
            (
                672,
                1333,
            ),
            (
                704,
                1333,
            ),
            (
                736,
                1333,
            ),
            (
                768,
                1333,
            ),
            (
                800,
                1333,
            ),
        ],
        type='RandomChoiceResize'),
    dict(
        max_tokens=256,
        num_sample_negative=85,
        tokenizer_name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        type='RandomSamplingNegPos'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'text',
            'custom_entities',
            'tokens_positive',
            'dataset_mode',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
                backend_args=None,
                data_prefix=dict(
                    img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='pillow',
                        type='LoadImageFromFile'),
                    dict(
                        backend='pillow',
                        keep_ratio=True,
                        scale=(
                            800,
                            1333,
                        ),
                        type='FixScaleResize'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(
                        meta_keys=(
                            'img_id',
                            'img_path',
                            'ori_shape',
                            'img_shape',
                            'scale_factor',
                            'text',
                            'custom_entities',
                            'tokens_positive',
                        ),
                        type='PackDetInputs'),
                ],
                return_classes=True,
                test_mode=True,
                type='MDETRStyleRefCocoDataset'),
        ],
        type='ConcatDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_dataset_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    backend_args=None,
    data_prefix=dict(img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='pillow',
            type='LoadImageFromFile'),
        dict(
            backend='pillow',
            keep_ratio=True,
            scale=(
                800,
                1333,
            ),
            type='FixScaleResize'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'text',
                'custom_entities',
                'tokens_positive',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    test_mode=True,
    type='MDETRStyleRefCocoDataset')
val_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
val_evaluator_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    iou_thrs=0.5,
    metric='bbox',
    thresh_f1=1.0,
    thresh_score=0.7,
    type='RefDroneMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2'

2025/10/29 08:25:31 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr=1e-05
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 08:25:34 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 08:25:35 - mmengine - INFO - LR is set based on batch size of 32 and the current batch size is 64. Scaling the original LR by 2.0.
2025/10/29 08:25:37 - mmengine - INFO - Loads checkpoint by local backend from path: /mnt/public/usr/sunzhichao/mmdetection/swin_tiny_patch4_window7_224.pth
Name of parameter - Initialization information

level_embed - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.patch_embed.projection.weight - torch.Size([96, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([288]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([96, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([384, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([96, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([288]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([96, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([384, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([96, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([192, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([384, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([768, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.conv.weight - torch.Size([256, 192, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.conv.weight - torch.Size([256, 384, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.conv.weight - torch.Size([256, 768, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.conv.weight - torch.Size([256, 768, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.extra_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.3.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.5.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.6.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.single_value_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

query_embedding.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.word_embeddings.weight - torch.Size([30522, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.position_embeddings.weight - torch.Size([512, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight - torch.Size([2, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.weight - torch.Size([256, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

dn_query_generator.label_embedding.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  
2025/10/29 08:25:39 - mmengine - INFO - Load checkpoint from /mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-t_numbranch_pretrain.pth
2025/10/29 08:25:39 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/10/29 08:25:39 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/10/29 08:25:39 - mmengine - INFO - Checkpoints will be saved to /mnt/public/usr/sunzhichao/mmdetection/work_dirs/epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2.
2025/10/29 08:27:16 - mmengine - INFO - Epoch(train) [1][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:27:04  time: 1.9383  data_time: 0.0239  memory: 16264  grad_norm: 44.5700  loss: 11.3749  loss_cls: 0.6551  loss_bbox: 0.0715  loss_iou: 0.4644  d0.loss_cls: 0.6669  d0.loss_bbox: 0.0786  d0.loss_iou: 0.4816  d1.loss_cls: 0.6656  d1.loss_bbox: 0.0750  d1.loss_iou: 0.4751  d2.loss_cls: 0.6597  d2.loss_bbox: 0.0710  d2.loss_iou: 0.4650  d3.loss_cls: 0.6493  d3.loss_bbox: 0.0744  d3.loss_iou: 0.4761  d4.loss_cls: 0.6480  d4.loss_bbox: 0.0753  d4.loss_iou: 0.4765  enc_loss_cls: 0.6496  enc_loss_bbox: 0.0795  enc_loss_iou: 0.4826  dn_loss_cls: 0.0432  dn_loss_bbox: 0.0479  dn_loss_iou: 0.3590  d0.dn_loss_cls: 0.0817  d0.dn_loss_bbox: 0.0667  d0.dn_loss_iou: 0.4687  d1.dn_loss_cls: 0.0510  d1.dn_loss_bbox: 0.0521  d1.dn_loss_iou: 0.3881  d2.dn_loss_cls: 0.0444  d2.dn_loss_bbox: 0.0488  d2.dn_loss_iou: 0.3648  d3.dn_loss_cls: 0.0451  d3.dn_loss_bbox: 0.0479  d3.dn_loss_iou: 0.3592  d4.dn_loss_cls: 0.0424  d4.dn_loss_bbox: 0.0478  d4.dn_loss_iou: 0.3587  loss_num: 0.0028  d0.loss_num: 0.0025  d1.loss_num: 0.0029  d2.loss_num: 0.0028  d3.loss_num: 0.0028  d4.loss_num: 0.0028
2025/10/29 08:28:46 - mmengine - INFO - Epoch(train) [1][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:13:09  time: 1.7917  data_time: 0.0173  memory: 16256  grad_norm: 38.8273  loss: 10.0024  loss_cls: 0.5427  loss_bbox: 0.0670  loss_iou: 0.4439  d0.loss_cls: 0.5697  d0.loss_bbox: 0.0668  d0.loss_iou: 0.4464  d1.loss_cls: 0.5571  d1.loss_bbox: 0.0683  d1.loss_iou: 0.4552  d2.loss_cls: 0.5502  d2.loss_bbox: 0.0684  d2.loss_iou: 0.4474  d3.loss_cls: 0.5472  d3.loss_bbox: 0.0660  d3.loss_iou: 0.4425  d4.loss_cls: 0.5422  d4.loss_bbox: 0.0683  d4.loss_iou: 0.4469  enc_loss_cls: 0.5700  enc_loss_bbox: 0.0702  enc_loss_iou: 0.4570  dn_loss_cls: 0.0298  dn_loss_bbox: 0.0388  dn_loss_iou: 0.3140  d0.dn_loss_cls: 0.0753  d0.dn_loss_bbox: 0.0555  d0.dn_loss_iou: 0.4142  d1.dn_loss_cls: 0.0387  d1.dn_loss_bbox: 0.0418  d1.dn_loss_iou: 0.3327  d2.dn_loss_cls: 0.0324  d2.dn_loss_bbox: 0.0395  d2.dn_loss_iou: 0.3183  d3.dn_loss_cls: 0.0303  d3.dn_loss_bbox: 0.0389  d3.dn_loss_iou: 0.3142  d4.dn_loss_cls: 0.0301  d4.dn_loss_bbox: 0.0388  d4.dn_loss_iou: 0.3138  loss_num: 0.0021  d0.loss_num: 0.0019  d1.loss_num: 0.0019  d2.loss_num: 0.0020  d3.loss_num: 0.0020  d4.loss_num: 0.0020
2025/10/29 08:30:16 - mmengine - INFO - Epoch(train) [1][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:07:48  time: 1.7967  data_time: 0.0173  memory: 16269  grad_norm: 39.1439  loss: 9.9035  loss_cls: 0.5645  loss_bbox: 0.0584  loss_iou: 0.4580  d0.loss_cls: 0.5966  d0.loss_bbox: 0.0636  d0.loss_iou: 0.4774  d1.loss_cls: 0.5837  d1.loss_bbox: 0.0610  d1.loss_iou: 0.4735  d2.loss_cls: 0.5751  d2.loss_bbox: 0.0593  d2.loss_iou: 0.4584  d3.loss_cls: 0.5707  d3.loss_bbox: 0.0581  d3.loss_iou: 0.4555  d4.loss_cls: 0.5634  d4.loss_bbox: 0.0589  d4.loss_iou: 0.4598  enc_loss_cls: 0.5971  enc_loss_bbox: 0.0648  enc_loss_iou: 0.4896  dn_loss_cls: 0.0211  dn_loss_bbox: 0.0327  dn_loss_iou: 0.2752  d0.dn_loss_cls: 0.0585  d0.dn_loss_bbox: 0.0465  d0.dn_loss_iou: 0.3655  d1.dn_loss_cls: 0.0289  d1.dn_loss_bbox: 0.0349  d1.dn_loss_iou: 0.2911  d2.dn_loss_cls: 0.0226  d2.dn_loss_bbox: 0.0331  d2.dn_loss_iou: 0.2786  d3.dn_loss_cls: 0.0215  d3.dn_loss_bbox: 0.0327  d3.dn_loss_iou: 0.2749  d4.dn_loss_cls: 0.0207  d4.dn_loss_bbox: 0.0327  d4.dn_loss_iou: 0.2748  loss_num: 0.0018  d0.loss_num: 0.0017  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0016
2025/10/29 08:31:47 - mmengine - INFO - Epoch(train) [1][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:05:22  time: 1.8207  data_time: 0.0162  memory: 16264  grad_norm: 43.8437  loss: 10.0845  loss_cls: 0.5199  loss_bbox: 0.0663  loss_iou: 0.4744  d0.loss_cls: 0.5560  d0.loss_bbox: 0.0677  d0.loss_iou: 0.4868  d1.loss_cls: 0.5509  d1.loss_bbox: 0.0656  d1.loss_iou: 0.4782  d2.loss_cls: 0.5331  d2.loss_bbox: 0.0657  d2.loss_iou: 0.4756  d3.loss_cls: 0.5263  d3.loss_bbox: 0.0659  d3.loss_iou: 0.4750  d4.loss_cls: 0.5201  d4.loss_bbox: 0.0665  d4.loss_iou: 0.4755  enc_loss_cls: 0.5641  enc_loss_bbox: 0.0696  enc_loss_iou: 0.5030  dn_loss_cls: 0.0218  dn_loss_bbox: 0.0377  dn_loss_iou: 0.3191  d0.dn_loss_cls: 0.0638  d0.dn_loss_bbox: 0.0547  d0.dn_loss_iou: 0.4230  d1.dn_loss_cls: 0.0286  d1.dn_loss_bbox: 0.0406  d1.dn_loss_iou: 0.3397  d2.dn_loss_cls: 0.0233  d2.dn_loss_bbox: 0.0382  d2.dn_loss_iou: 0.3220  d3.dn_loss_cls: 0.0219  d3.dn_loss_bbox: 0.0377  d3.dn_loss_iou: 0.3187  d4.dn_loss_cls: 0.0216  d4.dn_loss_bbox: 0.0377  d4.dn_loss_iou: 0.3187  loss_num: 0.0017  d0.loss_num: 0.0016  d1.loss_num: 0.0016  d2.loss_num: 0.0016  d3.loss_num: 0.0016  d4.loss_num: 0.0017
2025/10/29 08:33:17 - mmengine - INFO - Epoch(train) [1][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:02:31  time: 1.7972  data_time: 0.0165  memory: 16280  grad_norm: 34.0343  loss: 9.5936  loss_cls: 0.5192  loss_bbox: 0.0627  loss_iou: 0.4585  d0.loss_cls: 0.5537  d0.loss_bbox: 0.0618  d0.loss_iou: 0.4610  d1.loss_cls: 0.5384  d1.loss_bbox: 0.0626  d1.loss_iou: 0.4609  d2.loss_cls: 0.5282  d2.loss_bbox: 0.0643  d2.loss_iou: 0.4588  d3.loss_cls: 0.5223  d3.loss_bbox: 0.0622  d3.loss_iou: 0.4583  d4.loss_cls: 0.5187  d4.loss_bbox: 0.0629  d4.loss_iou: 0.4604  enc_loss_cls: 0.5596  enc_loss_bbox: 0.0633  enc_loss_iou: 0.4748  dn_loss_cls: 0.0197  dn_loss_bbox: 0.0327  dn_loss_iou: 0.2807  d0.dn_loss_cls: 0.0567  d0.dn_loss_bbox: 0.0464  d0.dn_loss_iou: 0.3700  d1.dn_loss_cls: 0.0262  d1.dn_loss_bbox: 0.0350  d1.dn_loss_iou: 0.2968  d2.dn_loss_cls: 0.0214  d2.dn_loss_bbox: 0.0333  d2.dn_loss_iou: 0.2849  d3.dn_loss_cls: 0.0203  d3.dn_loss_bbox: 0.0328  d3.dn_loss_iou: 0.2812  d4.dn_loss_cls: 0.0198  d4.dn_loss_bbox: 0.0327  d4.dn_loss_iou: 0.2806  loss_num: 0.0017  d0.loss_num: 0.0017  d1.loss_num: 0.0016  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0016
2025/10/29 08:34:46 - mmengine - INFO - Epoch(train) [1][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:00:02  time: 1.7935  data_time: 0.0168  memory: 16264  grad_norm: 38.8310  loss: 9.0106  loss_cls: 0.4778  loss_bbox: 0.0531  loss_iou: 0.4113  d0.loss_cls: 0.5216  d0.loss_bbox: 0.0557  d0.loss_iou: 0.4208  d1.loss_cls: 0.4975  d1.loss_bbox: 0.0569  d1.loss_iou: 0.4203  d2.loss_cls: 0.4882  d2.loss_bbox: 0.0540  d2.loss_iou: 0.4143  d3.loss_cls: 0.4837  d3.loss_bbox: 0.0539  d3.loss_iou: 0.4094  d4.loss_cls: 0.4776  d4.loss_bbox: 0.0542  d4.loss_iou: 0.4110  enc_loss_cls: 0.5254  enc_loss_bbox: 0.0548  enc_loss_iou: 0.4333  dn_loss_cls: 0.0155  dn_loss_bbox: 0.0331  dn_loss_iou: 0.2928  d0.dn_loss_cls: 0.0569  d0.dn_loss_bbox: 0.0472  d0.dn_loss_iou: 0.3832  d1.dn_loss_cls: 0.0229  d1.dn_loss_bbox: 0.0352  d1.dn_loss_iou: 0.3080  d2.dn_loss_cls: 0.0179  d2.dn_loss_bbox: 0.0335  d2.dn_loss_iou: 0.2959  d3.dn_loss_cls: 0.0165  d3.dn_loss_bbox: 0.0331  d3.dn_loss_iou: 0.2927  d4.dn_loss_cls: 0.0157  d4.dn_loss_bbox: 0.0331  d4.dn_loss_iou: 0.2926  loss_num: 0.0017  d0.loss_num: 0.0016  d1.loss_num: 0.0016  d2.loss_num: 0.0017  d3.loss_num: 0.0016  d4.loss_num: 0.0015
2025/10/29 08:36:16 - mmengine - INFO - Epoch(train) [1][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:57:55  time: 1.7977  data_time: 0.0172  memory: 16256  grad_norm: 44.9106  loss: 8.3907  loss_cls: 0.4299  loss_bbox: 0.0506  loss_iou: 0.3898  d0.loss_cls: 0.4648  d0.loss_bbox: 0.0512  d0.loss_iou: 0.3966  d1.loss_cls: 0.4508  d1.loss_bbox: 0.0498  d1.loss_iou: 0.3920  d2.loss_cls: 0.4338  d2.loss_bbox: 0.0522  d2.loss_iou: 0.3947  d3.loss_cls: 0.4307  d3.loss_bbox: 0.0527  d3.loss_iou: 0.3929  d4.loss_cls: 0.4324  d4.loss_bbox: 0.0492  d4.loss_iou: 0.3872  enc_loss_cls: 0.4725  enc_loss_bbox: 0.0511  enc_loss_iou: 0.4063  dn_loss_cls: 0.0147  dn_loss_bbox: 0.0348  dn_loss_iou: 0.2810  d0.dn_loss_cls: 0.0496  d0.dn_loss_bbox: 0.0498  d0.dn_loss_iou: 0.3721  d1.dn_loss_cls: 0.0211  d1.dn_loss_bbox: 0.0369  d1.dn_loss_iou: 0.2955  d2.dn_loss_cls: 0.0159  d2.dn_loss_bbox: 0.0353  d2.dn_loss_iou: 0.2833  d3.dn_loss_cls: 0.0152  d3.dn_loss_bbox: 0.0348  d3.dn_loss_iou: 0.2808  d4.dn_loss_cls: 0.0149  d4.dn_loss_bbox: 0.0348  d4.dn_loss_iou: 0.2807  loss_num: 0.0015  d0.loss_num: 0.0014  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2025/10/29 08:37:46 - mmengine - INFO - Epoch(train) [1][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:55:59  time: 1.7991  data_time: 0.0168  memory: 16268  grad_norm: 37.2519  loss: 8.8210  loss_cls: 0.4758  loss_bbox: 0.0562  loss_iou: 0.4015  d0.loss_cls: 0.5009  d0.loss_bbox: 0.0614  d0.loss_iou: 0.4216  d1.loss_cls: 0.4927  d1.loss_bbox: 0.0595  d1.loss_iou: 0.4124  d2.loss_cls: 0.4860  d2.loss_bbox: 0.0552  d2.loss_iou: 0.4006  d3.loss_cls: 0.4812  d3.loss_bbox: 0.0556  d3.loss_iou: 0.3972  d4.loss_cls: 0.4756  d4.loss_bbox: 0.0562  d4.loss_iou: 0.4014  enc_loss_cls: 0.5064  enc_loss_bbox: 0.0624  enc_loss_iou: 0.4288  dn_loss_cls: 0.0110  dn_loss_bbox: 0.0339  dn_loss_iou: 0.2814  d0.dn_loss_cls: 0.0462  d0.dn_loss_bbox: 0.0469  d0.dn_loss_iou: 0.3674  d1.dn_loss_cls: 0.0177  d1.dn_loss_bbox: 0.0360  d1.dn_loss_iou: 0.2975  d2.dn_loss_cls: 0.0127  d2.dn_loss_bbox: 0.0343  d2.dn_loss_iou: 0.2845  d3.dn_loss_cls: 0.0119  d3.dn_loss_bbox: 0.0339  d3.dn_loss_iou: 0.2815  d4.dn_loss_cls: 0.0112  d4.dn_loss_bbox: 0.0339  d4.dn_loss_iou: 0.2811  loss_num: 0.0017  d0.loss_num: 0.0016  d1.loss_num: 0.0016  d2.loss_num: 0.0016  d3.loss_num: 0.0016  d4.loss_num: 0.0015
2025/10/29 08:39:16 - mmengine - INFO - Epoch(train) [1][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:54:04  time: 1.7942  data_time: 0.0186  memory: 16269  grad_norm: 38.6622  loss: 9.1517  loss_cls: 0.4793  loss_bbox: 0.0549  loss_iou: 0.4417  d0.loss_cls: 0.5051  d0.loss_bbox: 0.0602  d0.loss_iou: 0.4675  d1.loss_cls: 0.4905  d1.loss_bbox: 0.0568  d1.loss_iou: 0.4590  d2.loss_cls: 0.4819  d2.loss_bbox: 0.0565  d2.loss_iou: 0.4486  d3.loss_cls: 0.4850  d3.loss_bbox: 0.0544  d3.loss_iou: 0.4394  d4.loss_cls: 0.4829  d4.loss_bbox: 0.0545  d4.loss_iou: 0.4402  enc_loss_cls: 0.5165  enc_loss_bbox: 0.0617  enc_loss_iou: 0.4879  dn_loss_cls: 0.0156  dn_loss_bbox: 0.0312  dn_loss_iou: 0.2787  d0.dn_loss_cls: 0.0499  d0.dn_loss_bbox: 0.0443  d0.dn_loss_iou: 0.3693  d1.dn_loss_cls: 0.0208  d1.dn_loss_bbox: 0.0334  d1.dn_loss_iou: 0.2958  d2.dn_loss_cls: 0.0160  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2820  d3.dn_loss_cls: 0.0156  d3.dn_loss_bbox: 0.0313  d3.dn_loss_iou: 0.2785  d4.dn_loss_cls: 0.0154  d4.dn_loss_bbox: 0.0312  d4.dn_loss_iou: 0.2784  loss_num: 0.0014  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2025/10/29 08:40:46 - mmengine - INFO - Epoch(train) [1][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:52:29  time: 1.8095  data_time: 0.0170  memory: 16291  grad_norm: 33.0687  loss: 8.7438  loss_cls: 0.4648  loss_bbox: 0.0521  loss_iou: 0.3821  d0.loss_cls: 0.4954  d0.loss_bbox: 0.0589  d0.loss_iou: 0.3999  d1.loss_cls: 0.4801  d1.loss_bbox: 0.0575  d1.loss_iou: 0.3915  d2.loss_cls: 0.4717  d2.loss_bbox: 0.0558  d2.loss_iou: 0.3856  d3.loss_cls: 0.4668  d3.loss_bbox: 0.0521  d3.loss_iou: 0.3829  d4.loss_cls: 0.4652  d4.loss_bbox: 0.0522  d4.loss_iou: 0.3820  enc_loss_cls: 0.5125  enc_loss_bbox: 0.0595  enc_loss_iou: 0.4104  dn_loss_cls: 0.0124  dn_loss_bbox: 0.0355  dn_loss_iou: 0.2974  d0.dn_loss_cls: 0.0521  d0.dn_loss_bbox: 0.0509  d0.dn_loss_iou: 0.3947  d1.dn_loss_cls: 0.0190  d1.dn_loss_bbox: 0.0381  d1.dn_loss_iou: 0.3140  d2.dn_loss_cls: 0.0142  d2.dn_loss_bbox: 0.0360  d2.dn_loss_iou: 0.3000  d3.dn_loss_cls: 0.0127  d3.dn_loss_bbox: 0.0355  d3.dn_loss_iou: 0.2974  d4.dn_loss_cls: 0.0124  d4.dn_loss_bbox: 0.0355  d4.dn_loss_iou: 0.2972  loss_num: 0.0016  d0.loss_num: 0.0016  d1.loss_num: 0.0016  d2.loss_num: 0.0017  d3.loss_num: 0.0016  d4.loss_num: 0.0015
2025/10/29 08:42:17 - mmengine - INFO - Epoch(train) [1][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:50:51  time: 1.8055  data_time: 0.0173  memory: 16265  grad_norm: 38.3254  loss: 9.0698  loss_cls: 0.4535  loss_bbox: 0.0540  loss_iou: 0.4525  d0.loss_cls: 0.4795  d0.loss_bbox: 0.0613  d0.loss_iou: 0.4804  d1.loss_cls: 0.4668  d1.loss_bbox: 0.0579  d1.loss_iou: 0.4679  d2.loss_cls: 0.4543  d2.loss_bbox: 0.0572  d2.loss_iou: 0.4638  d3.loss_cls: 0.4513  d3.loss_bbox: 0.0572  d3.loss_iou: 0.4641  d4.loss_cls: 0.4523  d4.loss_bbox: 0.0557  d4.loss_iou: 0.4576  enc_loss_cls: 0.4880  enc_loss_bbox: 0.0636  enc_loss_iou: 0.5009  dn_loss_cls: 0.0160  dn_loss_bbox: 0.0288  dn_loss_iou: 0.2799  d0.dn_loss_cls: 0.0522  d0.dn_loss_bbox: 0.0415  d0.dn_loss_iou: 0.3688  d1.dn_loss_cls: 0.0246  d1.dn_loss_bbox: 0.0308  d1.dn_loss_iou: 0.2965  d2.dn_loss_cls: 0.0190  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2837  d3.dn_loss_cls: 0.0179  d3.dn_loss_bbox: 0.0288  d3.dn_loss_iou: 0.2798  d4.dn_loss_cls: 0.0161  d4.dn_loss_bbox: 0.0288  d4.dn_loss_iou: 0.2796  loss_num: 0.0014  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0014  d4.loss_num: 0.0013
2025/10/29 08:43:46 - mmengine - INFO - Epoch(train) [1][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:49:03  time: 1.7915  data_time: 0.0171  memory: 16282  grad_norm: 44.2699  loss: 8.5664  loss_cls: 0.4318  loss_bbox: 0.0549  loss_iou: 0.3988  d0.loss_cls: 0.4544  d0.loss_bbox: 0.0595  d0.loss_iou: 0.4208  d1.loss_cls: 0.4438  d1.loss_bbox: 0.0555  d1.loss_iou: 0.4070  d2.loss_cls: 0.4379  d2.loss_bbox: 0.0550  d2.loss_iou: 0.4011  d3.loss_cls: 0.4359  d3.loss_bbox: 0.0550  d3.loss_iou: 0.3995  d4.loss_cls: 0.4319  d4.loss_bbox: 0.0548  d4.loss_iou: 0.3983  enc_loss_cls: 0.4592  enc_loss_bbox: 0.0622  enc_loss_iou: 0.4326  dn_loss_cls: 0.0113  dn_loss_bbox: 0.0348  dn_loss_iou: 0.2920  d0.dn_loss_cls: 0.0474  d0.dn_loss_bbox: 0.0487  d0.dn_loss_iou: 0.3888  d1.dn_loss_cls: 0.0188  d1.dn_loss_bbox: 0.0369  d1.dn_loss_iou: 0.3094  d2.dn_loss_cls: 0.0134  d2.dn_loss_bbox: 0.0352  d2.dn_loss_iou: 0.2962  d3.dn_loss_cls: 0.0119  d3.dn_loss_bbox: 0.0348  d3.dn_loss_iou: 0.2917  d4.dn_loss_cls: 0.0111  d4.dn_loss_bbox: 0.0348  d4.dn_loss_iou: 0.2915  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0012
2025/10/29 08:45:17 - mmengine - INFO - Epoch(train) [1][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:47:37  time: 1.8180  data_time: 0.0176  memory: 16257  grad_norm: 41.7280  loss: 7.8230  loss_cls: 0.4246  loss_bbox: 0.0458  loss_iou: 0.3484  d0.loss_cls: 0.4543  d0.loss_bbox: 0.0484  d0.loss_iou: 0.3630  d1.loss_cls: 0.4340  d1.loss_bbox: 0.0502  d1.loss_iou: 0.3661  d2.loss_cls: 0.4253  d2.loss_bbox: 0.0469  d2.loss_iou: 0.3524  d3.loss_cls: 0.4224  d3.loss_bbox: 0.0469  d3.loss_iou: 0.3529  d4.loss_cls: 0.4244  d4.loss_bbox: 0.0458  d4.loss_iou: 0.3485  enc_loss_cls: 0.4564  enc_loss_bbox: 0.0528  enc_loss_iou: 0.3843  dn_loss_cls: 0.0101  dn_loss_bbox: 0.0308  dn_loss_iou: 0.2544  d0.dn_loss_cls: 0.0440  d0.dn_loss_bbox: 0.0428  d0.dn_loss_iou: 0.3343  d1.dn_loss_cls: 0.0156  d1.dn_loss_bbox: 0.0326  d1.dn_loss_iou: 0.2679  d2.dn_loss_cls: 0.0111  d2.dn_loss_bbox: 0.0311  d2.dn_loss_iou: 0.2564  d3.dn_loss_cls: 0.0107  d3.dn_loss_bbox: 0.0308  d3.dn_loss_iou: 0.2540  d4.dn_loss_cls: 0.0101  d4.dn_loss_bbox: 0.0308  d4.dn_loss_iou: 0.2541  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0014  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0012
2025/10/29 08:46:47 - mmengine - INFO - Epoch(train) [1][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:46:02  time: 1.8050  data_time: 0.0172  memory: 16269  grad_norm: 33.4217  loss: 7.5775  loss_cls: 0.3894  loss_bbox: 0.0447  loss_iou: 0.3435  d0.loss_cls: 0.4121  d0.loss_bbox: 0.0478  d0.loss_iou: 0.3637  d1.loss_cls: 0.4041  d1.loss_bbox: 0.0462  d1.loss_iou: 0.3510  d2.loss_cls: 0.3930  d2.loss_bbox: 0.0461  d2.loss_iou: 0.3510  d3.loss_cls: 0.3945  d3.loss_bbox: 0.0437  d3.loss_iou: 0.3423  d4.loss_cls: 0.3890  d4.loss_bbox: 0.0448  d4.loss_iou: 0.3453  enc_loss_cls: 0.4179  enc_loss_bbox: 0.0518  enc_loss_iou: 0.3807  dn_loss_cls: 0.0130  dn_loss_bbox: 0.0308  dn_loss_iou: 0.2577  d0.dn_loss_cls: 0.0460  d0.dn_loss_bbox: 0.0447  d0.dn_loss_iou: 0.3432  d1.dn_loss_cls: 0.0186  d1.dn_loss_bbox: 0.0328  d1.dn_loss_iou: 0.2714  d2.dn_loss_cls: 0.0144  d2.dn_loss_bbox: 0.0313  d2.dn_loss_iou: 0.2606  d3.dn_loss_cls: 0.0136  d3.dn_loss_bbox: 0.0308  d3.dn_loss_iou: 0.2577  d4.dn_loss_cls: 0.0130  d4.dn_loss_bbox: 0.0308  d4.dn_loss_iou: 0.2574  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2025/10/29 08:48:17 - mmengine - INFO - Epoch(train) [1][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:44:19  time: 1.7923  data_time: 0.0172  memory: 16267  grad_norm: 35.8887  loss: 7.8972  loss_cls: 0.4017  loss_bbox: 0.0441  loss_iou: 0.3433  d0.loss_cls: 0.4234  d0.loss_bbox: 0.0467  d0.loss_iou: 0.3565  d1.loss_cls: 0.4132  d1.loss_bbox: 0.0460  d1.loss_iou: 0.3522  d2.loss_cls: 0.4028  d2.loss_bbox: 0.0440  d2.loss_iou: 0.3467  d3.loss_cls: 0.4025  d3.loss_bbox: 0.0453  d3.loss_iou: 0.3472  d4.loss_cls: 0.3993  d4.loss_bbox: 0.0468  d4.loss_iou: 0.3482  enc_loss_cls: 0.4352  enc_loss_bbox: 0.0488  enc_loss_iou: 0.3682  dn_loss_cls: 0.0225  dn_loss_bbox: 0.0339  dn_loss_iou: 0.2879  d0.dn_loss_cls: 0.0529  d0.dn_loss_bbox: 0.0472  d0.dn_loss_iou: 0.3778  d1.dn_loss_cls: 0.0277  d1.dn_loss_bbox: 0.0358  d1.dn_loss_iou: 0.3042  d2.dn_loss_cls: 0.0241  d2.dn_loss_bbox: 0.0342  d2.dn_loss_iou: 0.2906  d3.dn_loss_cls: 0.0232  d3.dn_loss_bbox: 0.0339  d3.dn_loss_iou: 0.2879  d4.dn_loss_cls: 0.0229  d4.dn_loss_bbox: 0.0339  d4.dn_loss_iou: 0.2877  loss_num: 0.0012  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2025/10/29 08:49:47 - mmengine - INFO - Epoch(train) [1][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:42:46  time: 1.8055  data_time: 0.0176  memory: 16262  grad_norm: 34.9628  loss: 8.4827  loss_cls: 0.4212  loss_bbox: 0.0488  loss_iou: 0.4048  d0.loss_cls: 0.4518  d0.loss_bbox: 0.0535  d0.loss_iou: 0.4252  d1.loss_cls: 0.4424  d1.loss_bbox: 0.0518  d1.loss_iou: 0.4159  d2.loss_cls: 0.4318  d2.loss_bbox: 0.0496  d2.loss_iou: 0.4101  d3.loss_cls: 0.4275  d3.loss_bbox: 0.0490  d3.loss_iou: 0.4095  d4.loss_cls: 0.4203  d4.loss_bbox: 0.0486  d4.loss_iou: 0.4066  enc_loss_cls: 0.4665  enc_loss_bbox: 0.0551  enc_loss_iou: 0.4390  dn_loss_cls: 0.0155  dn_loss_bbox: 0.0326  dn_loss_iou: 0.2809  d0.dn_loss_cls: 0.0493  d0.dn_loss_bbox: 0.0465  d0.dn_loss_iou: 0.3732  d1.dn_loss_cls: 0.0208  d1.dn_loss_bbox: 0.0352  d1.dn_loss_iou: 0.2991  d2.dn_loss_cls: 0.0166  d2.dn_loss_bbox: 0.0333  d2.dn_loss_iou: 0.2854  d3.dn_loss_cls: 0.0155  d3.dn_loss_bbox: 0.0327  d3.dn_loss_iou: 0.2813  d4.dn_loss_cls: 0.0155  d4.dn_loss_bbox: 0.0326  d4.dn_loss_iou: 0.2808  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2025/10/29 08:51:17 - mmengine - INFO - Epoch(train) [1][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:41:11  time: 1.8018  data_time: 0.0174  memory: 16268  grad_norm: 38.8419  loss: 7.6009  loss_cls: 0.3632  loss_bbox: 0.0451  loss_iou: 0.3546  d0.loss_cls: 0.3760  d0.loss_bbox: 0.0529  d0.loss_iou: 0.3854  d1.loss_cls: 0.3717  d1.loss_bbox: 0.0504  d1.loss_iou: 0.3699  d2.loss_cls: 0.3642  d2.loss_bbox: 0.0467  d2.loss_iou: 0.3643  d3.loss_cls: 0.3672  d3.loss_bbox: 0.0445  d3.loss_iou: 0.3526  d4.loss_cls: 0.3615  d4.loss_bbox: 0.0455  d4.loss_iou: 0.3564  enc_loss_cls: 0.3989  enc_loss_bbox: 0.0520  enc_loss_iou: 0.3898  dn_loss_cls: 0.0074  dn_loss_bbox: 0.0321  dn_loss_iou: 0.2766  d0.dn_loss_cls: 0.0438  d0.dn_loss_bbox: 0.0472  d0.dn_loss_iou: 0.3780  d1.dn_loss_cls: 0.0131  d1.dn_loss_bbox: 0.0342  d1.dn_loss_iou: 0.2947  d2.dn_loss_cls: 0.0088  d2.dn_loss_bbox: 0.0324  d2.dn_loss_iou: 0.2803  d3.dn_loss_cls: 0.0077  d3.dn_loss_bbox: 0.0321  d3.dn_loss_iou: 0.2767  d4.dn_loss_cls: 0.0074  d4.dn_loss_bbox: 0.0320  d4.dn_loss_iou: 0.2763  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0013  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0011
2025/10/29 08:52:47 - mmengine - INFO - Epoch(train) [1][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:39:33  time: 1.7949  data_time: 0.0174  memory: 16280  grad_norm: 48.3794  loss: 8.5566  loss_cls: 0.3898  loss_bbox: 0.0515  loss_iou: 0.4293  d0.loss_cls: 0.4204  d0.loss_bbox: 0.0566  d0.loss_iou: 0.4541  d1.loss_cls: 0.4076  d1.loss_bbox: 0.0527  d1.loss_iou: 0.4367  d2.loss_cls: 0.3952  d2.loss_bbox: 0.0512  d2.loss_iou: 0.4302  d3.loss_cls: 0.3953  d3.loss_bbox: 0.0509  d3.loss_iou: 0.4251  d4.loss_cls: 0.3876  d4.loss_bbox: 0.0527  d4.loss_iou: 0.4318  enc_loss_cls: 0.4411  enc_loss_bbox: 0.0606  enc_loss_iou: 0.4702  dn_loss_cls: 0.0170  dn_loss_bbox: 0.0332  dn_loss_iou: 0.2968  d0.dn_loss_cls: 0.0560  d0.dn_loss_bbox: 0.0470  d0.dn_loss_iou: 0.3919  d1.dn_loss_cls: 0.0242  d1.dn_loss_bbox: 0.0352  d1.dn_loss_iou: 0.3122  d2.dn_loss_cls: 0.0191  d2.dn_loss_bbox: 0.0336  d2.dn_loss_iou: 0.2989  d3.dn_loss_cls: 0.0177  d3.dn_loss_bbox: 0.0332  d3.dn_loss_iou: 0.2965  d4.dn_loss_cls: 0.0171  d4.dn_loss_bbox: 0.0332  d4.dn_loss_iou: 0.2966  loss_num: 0.0011  d0.loss_num: 0.0013  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0010
2025/10/29 08:54:17 - mmengine - INFO - Epoch(train) [1][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:37:58  time: 1.7975  data_time: 0.0174  memory: 16258  grad_norm: 41.7166  loss: 7.5024  loss_cls: 0.3790  loss_bbox: 0.0444  loss_iou: 0.3447  d0.loss_cls: 0.3917  d0.loss_bbox: 0.0493  d0.loss_iou: 0.3667  d1.loss_cls: 0.3848  d1.loss_bbox: 0.0489  d1.loss_iou: 0.3544  d2.loss_cls: 0.3791  d2.loss_bbox: 0.0448  d2.loss_iou: 0.3463  d3.loss_cls: 0.3798  d3.loss_bbox: 0.0441  d3.loss_iou: 0.3425  d4.loss_cls: 0.3795  d4.loss_bbox: 0.0441  d4.loss_iou: 0.3421  enc_loss_cls: 0.4132  enc_loss_bbox: 0.0501  enc_loss_iou: 0.3752  dn_loss_cls: 0.0078  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2662  d0.dn_loss_cls: 0.0382  d0.dn_loss_bbox: 0.0461  d0.dn_loss_iou: 0.3560  d1.dn_loss_cls: 0.0120  d1.dn_loss_bbox: 0.0333  d1.dn_loss_iou: 0.2798  d2.dn_loss_cls: 0.0090  d2.dn_loss_bbox: 0.0320  d2.dn_loss_iou: 0.2688  d3.dn_loss_cls: 0.0079  d3.dn_loss_bbox: 0.0316  d3.dn_loss_iou: 0.2663  d4.dn_loss_cls: 0.0077  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2659  loss_num: 0.0009  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2025/10/29 08:55:48 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 08:55:48 - mmengine - INFO - Epoch(train) [1][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:36:31  time: 1.8159  data_time: 0.0167  memory: 16247  grad_norm: 34.9002  loss: 7.8193  loss_cls: 0.3889  loss_bbox: 0.0430  loss_iou: 0.3694  d0.loss_cls: 0.4171  d0.loss_bbox: 0.0472  d0.loss_iou: 0.3917  d1.loss_cls: 0.3930  d1.loss_bbox: 0.0446  d1.loss_iou: 0.3808  d2.loss_cls: 0.3941  d2.loss_bbox: 0.0424  d2.loss_iou: 0.3704  d3.loss_cls: 0.3902  d3.loss_bbox: 0.0421  d3.loss_iou: 0.3711  d4.loss_cls: 0.3878  d4.loss_bbox: 0.0430  d4.loss_iou: 0.3699  enc_loss_cls: 0.4344  enc_loss_bbox: 0.0475  enc_loss_iou: 0.4051  dn_loss_cls: 0.0131  dn_loss_bbox: 0.0298  dn_loss_iou: 0.2692  d0.dn_loss_cls: 0.0451  d0.dn_loss_bbox: 0.0433  d0.dn_loss_iou: 0.3621  d1.dn_loss_cls: 0.0175  d1.dn_loss_bbox: 0.0321  d1.dn_loss_iou: 0.2854  d2.dn_loss_cls: 0.0143  d2.dn_loss_bbox: 0.0302  d2.dn_loss_iou: 0.2724  d3.dn_loss_cls: 0.0131  d3.dn_loss_bbox: 0.0299  d3.dn_loss_iou: 0.2696  d4.dn_loss_cls: 0.0133  d4.dn_loss_bbox: 0.0298  d4.dn_loss_iou: 0.2691  loss_num: 0.0010  d0.loss_num: 0.0013  d1.loss_num: 0.0011  d2.loss_num: 0.0010  d3.loss_num: 0.0009  d4.loss_num: 0.0010
2025/10/29 08:57:17 - mmengine - INFO - Epoch(train) [1][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:34:53  time: 1.7913  data_time: 0.0169  memory: 16256  grad_norm: 35.9026  loss: 7.4836  loss_cls: 0.3375  loss_bbox: 0.0461  loss_iou: 0.3672  d0.loss_cls: 0.3566  d0.loss_bbox: 0.0502  d0.loss_iou: 0.3871  d1.loss_cls: 0.3495  d1.loss_bbox: 0.0485  d1.loss_iou: 0.3729  d2.loss_cls: 0.3436  d2.loss_bbox: 0.0465  d2.loss_iou: 0.3693  d3.loss_cls: 0.3404  d3.loss_bbox: 0.0454  d3.loss_iou: 0.3661  d4.loss_cls: 0.3382  d4.loss_bbox: 0.0456  d4.loss_iou: 0.3658  enc_loss_cls: 0.3774  enc_loss_bbox: 0.0504  enc_loss_iou: 0.3924  dn_loss_cls: 0.0107  dn_loss_bbox: 0.0335  dn_loss_iou: 0.2744  d0.dn_loss_cls: 0.0443  d0.dn_loss_bbox: 0.0481  d0.dn_loss_iou: 0.3681  d1.dn_loss_cls: 0.0158  d1.dn_loss_bbox: 0.0358  d1.dn_loss_iou: 0.2912  d2.dn_loss_cls: 0.0120  d2.dn_loss_bbox: 0.0340  d2.dn_loss_iou: 0.2772  d3.dn_loss_cls: 0.0109  d3.dn_loss_bbox: 0.0335  d3.dn_loss_iou: 0.2745  d4.dn_loss_cls: 0.0106  d4.dn_loss_bbox: 0.0334  d4.dn_loss_iou: 0.2742  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 08:58:47 - mmengine - INFO - Epoch(train) [1][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:33:18  time: 1.7951  data_time: 0.0169  memory: 16259  grad_norm: 41.0905  loss: 6.9315  loss_cls: 0.3300  loss_bbox: 0.0397  loss_iou: 0.3353  d0.loss_cls: 0.3484  d0.loss_bbox: 0.0453  d0.loss_iou: 0.3577  d1.loss_cls: 0.3366  d1.loss_bbox: 0.0420  d1.loss_iou: 0.3445  d2.loss_cls: 0.3353  d2.loss_bbox: 0.0397  d2.loss_iou: 0.3370  d3.loss_cls: 0.3321  d3.loss_bbox: 0.0397  d3.loss_iou: 0.3356  d4.loss_cls: 0.3289  d4.loss_bbox: 0.0396  d4.loss_iou: 0.3363  enc_loss_cls: 0.3700  enc_loss_bbox: 0.0482  enc_loss_iou: 0.3787  dn_loss_cls: 0.0092  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2446  d0.dn_loss_cls: 0.0359  d0.dn_loss_bbox: 0.0384  d0.dn_loss_iou: 0.3238  d1.dn_loss_cls: 0.0135  d1.dn_loss_bbox: 0.0289  d1.dn_loss_iou: 0.2571  d2.dn_loss_cls: 0.0104  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2463  d3.dn_loss_cls: 0.0096  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2442  d4.dn_loss_cls: 0.0094  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2443  loss_num: 0.0009  d0.loss_num: 0.0010  d1.loss_num: 0.0010  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0008
2025/10/29 09:00:18 - mmengine - INFO - Epoch(train) [1][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:31:54  time: 1.8251  data_time: 0.0164  memory: 16256  grad_norm: 46.7550  loss: 7.4769  loss_cls: 0.3419  loss_bbox: 0.0472  loss_iou: 0.3602  d0.loss_cls: 0.3761  d0.loss_bbox: 0.0511  d0.loss_iou: 0.3818  d1.loss_cls: 0.3523  d1.loss_bbox: 0.0514  d1.loss_iou: 0.3745  d2.loss_cls: 0.3437  d2.loss_bbox: 0.0513  d2.loss_iou: 0.3725  d3.loss_cls: 0.3411  d3.loss_bbox: 0.0486  d3.loss_iou: 0.3662  d4.loss_cls: 0.3427  d4.loss_bbox: 0.0472  d4.loss_iou: 0.3597  enc_loss_cls: 0.3946  enc_loss_bbox: 0.0545  enc_loss_iou: 0.3972  dn_loss_cls: 0.0084  dn_loss_bbox: 0.0317  dn_loss_iou: 0.2692  d0.dn_loss_cls: 0.0438  d0.dn_loss_bbox: 0.0440  d0.dn_loss_iou: 0.3552  d1.dn_loss_cls: 0.0161  d1.dn_loss_bbox: 0.0336  d1.dn_loss_iou: 0.2832  d2.dn_loss_cls: 0.0105  d2.dn_loss_bbox: 0.0319  d2.dn_loss_iou: 0.2712  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.0316  d3.dn_loss_iou: 0.2684  d4.dn_loss_cls: 0.0087  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2687  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 09:01:48 - mmengine - INFO - Epoch(train) [1][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:30:17  time: 1.7882  data_time: 0.0168  memory: 16241  grad_norm: 38.2951  loss: 8.6048  loss_cls: 0.3808  loss_bbox: 0.0583  loss_iou: 0.4819  d0.loss_cls: 0.4202  d0.loss_bbox: 0.0639  d0.loss_iou: 0.5081  d1.loss_cls: 0.3978  d1.loss_bbox: 0.0603  d1.loss_iou: 0.4891  d2.loss_cls: 0.3911  d2.loss_bbox: 0.0584  d2.loss_iou: 0.4868  d3.loss_cls: 0.3822  d3.loss_bbox: 0.0586  d3.loss_iou: 0.4859  d4.loss_cls: 0.3799  d4.loss_bbox: 0.0589  d4.loss_iou: 0.4856  enc_loss_cls: 0.4345  enc_loss_bbox: 0.0678  enc_loss_iou: 0.5333  dn_loss_cls: 0.0090  dn_loss_bbox: 0.0288  dn_loss_iou: 0.2578  d0.dn_loss_cls: 0.0367  d0.dn_loss_bbox: 0.0401  d0.dn_loss_iou: 0.3369  d1.dn_loss_cls: 0.0139  d1.dn_loss_bbox: 0.0305  d1.dn_loss_iou: 0.2713  d2.dn_loss_cls: 0.0099  d2.dn_loss_bbox: 0.0291  d2.dn_loss_iou: 0.2603  d3.dn_loss_cls: 0.0092  d3.dn_loss_bbox: 0.0288  d3.dn_loss_iou: 0.2578  d4.dn_loss_cls: 0.0089  d4.dn_loss_bbox: 0.0288  d4.dn_loss_iou: 0.2576  loss_num: 0.0010  d0.loss_num: 0.0012  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2025/10/29 09:03:18 - mmengine - INFO - Epoch(train) [1][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:28:43  time: 1.7990  data_time: 0.0164  memory: 16262  grad_norm: 36.3004  loss: 6.9382  loss_cls: 0.2959  loss_bbox: 0.0395  loss_iou: 0.3361  d0.loss_cls: 0.3227  d0.loss_bbox: 0.0429  d0.loss_iou: 0.3495  d1.loss_cls: 0.3046  d1.loss_bbox: 0.0411  d1.loss_iou: 0.3416  d2.loss_cls: 0.3004  d2.loss_bbox: 0.0404  d2.loss_iou: 0.3400  d3.loss_cls: 0.3012  d3.loss_bbox: 0.0398  d3.loss_iou: 0.3353  d4.loss_cls: 0.2964  d4.loss_bbox: 0.0395  d4.loss_iou: 0.3360  enc_loss_cls: 0.3396  enc_loss_bbox: 0.0454  enc_loss_iou: 0.3656  dn_loss_cls: 0.0072  dn_loss_bbox: 0.0323  dn_loss_iou: 0.2805  d0.dn_loss_cls: 0.0397  d0.dn_loss_bbox: 0.0454  d0.dn_loss_iou: 0.3674  d1.dn_loss_cls: 0.0126  d1.dn_loss_bbox: 0.0343  d1.dn_loss_iou: 0.2954  d2.dn_loss_cls: 0.0086  d2.dn_loss_bbox: 0.0328  d2.dn_loss_iou: 0.2829  d3.dn_loss_cls: 0.0076  d3.dn_loss_bbox: 0.0323  d3.dn_loss_iou: 0.2805  d4.dn_loss_cls: 0.0073  d4.dn_loss_bbox: 0.0323  d4.dn_loss_iou: 0.2803  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 09:04:49 - mmengine - INFO - Epoch(train) [1][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:27:19  time: 1.8228  data_time: 0.0166  memory: 16274  grad_norm: 36.9098  loss: 7.7513  loss_cls: 0.3207  loss_bbox: 0.0556  loss_iou: 0.4284  d0.loss_cls: 0.3626  d0.loss_bbox: 0.0570  d0.loss_iou: 0.4354  d1.loss_cls: 0.3374  d1.loss_bbox: 0.0558  d1.loss_iou: 0.4285  d2.loss_cls: 0.3323  d2.loss_bbox: 0.0538  d2.loss_iou: 0.4230  d3.loss_cls: 0.3259  d3.loss_bbox: 0.0549  d3.loss_iou: 0.4265  d4.loss_cls: 0.3205  d4.loss_bbox: 0.0551  d4.loss_iou: 0.4270  enc_loss_cls: 0.3745  enc_loss_bbox: 0.0600  enc_loss_iou: 0.4535  dn_loss_cls: 0.0097  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2593  d0.dn_loss_cls: 0.0384  d0.dn_loss_bbox: 0.0442  d0.dn_loss_iou: 0.3424  d1.dn_loss_cls: 0.0158  d1.dn_loss_bbox: 0.0336  d1.dn_loss_iou: 0.2755  d2.dn_loss_cls: 0.0115  d2.dn_loss_bbox: 0.0320  d2.dn_loss_iou: 0.2625  d3.dn_loss_cls: 0.0101  d3.dn_loss_bbox: 0.0316  d3.dn_loss_iou: 0.2593  d4.dn_loss_cls: 0.0098  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2591  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 09:06:18 - mmengine - INFO - Epoch(train) [1][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:25:43  time: 1.7910  data_time: 0.0170  memory: 16256  grad_norm: 38.1772  loss: 7.1381  loss_cls: 0.2858  loss_bbox: 0.0422  loss_iou: 0.3562  d0.loss_cls: 0.3184  d0.loss_bbox: 0.0464  d0.loss_iou: 0.3744  d1.loss_cls: 0.3022  d1.loss_bbox: 0.0439  d1.loss_iou: 0.3643  d2.loss_cls: 0.2951  d2.loss_bbox: 0.0420  d2.loss_iou: 0.3556  d3.loss_cls: 0.2915  d3.loss_bbox: 0.0416  d3.loss_iou: 0.3518  d4.loss_cls: 0.2865  d4.loss_bbox: 0.0422  d4.loss_iou: 0.3564  enc_loss_cls: 0.3432  enc_loss_bbox: 0.0491  enc_loss_iou: 0.3941  dn_loss_cls: 0.0114  dn_loss_bbox: 0.0328  dn_loss_iou: 0.2843  d0.dn_loss_cls: 0.0454  d0.dn_loss_bbox: 0.0479  d0.dn_loss_iou: 0.3823  d1.dn_loss_cls: 0.0175  d1.dn_loss_bbox: 0.0353  d1.dn_loss_iou: 0.3028  d2.dn_loss_cls: 0.0128  d2.dn_loss_bbox: 0.0333  d2.dn_loss_iou: 0.2879  d3.dn_loss_cls: 0.0118  d3.dn_loss_bbox: 0.0328  d3.dn_loss_iou: 0.2845  d4.dn_loss_cls: 0.0115  d4.dn_loss_bbox: 0.0328  d4.dn_loss_iou: 0.2840  loss_num: 0.0007  d0.loss_num: 0.0007  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:07:48 - mmengine - INFO - Epoch(train) [1][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:24:09  time: 1.7948  data_time: 0.0168  memory: 16262  grad_norm: 37.1384  loss: 6.5659  loss_cls: 0.2819  loss_bbox: 0.0363  loss_iou: 0.3084  d0.loss_cls: 0.3090  d0.loss_bbox: 0.0381  d0.loss_iou: 0.3235  d1.loss_cls: 0.2921  d1.loss_bbox: 0.0377  d1.loss_iou: 0.3170  d2.loss_cls: 0.2856  d2.loss_bbox: 0.0368  d2.loss_iou: 0.3119  d3.loss_cls: 0.2869  d3.loss_bbox: 0.0362  d3.loss_iou: 0.3086  d4.loss_cls: 0.2857  d4.loss_bbox: 0.0360  d4.loss_iou: 0.3055  enc_loss_cls: 0.3312  enc_loss_bbox: 0.0428  enc_loss_iou: 0.3488  dn_loss_cls: 0.0116  dn_loss_bbox: 0.0314  dn_loss_iou: 0.2652  d0.dn_loss_cls: 0.0440  d0.dn_loss_bbox: 0.0438  d0.dn_loss_iou: 0.3491  d1.dn_loss_cls: 0.0166  d1.dn_loss_bbox: 0.0330  d1.dn_loss_iou: 0.2789  d2.dn_loss_cls: 0.0126  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2679  d3.dn_loss_cls: 0.0116  d3.dn_loss_bbox: 0.0314  d3.dn_loss_iou: 0.2656  d4.dn_loss_cls: 0.0115  d4.dn_loss_bbox: 0.0314  d4.dn_loss_iou: 0.2651  loss_num: 0.0006  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:09:19 - mmengine - INFO - Epoch(train) [1][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:22:42  time: 1.8189  data_time: 0.0178  memory: 16265  grad_norm: 35.5094  loss: 7.1495  loss_cls: 0.3221  loss_bbox: 0.0489  loss_iou: 0.3488  d0.loss_cls: 0.3449  d0.loss_bbox: 0.0530  d0.loss_iou: 0.3644  d1.loss_cls: 0.3296  d1.loss_bbox: 0.0511  d1.loss_iou: 0.3573  d2.loss_cls: 0.3231  d2.loss_bbox: 0.0499  d2.loss_iou: 0.3525  d3.loss_cls: 0.3223  d3.loss_bbox: 0.0486  d3.loss_iou: 0.3479  d4.loss_cls: 0.3222  d4.loss_bbox: 0.0489  d4.loss_iou: 0.3486  enc_loss_cls: 0.3612  enc_loss_bbox: 0.0547  enc_loss_iou: 0.3845  dn_loss_cls: 0.0087  dn_loss_bbox: 0.0329  dn_loss_iou: 0.2599  d0.dn_loss_cls: 0.0420  d0.dn_loss_bbox: 0.0461  d0.dn_loss_iou: 0.3416  d1.dn_loss_cls: 0.0141  d1.dn_loss_bbox: 0.0346  d1.dn_loss_iou: 0.2722  d2.dn_loss_cls: 0.0105  d2.dn_loss_bbox: 0.0332  d2.dn_loss_iou: 0.2619  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.0329  d3.dn_loss_iou: 0.2595  d4.dn_loss_cls: 0.0093  d4.dn_loss_bbox: 0.0328  d4.dn_loss_iou: 0.2594  loss_num: 0.0007  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0006
2025/10/29 09:10:49 - mmengine - INFO - Epoch(train) [1][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:21:10  time: 1.7986  data_time: 0.0173  memory: 16272  grad_norm: 39.5307  loss: 6.9209  loss_cls: 0.3021  loss_bbox: 0.0443  loss_iou: 0.3488  d0.loss_cls: 0.3329  d0.loss_bbox: 0.0503  d0.loss_iou: 0.3732  d1.loss_cls: 0.3149  d1.loss_bbox: 0.0464  d1.loss_iou: 0.3591  d2.loss_cls: 0.3079  d2.loss_bbox: 0.0459  d2.loss_iou: 0.3520  d3.loss_cls: 0.3047  d3.loss_bbox: 0.0458  d3.loss_iou: 0.3512  d4.loss_cls: 0.3043  d4.loss_bbox: 0.0443  d4.loss_iou: 0.3487  enc_loss_cls: 0.3578  enc_loss_bbox: 0.0528  enc_loss_iou: 0.3920  dn_loss_cls: 0.0126  dn_loss_bbox: 0.0287  dn_loss_iou: 0.2400  d0.dn_loss_cls: 0.0386  d0.dn_loss_bbox: 0.0402  d0.dn_loss_iou: 0.3196  d1.dn_loss_cls: 0.0183  d1.dn_loss_bbox: 0.0305  d1.dn_loss_iou: 0.2540  d2.dn_loss_cls: 0.0172  d2.dn_loss_bbox: 0.0292  d2.dn_loss_iou: 0.2432  d3.dn_loss_cls: 0.0145  d3.dn_loss_bbox: 0.0288  d3.dn_loss_iou: 0.2400  d4.dn_loss_cls: 0.0134  d4.dn_loss_bbox: 0.0287  d4.dn_loss_iou: 0.2398  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 09:12:19 - mmengine - INFO - Epoch(train) [1][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:19:35  time: 1.7913  data_time: 0.0166  memory: 16269  grad_norm: 32.9031  loss: 7.2325  loss_cls: 0.2877  loss_bbox: 0.0507  loss_iou: 0.3816  d0.loss_cls: 0.3194  d0.loss_bbox: 0.0569  d0.loss_iou: 0.4068  d1.loss_cls: 0.2985  d1.loss_bbox: 0.0557  d1.loss_iou: 0.3989  d2.loss_cls: 0.3003  d2.loss_bbox: 0.0516  d2.loss_iou: 0.3869  d3.loss_cls: 0.2929  d3.loss_bbox: 0.0506  d3.loss_iou: 0.3801  d4.loss_cls: 0.2863  d4.loss_bbox: 0.0506  d4.loss_iou: 0.3818  enc_loss_cls: 0.3390  enc_loss_bbox: 0.0582  enc_loss_iou: 0.4217  dn_loss_cls: 0.0089  dn_loss_bbox: 0.0327  dn_loss_iou: 0.2604  d0.dn_loss_cls: 0.0404  d0.dn_loss_bbox: 0.0460  d0.dn_loss_iou: 0.3446  d1.dn_loss_cls: 0.0147  d1.dn_loss_bbox: 0.0349  d1.dn_loss_iou: 0.2754  d2.dn_loss_cls: 0.0106  d2.dn_loss_bbox: 0.0334  d2.dn_loss_iou: 0.2648  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.0328  d3.dn_loss_iou: 0.2610  d4.dn_loss_cls: 0.0087  d4.dn_loss_bbox: 0.0328  d4.dn_loss_iou: 0.2604  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 09:13:50 - mmengine - INFO - Epoch(train) [1][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:18:08  time: 1.8180  data_time: 0.0165  memory: 16282  grad_norm: 36.7355  loss: 7.0047  loss_cls: 0.2793  loss_bbox: 0.0476  loss_iou: 0.3629  d0.loss_cls: 0.3104  d0.loss_bbox: 0.0482  d0.loss_iou: 0.3736  d1.loss_cls: 0.2978  d1.loss_bbox: 0.0476  d1.loss_iou: 0.3638  d2.loss_cls: 0.2875  d2.loss_bbox: 0.0470  d2.loss_iou: 0.3617  d3.loss_cls: 0.2850  d3.loss_bbox: 0.0471  d3.loss_iou: 0.3590  d4.loss_cls: 0.2810  d4.loss_bbox: 0.0472  d4.loss_iou: 0.3603  enc_loss_cls: 0.3343  enc_loss_bbox: 0.0520  enc_loss_iou: 0.3964  dn_loss_cls: 0.0152  dn_loss_bbox: 0.0315  dn_loss_iou: 0.2618  d0.dn_loss_cls: 0.0497  d0.dn_loss_bbox: 0.0450  d0.dn_loss_iou: 0.3465  d1.dn_loss_cls: 0.0217  d1.dn_loss_bbox: 0.0334  d1.dn_loss_iou: 0.2765  d2.dn_loss_cls: 0.0171  d2.dn_loss_bbox: 0.0319  d2.dn_loss_iou: 0.2649  d3.dn_loss_cls: 0.0150  d3.dn_loss_bbox: 0.0315  d3.dn_loss_iou: 0.2618  d4.dn_loss_cls: 0.0144  d4.dn_loss_bbox: 0.0314  d4.dn_loss_iou: 0.2615  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0006
2025/10/29 09:15:19 - mmengine - INFO - Epoch(train) [1][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:16:35  time: 1.7957  data_time: 0.0169  memory: 16273  grad_norm: 41.9324  loss: 7.3517  loss_cls: 0.3015  loss_bbox: 0.0455  loss_iou: 0.3829  d0.loss_cls: 0.3378  d0.loss_bbox: 0.0531  d0.loss_iou: 0.4101  d1.loss_cls: 0.3212  d1.loss_bbox: 0.0498  d1.loss_iou: 0.3943  d2.loss_cls: 0.3137  d2.loss_bbox: 0.0463  d2.loss_iou: 0.3845  d3.loss_cls: 0.3055  d3.loss_bbox: 0.0463  d3.loss_iou: 0.3855  d4.loss_cls: 0.3016  d4.loss_bbox: 0.0457  d4.loss_iou: 0.3831  enc_loss_cls: 0.3644  enc_loss_bbox: 0.0563  enc_loss_iou: 0.4299  dn_loss_cls: 0.0110  dn_loss_bbox: 0.0306  dn_loss_iou: 0.2642  d0.dn_loss_cls: 0.0410  d0.dn_loss_bbox: 0.0431  d0.dn_loss_iou: 0.3498  d1.dn_loss_cls: 0.0162  d1.dn_loss_bbox: 0.0325  d1.dn_loss_iou: 0.2783  d2.dn_loss_cls: 0.0126  d2.dn_loss_bbox: 0.0309  d2.dn_loss_iou: 0.2667  d3.dn_loss_cls: 0.0115  d3.dn_loss_bbox: 0.0306  d3.dn_loss_iou: 0.2640  d4.dn_loss_cls: 0.0109  d4.dn_loss_bbox: 0.0306  d4.dn_loss_iou: 0.2639  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 09:16:49 - mmengine - INFO - Epoch(train) [1][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:15:02  time: 1.7947  data_time: 0.0169  memory: 16264  grad_norm: 45.0828  loss: 6.7589  loss_cls: 0.2716  loss_bbox: 0.0422  loss_iou: 0.3483  d0.loss_cls: 0.3134  d0.loss_bbox: 0.0431  d0.loss_iou: 0.3584  d1.loss_cls: 0.2899  d1.loss_bbox: 0.0427  d1.loss_iou: 0.3566  d2.loss_cls: 0.2824  d2.loss_bbox: 0.0411  d2.loss_iou: 0.3472  d3.loss_cls: 0.2725  d3.loss_bbox: 0.0436  d3.loss_iou: 0.3521  d4.loss_cls: 0.2686  d4.loss_bbox: 0.0434  d4.loss_iou: 0.3502  enc_loss_cls: 0.3310  enc_loss_bbox: 0.0460  enc_loss_iou: 0.3776  dn_loss_cls: 0.0065  dn_loss_bbox: 0.0308  dn_loss_iou: 0.2600  d0.dn_loss_cls: 0.0344  d0.dn_loss_bbox: 0.0430  d0.dn_loss_iou: 0.3457  d1.dn_loss_cls: 0.0110  d1.dn_loss_bbox: 0.0323  d1.dn_loss_iou: 0.2729  d2.dn_loss_cls: 0.0081  d2.dn_loss_bbox: 0.0311  d2.dn_loss_iou: 0.2624  d3.dn_loss_cls: 0.0070  d3.dn_loss_bbox: 0.0308  d3.dn_loss_iou: 0.2599  d4.dn_loss_cls: 0.0066  d4.dn_loss_bbox: 0.0308  d4.dn_loss_iou: 0.2596  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:18:20 - mmengine - INFO - Epoch(train) [1][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:13:36  time: 1.8239  data_time: 0.0186  memory: 16268  grad_norm: 40.6668  loss: 6.8502  loss_cls: 0.2827  loss_bbox: 0.0448  loss_iou: 0.3539  d0.loss_cls: 0.3157  d0.loss_bbox: 0.0492  d0.loss_iou: 0.3730  d1.loss_cls: 0.3012  d1.loss_bbox: 0.0450  d1.loss_iou: 0.3630  d2.loss_cls: 0.2898  d2.loss_bbox: 0.0447  d2.loss_iou: 0.3557  d3.loss_cls: 0.2864  d3.loss_bbox: 0.0444  d3.loss_iou: 0.3519  d4.loss_cls: 0.2800  d4.loss_bbox: 0.0456  d4.loss_iou: 0.3584  enc_loss_cls: 0.3405  enc_loss_bbox: 0.0523  enc_loss_iou: 0.3953  dn_loss_cls: 0.0064  dn_loss_bbox: 0.0312  dn_loss_iou: 0.2491  d0.dn_loss_cls: 0.0335  d0.dn_loss_bbox: 0.0442  d0.dn_loss_iou: 0.3327  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0333  d1.dn_loss_iou: 0.2650  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2524  d3.dn_loss_cls: 0.0067  d3.dn_loss_bbox: 0.0312  d3.dn_loss_iou: 0.2493  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0312  d4.dn_loss_iou: 0.2489  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 09:19:50 - mmengine - INFO - Epoch(train) [1][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:12:05  time: 1.8018  data_time: 0.0169  memory: 16269  grad_norm: 38.6355  loss: 7.1884  loss_cls: 0.2863  loss_bbox: 0.0487  loss_iou: 0.3764  d0.loss_cls: 0.3122  d0.loss_bbox: 0.0542  d0.loss_iou: 0.4025  d1.loss_cls: 0.3015  d1.loss_bbox: 0.0488  d1.loss_iou: 0.3900  d2.loss_cls: 0.2910  d2.loss_bbox: 0.0465  d2.loss_iou: 0.3777  d3.loss_cls: 0.2889  d3.loss_bbox: 0.0462  d3.loss_iou: 0.3757  d4.loss_cls: 0.2876  d4.loss_bbox: 0.0458  d4.loss_iou: 0.3740  enc_loss_cls: 0.3269  enc_loss_bbox: 0.0569  enc_loss_iou: 0.4246  dn_loss_cls: 0.0079  dn_loss_bbox: 0.0337  dn_loss_iou: 0.2695  d0.dn_loss_cls: 0.0372  d0.dn_loss_bbox: 0.0467  d0.dn_loss_iou: 0.3541  d1.dn_loss_cls: 0.0129  d1.dn_loss_bbox: 0.0356  d1.dn_loss_iou: 0.2851  d2.dn_loss_cls: 0.0093  d2.dn_loss_bbox: 0.0341  d2.dn_loss_iou: 0.2725  d3.dn_loss_cls: 0.0085  d3.dn_loss_bbox: 0.0337  d3.dn_loss_iou: 0.2701  d4.dn_loss_cls: 0.0082  d4.dn_loss_bbox: 0.0337  d4.dn_loss_iou: 0.2693  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:21:20 - mmengine - INFO - Epoch(train) [1][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:10:33  time: 1.7985  data_time: 0.0168  memory: 16267  grad_norm: 37.6464  loss: 7.1342  loss_cls: 0.3038  loss_bbox: 0.0436  loss_iou: 0.3567  d0.loss_cls: 0.3401  d0.loss_bbox: 0.0483  d0.loss_iou: 0.3814  d1.loss_cls: 0.3200  d1.loss_bbox: 0.0445  d1.loss_iou: 0.3638  d2.loss_cls: 0.3056  d2.loss_bbox: 0.0462  d2.loss_iou: 0.3642  d3.loss_cls: 0.3050  d3.loss_bbox: 0.0443  d3.loss_iou: 0.3578  d4.loss_cls: 0.3021  d4.loss_bbox: 0.0449  d4.loss_iou: 0.3590  enc_loss_cls: 0.3562  enc_loss_bbox: 0.0515  enc_loss_iou: 0.4008  dn_loss_cls: 0.0090  dn_loss_bbox: 0.0312  dn_loss_iou: 0.2635  d0.dn_loss_cls: 0.0419  d0.dn_loss_bbox: 0.0449  d0.dn_loss_iou: 0.3547  d1.dn_loss_cls: 0.0151  d1.dn_loss_bbox: 0.0335  d1.dn_loss_iou: 0.2799  d2.dn_loss_cls: 0.0107  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2665  d3.dn_loss_cls: 0.0098  d3.dn_loss_bbox: 0.0312  d3.dn_loss_iou: 0.2634  d4.dn_loss_cls: 0.0090  d4.dn_loss_bbox: 0.0312  d4.dn_loss_iou: 0.2633  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:22:51 - mmengine - INFO - Epoch(train) [1][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:09:05  time: 1.8161  data_time: 0.0171  memory: 16281  grad_norm: 38.7933  loss: 7.6301  loss_cls: 0.2948  loss_bbox: 0.0507  loss_iou: 0.4333  d0.loss_cls: 0.3269  d0.loss_bbox: 0.0538  d0.loss_iou: 0.4541  d1.loss_cls: 0.3065  d1.loss_bbox: 0.0522  d1.loss_iou: 0.4380  d2.loss_cls: 0.3025  d2.loss_bbox: 0.0496  d2.loss_iou: 0.4311  d3.loss_cls: 0.2978  d3.loss_bbox: 0.0497  d3.loss_iou: 0.4313  d4.loss_cls: 0.2974  d4.loss_bbox: 0.0501  d4.loss_iou: 0.4307  enc_loss_cls: 0.3498  enc_loss_bbox: 0.0567  enc_loss_iou: 0.4771  dn_loss_cls: 0.0070  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2701  d0.dn_loss_cls: 0.0335  d0.dn_loss_bbox: 0.0418  d0.dn_loss_iou: 0.3542  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0320  d1.dn_loss_iou: 0.2844  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0306  d2.dn_loss_iou: 0.2730  d3.dn_loss_cls: 0.0072  d3.dn_loss_bbox: 0.0304  d3.dn_loss_iou: 0.2703  d4.dn_loss_cls: 0.0072  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2699  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:24:22 - mmengine - INFO - Epoch(train) [1][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:07:35  time: 1.8096  data_time: 0.0176  memory: 16269  grad_norm: 32.9564  loss: 7.5377  loss_cls: 0.2951  loss_bbox: 0.0502  loss_iou: 0.4250  d0.loss_cls: 0.3203  d0.loss_bbox: 0.0566  d0.loss_iou: 0.4467  d1.loss_cls: 0.3070  d1.loss_bbox: 0.0531  d1.loss_iou: 0.4343  d2.loss_cls: 0.3014  d2.loss_bbox: 0.0500  d2.loss_iou: 0.4202  d3.loss_cls: 0.2954  d3.loss_bbox: 0.0499  d3.loss_iou: 0.4242  d4.loss_cls: 0.2957  d4.loss_bbox: 0.0497  d4.loss_iou: 0.4231  enc_loss_cls: 0.3512  enc_loss_bbox: 0.0556  enc_loss_iou: 0.4550  dn_loss_cls: 0.0079  dn_loss_bbox: 0.0322  dn_loss_iou: 0.2634  d0.dn_loss_cls: 0.0357  d0.dn_loss_bbox: 0.0456  d0.dn_loss_iou: 0.3481  d1.dn_loss_cls: 0.0127  d1.dn_loss_bbox: 0.0346  d1.dn_loss_iou: 0.2787  d2.dn_loss_cls: 0.0090  d2.dn_loss_bbox: 0.0326  d2.dn_loss_iou: 0.2664  d3.dn_loss_cls: 0.0084  d3.dn_loss_bbox: 0.0322  d3.dn_loss_iou: 0.2634  d4.dn_loss_cls: 0.0080  d4.dn_loss_bbox: 0.0321  d4.dn_loss_iou: 0.2632  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:25:51 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 09:25:51 - mmengine - INFO - Epoch(train) [1][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:06:02  time: 1.7916  data_time: 0.0157  memory: 16256  grad_norm: 39.0266  loss: 6.3238  loss_cls: 0.2580  loss_bbox: 0.0389  loss_iou: 0.3223  d0.loss_cls: 0.2837  d0.loss_bbox: 0.0402  d0.loss_iou: 0.3347  d1.loss_cls: 0.2696  d1.loss_bbox: 0.0394  d1.loss_iou: 0.3273  d2.loss_cls: 0.2623  d2.loss_bbox: 0.0378  d2.loss_iou: 0.3209  d3.loss_cls: 0.2561  d3.loss_bbox: 0.0391  d3.loss_iou: 0.3251  d4.loss_cls: 0.2566  d4.loss_bbox: 0.0388  d4.loss_iou: 0.3223  enc_loss_cls: 0.3065  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3537  dn_loss_cls: 0.0065  dn_loss_bbox: 0.0307  dn_loss_iou: 0.2465  d0.dn_loss_cls: 0.0316  d0.dn_loss_bbox: 0.0431  d0.dn_loss_iou: 0.3295  d1.dn_loss_cls: 0.0096  d1.dn_loss_bbox: 0.0322  d1.dn_loss_iou: 0.2589  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0310  d2.dn_loss_iou: 0.2494  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0307  d3.dn_loss_iou: 0.2464  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0307  d4.dn_loss_iou: 0.2464  loss_num: 0.0007  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:26:54 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 09:26:54 - mmengine - INFO - Saving checkpoint at 1 epochs
2025/10/29 09:27:07 - mmengine - INFO - Epoch(val) [1][ 50/429]    eta: 0:00:38  time: 0.1004  data_time: 0.0069  memory: 16256  
2025/10/29 09:27:11 - mmengine - INFO - Epoch(val) [1][100/429]    eta: 0:00:31  time: 0.0890  data_time: 0.0024  memory: 3164  
2025/10/29 09:27:15 - mmengine - INFO - Epoch(val) [1][150/429]    eta: 0:00:25  time: 0.0879  data_time: 0.0023  memory: 3169  
2025/10/29 09:27:20 - mmengine - INFO - Epoch(val) [1][200/429]    eta: 0:00:20  time: 0.0876  data_time: 0.0024  memory: 3165  
2025/10/29 09:27:24 - mmengine - INFO - Epoch(val) [1][250/429]    eta: 0:00:16  time: 0.0873  data_time: 0.0023  memory: 3167  
2025/10/29 09:27:29 - mmengine - INFO - Epoch(val) [1][300/429]    eta: 0:00:11  time: 0.0869  data_time: 0.0023  memory: 3175  
2025/10/29 09:27:33 - mmengine - INFO - Epoch(val) [1][350/429]    eta: 0:00:07  time: 0.0872  data_time: 0.0022  memory: 3169  
2025/10/29 09:27:37 - mmengine - INFO - Epoch(val) [1][400/429]    eta: 0:00:02  time: 0.0862  data_time: 0.0023  memory: 3165  
2025/10/29 09:27:41 - mmengine - INFO - {'instance_F1_score': 0.4035862614542818, 'instance_acc': 0.2619513786914668, 'image_F1_score': 0.3780578206078577, 'image_acc': 0.2666083916083916}
2025/10/29 09:27:41 - mmengine - INFO - Epoch(val) [1][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.4036  grefcoco_val/refdrone/instance_acc: 0.2620  grefcoco_val/refdrone/image_F1_score: 0.3781  grefcoco_val/refdrone/image_acc: 0.2666  data_time: 0.0028  time: 0.0889
2025/10/29 09:29:12 - mmengine - INFO - Epoch(train) [2][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:03:29  time: 1.8146  data_time: 0.0171  memory: 16284  grad_norm: 40.3765  loss: 6.3947  loss_cls: 0.2400  loss_bbox: 0.0437  loss_iou: 0.3432  d0.loss_cls: 0.2877  d0.loss_bbox: 0.0441  d0.loss_iou: 0.3468  d1.loss_cls: 0.2579  d1.loss_bbox: 0.0446  d1.loss_iou: 0.3485  d2.loss_cls: 0.2499  d2.loss_bbox: 0.0440  d2.loss_iou: 0.3430  d3.loss_cls: 0.2438  d3.loss_bbox: 0.0438  d3.loss_iou: 0.3428  d4.loss_cls: 0.2428  d4.loss_bbox: 0.0435  d4.loss_iou: 0.3416  enc_loss_cls: 0.3080  enc_loss_bbox: 0.0476  enc_loss_iou: 0.3674  dn_loss_cls: 0.0087  dn_loss_bbox: 0.0304  dn_loss_iou: 0.2400  d0.dn_loss_cls: 0.0342  d0.dn_loss_bbox: 0.0432  d0.dn_loss_iou: 0.3192  d1.dn_loss_cls: 0.0129  d1.dn_loss_bbox: 0.0325  d1.dn_loss_iou: 0.2545  d2.dn_loss_cls: 0.0097  d2.dn_loss_bbox: 0.0309  d2.dn_loss_iou: 0.2426  d3.dn_loss_cls: 0.0088  d3.dn_loss_bbox: 0.0304  d3.dn_loss_iou: 0.2396  d4.dn_loss_cls: 0.0088  d4.dn_loss_bbox: 0.0304  d4.dn_loss_iou: 0.2397  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:30:42 - mmengine - INFO - Epoch(train) [2][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:01:56  time: 1.7940  data_time: 0.0162  memory: 16256  grad_norm: 37.9808  loss: 6.1282  loss_cls: 0.2107  loss_bbox: 0.0405  loss_iou: 0.3191  d0.loss_cls: 0.2469  d0.loss_bbox: 0.0427  d0.loss_iou: 0.3313  d1.loss_cls: 0.2331  d1.loss_bbox: 0.0418  d1.loss_iou: 0.3237  d2.loss_cls: 0.2194  d2.loss_bbox: 0.0415  d2.loss_iou: 0.3206  d3.loss_cls: 0.2134  d3.loss_bbox: 0.0413  d3.loss_iou: 0.3215  d4.loss_cls: 0.2106  d4.loss_bbox: 0.0413  d4.loss_iou: 0.3212  enc_loss_cls: 0.2761  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3457  dn_loss_cls: 0.0115  dn_loss_bbox: 0.0300  dn_loss_iou: 0.2576  d0.dn_loss_cls: 0.0438  d0.dn_loss_bbox: 0.0411  d0.dn_loss_iou: 0.3343  d1.dn_loss_cls: 0.0178  d1.dn_loss_bbox: 0.0317  d1.dn_loss_iou: 0.2699  d2.dn_loss_cls: 0.0126  d2.dn_loss_bbox: 0.0304  d2.dn_loss_iou: 0.2599  d3.dn_loss_cls: 0.0123  d3.dn_loss_bbox: 0.0301  d3.dn_loss_iou: 0.2574  d4.dn_loss_cls: 0.0112  d4.dn_loss_bbox: 0.0300  d4.dn_loss_iou: 0.2574  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:32:12 - mmengine - INFO - Epoch(train) [2][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:00:25  time: 1.7990  data_time: 0.0172  memory: 16269  grad_norm: 33.7933  loss: 6.7139  loss_cls: 0.2655  loss_bbox: 0.0430  loss_iou: 0.3424  d0.loss_cls: 0.2898  d0.loss_bbox: 0.0461  d0.loss_iou: 0.3598  d1.loss_cls: 0.2742  d1.loss_bbox: 0.0437  d1.loss_iou: 0.3471  d2.loss_cls: 0.2661  d2.loss_bbox: 0.0432  d2.loss_iou: 0.3453  d3.loss_cls: 0.2651  d3.loss_bbox: 0.0424  d3.loss_iou: 0.3411  d4.loss_cls: 0.2648  d4.loss_bbox: 0.0430  d4.loss_iou: 0.3423  enc_loss_cls: 0.3147  enc_loss_bbox: 0.0473  enc_loss_iou: 0.3751  dn_loss_cls: 0.0077  dn_loss_bbox: 0.0328  dn_loss_iou: 0.2673  d0.dn_loss_cls: 0.0393  d0.dn_loss_bbox: 0.0456  d0.dn_loss_iou: 0.3550  d1.dn_loss_cls: 0.0141  d1.dn_loss_bbox: 0.0346  d1.dn_loss_iou: 0.2831  d2.dn_loss_cls: 0.0094  d2.dn_loss_bbox: 0.0332  d2.dn_loss_iou: 0.2701  d3.dn_loss_cls: 0.0081  d3.dn_loss_bbox: 0.0328  d3.dn_loss_iou: 0.2675  d4.dn_loss_cls: 0.0078  d4.dn_loss_bbox: 0.0328  d4.dn_loss_iou: 0.2670  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:33:43 - mmengine - INFO - Epoch(train) [2][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:58:56  time: 1.8163  data_time: 0.0169  memory: 16269  grad_norm: 36.3266  loss: 6.1885  loss_cls: 0.2299  loss_bbox: 0.0410  loss_iou: 0.3115  d0.loss_cls: 0.2598  d0.loss_bbox: 0.0419  d0.loss_iou: 0.3214  d1.loss_cls: 0.2433  d1.loss_bbox: 0.0412  d1.loss_iou: 0.3152  d2.loss_cls: 0.2359  d2.loss_bbox: 0.0412  d2.loss_iou: 0.3147  d3.loss_cls: 0.2352  d3.loss_bbox: 0.0401  d3.loss_iou: 0.3093  d4.loss_cls: 0.2308  d4.loss_bbox: 0.0410  d4.loss_iou: 0.3113  enc_loss_cls: 0.2890  enc_loss_bbox: 0.0459  enc_loss_iou: 0.3463  dn_loss_cls: 0.0093  dn_loss_bbox: 0.0307  dn_loss_iou: 0.2585  d0.dn_loss_cls: 0.0364  d0.dn_loss_bbox: 0.0426  d0.dn_loss_iou: 0.3418  d1.dn_loss_cls: 0.0147  d1.dn_loss_bbox: 0.0325  d1.dn_loss_iou: 0.2725  d2.dn_loss_cls: 0.0111  d2.dn_loss_bbox: 0.0312  d2.dn_loss_iou: 0.2608  d3.dn_loss_cls: 0.0092  d3.dn_loss_bbox: 0.0308  d3.dn_loss_iou: 0.2587  d4.dn_loss_cls: 0.0094  d4.dn_loss_bbox: 0.0307  d4.dn_loss_iou: 0.2583  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:35:12 - mmengine - INFO - Epoch(train) [2][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:57:24  time: 1.7950  data_time: 0.0164  memory: 16258  grad_norm: 37.3807  loss: 6.7111  loss_cls: 0.2531  loss_bbox: 0.0468  loss_iou: 0.3681  d0.loss_cls: 0.2915  d0.loss_bbox: 0.0473  d0.loss_iou: 0.3867  d1.loss_cls: 0.2684  d1.loss_bbox: 0.0457  d1.loss_iou: 0.3758  d2.loss_cls: 0.2557  d2.loss_bbox: 0.0476  d2.loss_iou: 0.3744  d3.loss_cls: 0.2577  d3.loss_bbox: 0.0469  d3.loss_iou: 0.3700  d4.loss_cls: 0.2553  d4.loss_bbox: 0.0457  d4.loss_iou: 0.3679  enc_loss_cls: 0.3176  enc_loss_bbox: 0.0510  enc_loss_iou: 0.4063  dn_loss_cls: 0.0066  dn_loss_bbox: 0.0288  dn_loss_iou: 0.2442  d0.dn_loss_cls: 0.0381  d0.dn_loss_bbox: 0.0401  d0.dn_loss_iou: 0.3254  d1.dn_loss_cls: 0.0114  d1.dn_loss_bbox: 0.0306  d1.dn_loss_iou: 0.2587  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0292  d2.dn_loss_iou: 0.2474  d3.dn_loss_cls: 0.0067  d3.dn_loss_bbox: 0.0288  d3.dn_loss_iou: 0.2440  d4.dn_loss_cls: 0.0066  d4.dn_loss_bbox: 0.0288  d4.dn_loss_iou: 0.2440  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:36:42 - mmengine - INFO - Epoch(train) [2][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:55:52  time: 1.7931  data_time: 0.0169  memory: 16275  grad_norm: 35.4285  loss: 6.4193  loss_cls: 0.2267  loss_bbox: 0.0394  loss_iou: 0.3383  d0.loss_cls: 0.2574  d0.loss_bbox: 0.0415  d0.loss_iou: 0.3530  d1.loss_cls: 0.2348  d1.loss_bbox: 0.0404  d1.loss_iou: 0.3444  d2.loss_cls: 0.2307  d2.loss_bbox: 0.0391  d2.loss_iou: 0.3366  d3.loss_cls: 0.2295  d3.loss_bbox: 0.0387  d3.loss_iou: 0.3350  d4.loss_cls: 0.2283  d4.loss_bbox: 0.0393  d4.loss_iou: 0.3363  enc_loss_cls: 0.2799  enc_loss_bbox: 0.0464  enc_loss_iou: 0.3806  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0330  dn_loss_iou: 0.2728  d0.dn_loss_cls: 0.0335  d0.dn_loss_bbox: 0.0452  d0.dn_loss_iou: 0.3591  d1.dn_loss_cls: 0.0105  d1.dn_loss_bbox: 0.0348  d1.dn_loss_iou: 0.2867  d2.dn_loss_cls: 0.0068  d2.dn_loss_bbox: 0.0333  d2.dn_loss_iou: 0.2755  d3.dn_loss_cls: 0.0060  d3.dn_loss_bbox: 0.0330  d3.dn_loss_iou: 0.2727  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0330  d4.dn_loss_iou: 0.2726  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:38:13 - mmengine - INFO - Epoch(train) [2][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:54:24  time: 1.8180  data_time: 0.0167  memory: 16264  grad_norm: 36.6590  loss: 6.4983  loss_cls: 0.2497  loss_bbox: 0.0391  loss_iou: 0.3544  d0.loss_cls: 0.2850  d0.loss_bbox: 0.0418  d0.loss_iou: 0.3700  d1.loss_cls: 0.2650  d1.loss_bbox: 0.0407  d1.loss_iou: 0.3644  d2.loss_cls: 0.2548  d2.loss_bbox: 0.0399  d2.loss_iou: 0.3567  d3.loss_cls: 0.2543  d3.loss_bbox: 0.0389  d3.loss_iou: 0.3523  d4.loss_cls: 0.2510  d4.loss_bbox: 0.0390  d4.loss_iou: 0.3534  enc_loss_cls: 0.3047  enc_loss_bbox: 0.0466  enc_loss_iou: 0.3944  dn_loss_cls: 0.0081  dn_loss_bbox: 0.0283  dn_loss_iou: 0.2399  d0.dn_loss_cls: 0.0343  d0.dn_loss_bbox: 0.0402  d0.dn_loss_iou: 0.3172  d1.dn_loss_cls: 0.0121  d1.dn_loss_bbox: 0.0306  d1.dn_loss_iou: 0.2551  d2.dn_loss_cls: 0.0089  d2.dn_loss_bbox: 0.0288  d2.dn_loss_iou: 0.2431  d3.dn_loss_cls: 0.0083  d3.dn_loss_bbox: 0.0283  d3.dn_loss_iou: 0.2400  d4.dn_loss_cls: 0.0082  d4.dn_loss_bbox: 0.0282  d4.dn_loss_iou: 0.2398  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:39:43 - mmengine - INFO - Epoch(train) [2][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:52:52  time: 1.7944  data_time: 0.0168  memory: 16252  grad_norm: 38.1226  loss: 6.1726  loss_cls: 0.2058  loss_bbox: 0.0392  loss_iou: 0.3188  d0.loss_cls: 0.2393  d0.loss_bbox: 0.0416  d0.loss_iou: 0.3343  d1.loss_cls: 0.2220  d1.loss_bbox: 0.0406  d1.loss_iou: 0.3261  d2.loss_cls: 0.2155  d2.loss_bbox: 0.0398  d2.loss_iou: 0.3215  d3.loss_cls: 0.2106  d3.loss_bbox: 0.0394  d3.loss_iou: 0.3190  d4.loss_cls: 0.2055  d4.loss_bbox: 0.0392  d4.loss_iou: 0.3193  enc_loss_cls: 0.2697  enc_loss_bbox: 0.0438  enc_loss_iou: 0.3487  dn_loss_cls: 0.0066  dn_loss_bbox: 0.0319  dn_loss_iou: 0.2730  d0.dn_loss_cls: 0.0356  d0.dn_loss_bbox: 0.0452  d0.dn_loss_iou: 0.3637  d1.dn_loss_cls: 0.0110  d1.dn_loss_bbox: 0.0338  d1.dn_loss_iou: 0.2878  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0323  d2.dn_loss_iou: 0.2763  d3.dn_loss_cls: 0.0069  d3.dn_loss_bbox: 0.0320  d3.dn_loss_iou: 0.2731  d4.dn_loss_cls: 0.0066  d4.dn_loss_bbox: 0.0319  d4.dn_loss_iou: 0.2728  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 09:41:13 - mmengine - INFO - Epoch(train) [2][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:51:21  time: 1.8022  data_time: 0.0177  memory: 16275  grad_norm: 40.4394  loss: 5.7523  loss_cls: 0.2139  loss_bbox: 0.0359  loss_iou: 0.3013  d0.loss_cls: 0.2318  d0.loss_bbox: 0.0387  d0.loss_iou: 0.3169  d1.loss_cls: 0.2203  d1.loss_bbox: 0.0369  d1.loss_iou: 0.3090  d2.loss_cls: 0.2168  d2.loss_bbox: 0.0364  d2.loss_iou: 0.3045  d3.loss_cls: 0.2165  d3.loss_bbox: 0.0359  d3.loss_iou: 0.3007  d4.loss_cls: 0.2103  d4.loss_bbox: 0.0365  d4.loss_iou: 0.3047  enc_loss_cls: 0.2611  enc_loss_bbox: 0.0387  enc_loss_iou: 0.3243  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0288  dn_loss_iou: 0.2380  d0.dn_loss_cls: 0.0276  d0.dn_loss_bbox: 0.0396  d0.dn_loss_iou: 0.3131  d1.dn_loss_cls: 0.0085  d1.dn_loss_bbox: 0.0302  d1.dn_loss_iou: 0.2494  d2.dn_loss_cls: 0.0060  d2.dn_loss_bbox: 0.0289  d2.dn_loss_iou: 0.2397  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0287  d3.dn_loss_iou: 0.2374  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0287  d4.dn_loss_iou: 0.2376  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 09:42:44 - mmengine - INFO - Epoch(train) [2][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:49:53  time: 1.8173  data_time: 0.0172  memory: 16252  grad_norm: 40.4535  loss: 6.3495  loss_cls: 0.2185  loss_bbox: 0.0432  loss_iou: 0.3618  d0.loss_cls: 0.2488  d0.loss_bbox: 0.0470  d0.loss_iou: 0.3760  d1.loss_cls: 0.2359  d1.loss_bbox: 0.0437  d1.loss_iou: 0.3667  d2.loss_cls: 0.2264  d2.loss_bbox: 0.0435  d2.loss_iou: 0.3654  d3.loss_cls: 0.2200  d3.loss_bbox: 0.0436  d3.loss_iou: 0.3653  d4.loss_cls: 0.2190  d4.loss_bbox: 0.0428  d4.loss_iou: 0.3617  enc_loss_cls: 0.2695  enc_loss_bbox: 0.0484  enc_loss_iou: 0.3914  dn_loss_cls: 0.0046  dn_loss_bbox: 0.0306  dn_loss_iou: 0.2425  d0.dn_loss_cls: 0.0314  d0.dn_loss_bbox: 0.0431  d0.dn_loss_iou: 0.3202  d1.dn_loss_cls: 0.0089  d1.dn_loss_bbox: 0.0324  d1.dn_loss_iou: 0.2558  d2.dn_loss_cls: 0.0059  d2.dn_loss_bbox: 0.0310  d2.dn_loss_iou: 0.2451  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0306  d3.dn_loss_iou: 0.2426  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0306  d4.dn_loss_iou: 0.2423  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:44:14 - mmengine - INFO - Epoch(train) [2][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:48:22  time: 1.8009  data_time: 0.0174  memory: 16269  grad_norm: 33.9338  loss: 5.9324  loss_cls: 0.2078  loss_bbox: 0.0393  loss_iou: 0.3065  d0.loss_cls: 0.2311  d0.loss_bbox: 0.0437  d0.loss_iou: 0.3222  d1.loss_cls: 0.2151  d1.loss_bbox: 0.0404  d1.loss_iou: 0.3156  d2.loss_cls: 0.2091  d2.loss_bbox: 0.0418  d2.loss_iou: 0.3124  d3.loss_cls: 0.2119  d3.loss_bbox: 0.0397  d3.loss_iou: 0.3086  d4.loss_cls: 0.2084  d4.loss_bbox: 0.0393  d4.loss_iou: 0.3068  enc_loss_cls: 0.2557  enc_loss_bbox: 0.0435  enc_loss_iou: 0.3349  dn_loss_cls: 0.0050  dn_loss_bbox: 0.0309  dn_loss_iou: 0.2560  d0.dn_loss_cls: 0.0315  d0.dn_loss_bbox: 0.0434  d0.dn_loss_iou: 0.3392  d1.dn_loss_cls: 0.0088  d1.dn_loss_bbox: 0.0327  d1.dn_loss_iou: 0.2684  d2.dn_loss_cls: 0.0058  d2.dn_loss_bbox: 0.0311  d2.dn_loss_iou: 0.2582  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0309  d3.dn_loss_iou: 0.2560  d4.dn_loss_cls: 0.0051  d4.dn_loss_bbox: 0.0308  d4.dn_loss_iou: 0.2559  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 09:45:43 - mmengine - INFO - Epoch(train) [2][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:46:49  time: 1.7919  data_time: 0.0168  memory: 16251  grad_norm: 38.8191  loss: 6.1120  loss_cls: 0.2018  loss_bbox: 0.0424  loss_iou: 0.3288  d0.loss_cls: 0.2478  d0.loss_bbox: 0.0430  d0.loss_iou: 0.3340  d1.loss_cls: 0.2180  d1.loss_bbox: 0.0427  d1.loss_iou: 0.3303  d2.loss_cls: 0.2083  d2.loss_bbox: 0.0420  d2.loss_iou: 0.3273  d3.loss_cls: 0.2050  d3.loss_bbox: 0.0420  d3.loss_iou: 0.3276  d4.loss_cls: 0.2022  d4.loss_bbox: 0.0424  d4.loss_iou: 0.3285  enc_loss_cls: 0.2701  enc_loss_bbox: 0.0465  enc_loss_iou: 0.3521  dn_loss_cls: 0.0076  dn_loss_bbox: 0.0310  dn_loss_iou: 0.2579  d0.dn_loss_cls: 0.0338  d0.dn_loss_bbox: 0.0435  d0.dn_loss_iou: 0.3425  d1.dn_loss_cls: 0.0127  d1.dn_loss_bbox: 0.0327  d1.dn_loss_iou: 0.2714  d2.dn_loss_cls: 0.0089  d2.dn_loss_bbox: 0.0313  d2.dn_loss_iou: 0.2594  d3.dn_loss_cls: 0.0079  d3.dn_loss_bbox: 0.0310  d3.dn_loss_iou: 0.2578  d4.dn_loss_cls: 0.0076  d4.dn_loss_bbox: 0.0310  d4.dn_loss_iou: 0.2576  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:47:14 - mmengine - INFO - Epoch(train) [2][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:45:22  time: 1.8219  data_time: 0.0169  memory: 16257  grad_norm: 38.5279  loss: 6.0562  loss_cls: 0.2264  loss_bbox: 0.0393  loss_iou: 0.3204  d0.loss_cls: 0.2556  d0.loss_bbox: 0.0427  d0.loss_iou: 0.3398  d1.loss_cls: 0.2351  d1.loss_bbox: 0.0400  d1.loss_iou: 0.3270  d2.loss_cls: 0.2315  d2.loss_bbox: 0.0392  d2.loss_iou: 0.3219  d3.loss_cls: 0.2272  d3.loss_bbox: 0.0395  d3.loss_iou: 0.3220  d4.loss_cls: 0.2270  d4.loss_bbox: 0.0392  d4.loss_iou: 0.3199  enc_loss_cls: 0.2827  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3526  dn_loss_cls: 0.0074  dn_loss_bbox: 0.0280  dn_loss_iou: 0.2386  d0.dn_loss_cls: 0.0346  d0.dn_loss_bbox: 0.0387  d0.dn_loss_iou: 0.3148  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0297  d1.dn_loss_iou: 0.2519  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0283  d2.dn_loss_iou: 0.2407  d3.dn_loss_cls: 0.0076  d3.dn_loss_bbox: 0.0280  d3.dn_loss_iou: 0.2385  d4.dn_loss_cls: 0.0074  d4.dn_loss_bbox: 0.0280  d4.dn_loss_iou: 0.2384  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:48:44 - mmengine - INFO - Epoch(train) [2][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:43:50  time: 1.7965  data_time: 0.0168  memory: 16282  grad_norm: 36.4217  loss: 5.6868  loss_cls: 0.1915  loss_bbox: 0.0349  loss_iou: 0.2927  d0.loss_cls: 0.2241  d0.loss_bbox: 0.0374  d0.loss_iou: 0.3088  d1.loss_cls: 0.2022  d1.loss_bbox: 0.0362  d1.loss_iou: 0.3032  d2.loss_cls: 0.1955  d2.loss_bbox: 0.0354  d2.loss_iou: 0.2947  d3.loss_cls: 0.1943  d3.loss_bbox: 0.0346  d3.loss_iou: 0.2922  d4.loss_cls: 0.1915  d4.loss_bbox: 0.0351  d4.loss_iou: 0.2934  enc_loss_cls: 0.2544  enc_loss_bbox: 0.0415  enc_loss_iou: 0.3258  dn_loss_cls: 0.0067  dn_loss_bbox: 0.0294  dn_loss_iou: 0.2499  d0.dn_loss_cls: 0.0327  d0.dn_loss_bbox: 0.0416  d0.dn_loss_iou: 0.3337  d1.dn_loss_cls: 0.0110  d1.dn_loss_bbox: 0.0312  d1.dn_loss_iou: 0.2640  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0298  d2.dn_loss_iou: 0.2527  d3.dn_loss_cls: 0.0075  d3.dn_loss_bbox: 0.0294  d3.dn_loss_iou: 0.2502  d4.dn_loss_cls: 0.0070  d4.dn_loss_bbox: 0.0293  d4.dn_loss_iou: 0.2498  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:50:14 - mmengine - INFO - Epoch(train) [2][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:42:19  time: 1.7977  data_time: 0.0172  memory: 16247  grad_norm: 43.5693  loss: 6.0645  loss_cls: 0.1988  loss_bbox: 0.0401  loss_iou: 0.3470  d0.loss_cls: 0.2357  d0.loss_bbox: 0.0434  d0.loss_iou: 0.3679  d1.loss_cls: 0.2132  d1.loss_bbox: 0.0415  d1.loss_iou: 0.3553  d2.loss_cls: 0.2087  d2.loss_bbox: 0.0396  d2.loss_iou: 0.3457  d3.loss_cls: 0.2023  d3.loss_bbox: 0.0401  d3.loss_iou: 0.3487  d4.loss_cls: 0.1979  d4.loss_bbox: 0.0399  d4.loss_iou: 0.3475  enc_loss_cls: 0.2594  enc_loss_bbox: 0.0469  enc_loss_iou: 0.3930  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2347  d0.dn_loss_cls: 0.0287  d0.dn_loss_bbox: 0.0397  d0.dn_loss_iou: 0.3161  d1.dn_loss_cls: 0.0093  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2489  d2.dn_loss_cls: 0.0068  d2.dn_loss_bbox: 0.0279  d2.dn_loss_iou: 0.2374  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0275  d3.dn_loss_iou: 0.2344  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0275  d4.dn_loss_iou: 0.2344  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:51:44 - mmengine - INFO - Epoch(train) [2][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:40:49  time: 1.8081  data_time: 0.0168  memory: 16269  grad_norm: 36.3774  loss: 5.9450  loss_cls: 0.2034  loss_bbox: 0.0349  loss_iou: 0.3142  d0.loss_cls: 0.2349  d0.loss_bbox: 0.0393  d0.loss_iou: 0.3321  d1.loss_cls: 0.2205  d1.loss_bbox: 0.0349  d1.loss_iou: 0.3163  d2.loss_cls: 0.2085  d2.loss_bbox: 0.0350  d2.loss_iou: 0.3164  d3.loss_cls: 0.2057  d3.loss_bbox: 0.0344  d3.loss_iou: 0.3138  d4.loss_cls: 0.2036  d4.loss_bbox: 0.0349  d4.loss_iou: 0.3140  enc_loss_cls: 0.2523  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3579  dn_loss_cls: 0.0064  dn_loss_bbox: 0.0285  dn_loss_iou: 0.2571  d0.dn_loss_cls: 0.0308  d0.dn_loss_bbox: 0.0397  d0.dn_loss_iou: 0.3388  d1.dn_loss_cls: 0.0097  d1.dn_loss_bbox: 0.0301  d1.dn_loss_iou: 0.2704  d2.dn_loss_cls: 0.0072  d2.dn_loss_bbox: 0.0288  d2.dn_loss_iou: 0.2596  d3.dn_loss_cls: 0.0066  d3.dn_loss_bbox: 0.0286  d3.dn_loss_iou: 0.2572  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0285  d4.dn_loss_iou: 0.2570  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:53:14 - mmengine - INFO - Epoch(train) [2][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:39:18  time: 1.7987  data_time: 0.0180  memory: 16285  grad_norm: 35.5492  loss: 6.6319  loss_cls: 0.2181  loss_bbox: 0.0437  loss_iou: 0.3695  d0.loss_cls: 0.2621  d0.loss_bbox: 0.0456  d0.loss_iou: 0.3850  d1.loss_cls: 0.2357  d1.loss_bbox: 0.0448  d1.loss_iou: 0.3752  d2.loss_cls: 0.2283  d2.loss_bbox: 0.0433  d2.loss_iou: 0.3674  d3.loss_cls: 0.2190  d3.loss_bbox: 0.0447  d3.loss_iou: 0.3686  d4.loss_cls: 0.2179  d4.loss_bbox: 0.0437  d4.loss_iou: 0.3690  enc_loss_cls: 0.2814  enc_loss_bbox: 0.0515  enc_loss_iou: 0.4078  dn_loss_cls: 0.0094  dn_loss_bbox: 0.0314  dn_loss_iou: 0.2669  d0.dn_loss_cls: 0.0411  d0.dn_loss_bbox: 0.0444  d0.dn_loss_iou: 0.3551  d1.dn_loss_cls: 0.0146  d1.dn_loss_bbox: 0.0334  d1.dn_loss_iou: 0.2822  d2.dn_loss_cls: 0.0106  d2.dn_loss_bbox: 0.0318  d2.dn_loss_iou: 0.2698  d3.dn_loss_cls: 0.0095  d3.dn_loss_bbox: 0.0314  d3.dn_loss_iou: 0.2670  d4.dn_loss_cls: 0.0094  d4.dn_loss_bbox: 0.0314  d4.dn_loss_iou: 0.2667  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0005
2025/10/29 09:54:44 - mmengine - INFO - Epoch(train) [2][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:37:47  time: 1.7997  data_time: 0.0172  memory: 16269  grad_norm: 37.9264  loss: 6.4881  loss_cls: 0.1977  loss_bbox: 0.0424  loss_iou: 0.3833  d0.loss_cls: 0.2321  d0.loss_bbox: 0.0461  d0.loss_iou: 0.4025  d1.loss_cls: 0.2206  d1.loss_bbox: 0.0439  d1.loss_iou: 0.3863  d2.loss_cls: 0.2066  d2.loss_bbox: 0.0427  d2.loss_iou: 0.3831  d3.loss_cls: 0.2001  d3.loss_bbox: 0.0424  d3.loss_iou: 0.3843  d4.loss_cls: 0.1962  d4.loss_bbox: 0.0425  d4.loss_iou: 0.3829  enc_loss_cls: 0.2516  enc_loss_bbox: 0.0490  enc_loss_iou: 0.4214  dn_loss_cls: 0.0090  dn_loss_bbox: 0.0282  dn_loss_iou: 0.2608  d0.dn_loss_cls: 0.0328  d0.dn_loss_bbox: 0.0398  d0.dn_loss_iou: 0.3456  d1.dn_loss_cls: 0.0122  d1.dn_loss_bbox: 0.0297  d1.dn_loss_iou: 0.2737  d2.dn_loss_cls: 0.0092  d2.dn_loss_bbox: 0.0285  d2.dn_loss_iou: 0.2630  d3.dn_loss_cls: 0.0087  d3.dn_loss_bbox: 0.0282  d3.dn_loss_iou: 0.2605  d4.dn_loss_cls: 0.0086  d4.dn_loss_bbox: 0.0282  d4.dn_loss_iou: 0.2605  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:56:15 - mmengine - INFO - Epoch(train) [2][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:36:17  time: 1.8097  data_time: 0.0164  memory: 16257  grad_norm: 41.5436  loss: 6.3409  loss_cls: 0.2110  loss_bbox: 0.0417  loss_iou: 0.3495  d0.loss_cls: 0.2357  d0.loss_bbox: 0.0453  d0.loss_iou: 0.3699  d1.loss_cls: 0.2234  d1.loss_bbox: 0.0432  d1.loss_iou: 0.3600  d2.loss_cls: 0.2198  d2.loss_bbox: 0.0419  d2.loss_iou: 0.3489  d3.loss_cls: 0.2159  d3.loss_bbox: 0.0417  d3.loss_iou: 0.3483  d4.loss_cls: 0.2119  d4.loss_bbox: 0.0418  d4.loss_iou: 0.3492  enc_loss_cls: 0.2612  enc_loss_bbox: 0.0494  enc_loss_iou: 0.3907  dn_loss_cls: 0.0079  dn_loss_bbox: 0.0310  dn_loss_iou: 0.2603  d0.dn_loss_cls: 0.0377  d0.dn_loss_bbox: 0.0434  d0.dn_loss_iou: 0.3383  d1.dn_loss_cls: 0.0122  d1.dn_loss_bbox: 0.0329  d1.dn_loss_iou: 0.2736  d2.dn_loss_cls: 0.0087  d2.dn_loss_bbox: 0.0314  d2.dn_loss_iou: 0.2622  d3.dn_loss_cls: 0.0083  d3.dn_loss_bbox: 0.0311  d3.dn_loss_iou: 0.2599  d4.dn_loss_cls: 0.0079  d4.dn_loss_bbox: 0.0310  d4.dn_loss_iou: 0.2600  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 09:56:42 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 09:57:45 - mmengine - INFO - Epoch(train) [2][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:34:47  time: 1.8020  data_time: 0.0168  memory: 16259  grad_norm: 34.9637  loss: 6.8502  loss_cls: 0.2295  loss_bbox: 0.0465  loss_iou: 0.3965  d0.loss_cls: 0.2687  d0.loss_bbox: 0.0498  d0.loss_iou: 0.4149  d1.loss_cls: 0.2507  d1.loss_bbox: 0.0471  d1.loss_iou: 0.4009  d2.loss_cls: 0.2394  d2.loss_bbox: 0.0469  d2.loss_iou: 0.3977  d3.loss_cls: 0.2321  d3.loss_bbox: 0.0469  d3.loss_iou: 0.3989  d4.loss_cls: 0.2300  d4.loss_bbox: 0.0467  d4.loss_iou: 0.3972  enc_loss_cls: 0.2984  enc_loss_bbox: 0.0530  enc_loss_iou: 0.4329  dn_loss_cls: 0.0078  dn_loss_bbox: 0.0314  dn_loss_iou: 0.2559  d0.dn_loss_cls: 0.0375  d0.dn_loss_bbox: 0.0436  d0.dn_loss_iou: 0.3373  d1.dn_loss_cls: 0.0138  d1.dn_loss_bbox: 0.0333  d1.dn_loss_iou: 0.2706  d2.dn_loss_cls: 0.0098  d2.dn_loss_bbox: 0.0318  d2.dn_loss_iou: 0.2586  d3.dn_loss_cls: 0.0086  d3.dn_loss_bbox: 0.0314  d3.dn_loss_iou: 0.2557  d4.dn_loss_cls: 0.0078  d4.dn_loss_bbox: 0.0314  d4.dn_loss_iou: 0.2557  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 09:59:15 - mmengine - INFO - Epoch(train) [2][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:33:15  time: 1.7924  data_time: 0.0162  memory: 16247  grad_norm: 35.7321  loss: 6.2405  loss_cls: 0.2107  loss_bbox: 0.0407  loss_iou: 0.3428  d0.loss_cls: 0.2542  d0.loss_bbox: 0.0426  d0.loss_iou: 0.3557  d1.loss_cls: 0.2226  d1.loss_bbox: 0.0422  d1.loss_iou: 0.3526  d2.loss_cls: 0.2175  d2.loss_bbox: 0.0407  d2.loss_iou: 0.3447  d3.loss_cls: 0.2130  d3.loss_bbox: 0.0401  d3.loss_iou: 0.3421  d4.loss_cls: 0.2099  d4.loss_bbox: 0.0407  d4.loss_iou: 0.3426  enc_loss_cls: 0.2755  enc_loss_bbox: 0.0445  enc_loss_iou: 0.3712  dn_loss_cls: 0.0081  dn_loss_bbox: 0.0286  dn_loss_iou: 0.2545  d0.dn_loss_cls: 0.0351  d0.dn_loss_bbox: 0.0399  d0.dn_loss_iou: 0.3356  d1.dn_loss_cls: 0.0122  d1.dn_loss_bbox: 0.0302  d1.dn_loss_iou: 0.2683  d2.dn_loss_cls: 0.0096  d2.dn_loss_bbox: 0.0290  d2.dn_loss_iou: 0.2579  d3.dn_loss_cls: 0.0076  d3.dn_loss_bbox: 0.0286  d3.dn_loss_iou: 0.2546  d4.dn_loss_cls: 0.0076  d4.dn_loss_bbox: 0.0286  d4.dn_loss_iou: 0.2543  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:00:45 - mmengine - INFO - Epoch(train) [2][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:31:45  time: 1.8091  data_time: 0.0168  memory: 16280  grad_norm: 39.6602  loss: 5.0919  loss_cls: 0.1649  loss_bbox: 0.0328  loss_iou: 0.2699  d0.loss_cls: 0.1891  d0.loss_bbox: 0.0364  d0.loss_iou: 0.2902  d1.loss_cls: 0.1773  d1.loss_bbox: 0.0345  d1.loss_iou: 0.2782  d2.loss_cls: 0.1691  d2.loss_bbox: 0.0333  d2.loss_iou: 0.2732  d3.loss_cls: 0.1674  d3.loss_bbox: 0.0329  d3.loss_iou: 0.2710  d4.loss_cls: 0.1655  d4.loss_bbox: 0.0328  d4.loss_iou: 0.2709  enc_loss_cls: 0.2299  enc_loss_bbox: 0.0389  enc_loss_iou: 0.3058  dn_loss_cls: 0.0039  dn_loss_bbox: 0.0266  dn_loss_iou: 0.2209  d0.dn_loss_cls: 0.0243  d0.dn_loss_bbox: 0.0367  d0.dn_loss_iou: 0.2885  d1.dn_loss_cls: 0.0070  d1.dn_loss_bbox: 0.0280  d1.dn_loss_iou: 0.2314  d2.dn_loss_cls: 0.0047  d2.dn_loss_bbox: 0.0269  d2.dn_loss_iou: 0.2232  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0266  d3.dn_loss_iou: 0.2209  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.0266  d4.dn_loss_iou: 0.2207  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:02:16 - mmengine - INFO - Epoch(train) [2][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:30:16  time: 1.8105  data_time: 0.0169  memory: 16262  grad_norm: 38.0253  loss: 5.4777  loss_cls: 0.1784  loss_bbox: 0.0337  loss_iou: 0.2843  d0.loss_cls: 0.2111  d0.loss_bbox: 0.0372  d0.loss_iou: 0.3023  d1.loss_cls: 0.1914  d1.loss_bbox: 0.0352  d1.loss_iou: 0.2949  d2.loss_cls: 0.1844  d2.loss_bbox: 0.0341  d2.loss_iou: 0.2875  d3.loss_cls: 0.1796  d3.loss_bbox: 0.0339  d3.loss_iou: 0.2858  d4.loss_cls: 0.1801  d4.loss_bbox: 0.0337  d4.loss_iou: 0.2839  enc_loss_cls: 0.2392  enc_loss_bbox: 0.0377  enc_loss_iou: 0.3129  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0291  dn_loss_iou: 0.2439  d0.dn_loss_cls: 0.0328  d0.dn_loss_bbox: 0.0400  d0.dn_loss_iou: 0.3223  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0308  d1.dn_loss_iou: 0.2578  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2465  d3.dn_loss_cls: 0.0061  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2439  d4.dn_loss_cls: 0.0061  d4.dn_loss_bbox: 0.0291  d4.dn_loss_iou: 0.2437  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:03:45 - mmengine - INFO - Epoch(train) [2][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:28:44  time: 1.7932  data_time: 0.0169  memory: 16264  grad_norm: 36.6627  loss: 6.2121  loss_cls: 0.2071  loss_bbox: 0.0392  loss_iou: 0.3296  d0.loss_cls: 0.2372  d0.loss_bbox: 0.0404  d0.loss_iou: 0.3368  d1.loss_cls: 0.2238  d1.loss_bbox: 0.0379  d1.loss_iou: 0.3263  d2.loss_cls: 0.2144  d2.loss_bbox: 0.0384  d2.loss_iou: 0.3263  d3.loss_cls: 0.2108  d3.loss_bbox: 0.0388  d3.loss_iou: 0.3283  d4.loss_cls: 0.2096  d4.loss_bbox: 0.0388  d4.loss_iou: 0.3276  enc_loss_cls: 0.2693  enc_loss_bbox: 0.0408  enc_loss_iou: 0.3494  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0299  dn_loss_iou: 0.2782  d0.dn_loss_cls: 0.0340  d0.dn_loss_bbox: 0.0427  d0.dn_loss_iou: 0.3685  d1.dn_loss_cls: 0.0102  d1.dn_loss_bbox: 0.0319  d1.dn_loss_iou: 0.2939  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0304  d2.dn_loss_iou: 0.2811  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0300  d3.dn_loss_iou: 0.2779  d4.dn_loss_cls: 0.0052  d4.dn_loss_bbox: 0.0299  d4.dn_loss_iou: 0.2779  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:05:15 - mmengine - INFO - Epoch(train) [2][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:27:13  time: 1.7927  data_time: 0.0169  memory: 16263  grad_norm: 39.8229  loss: 5.9487  loss_cls: 0.1757  loss_bbox: 0.0389  loss_iou: 0.3274  d0.loss_cls: 0.2149  d0.loss_bbox: 0.0424  d0.loss_iou: 0.3398  d1.loss_cls: 0.1942  d1.loss_bbox: 0.0392  d1.loss_iou: 0.3294  d2.loss_cls: 0.1864  d2.loss_bbox: 0.0388  d2.loss_iou: 0.3261  d3.loss_cls: 0.1827  d3.loss_bbox: 0.0386  d3.loss_iou: 0.3258  d4.loss_cls: 0.1754  d4.loss_bbox: 0.0389  d4.loss_iou: 0.3271  enc_loss_cls: 0.2353  enc_loss_bbox: 0.0458  enc_loss_iou: 0.3598  dn_loss_cls: 0.0081  dn_loss_bbox: 0.0301  dn_loss_iou: 0.2634  d0.dn_loss_cls: 0.0333  d0.dn_loss_bbox: 0.0429  d0.dn_loss_iou: 0.3526  d1.dn_loss_cls: 0.0124  d1.dn_loss_bbox: 0.0325  d1.dn_loss_iou: 0.2792  d2.dn_loss_cls: 0.0093  d2.dn_loss_bbox: 0.0306  d2.dn_loss_iou: 0.2663  d3.dn_loss_cls: 0.0083  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2632  d4.dn_loss_cls: 0.0080  d4.dn_loss_bbox: 0.0301  d4.dn_loss_iou: 0.2630  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0005
2025/10/29 10:06:45 - mmengine - INFO - Epoch(train) [2][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:25:43  time: 1.8129  data_time: 0.0169  memory: 16264  grad_norm: 35.7071  loss: 5.3513  loss_cls: 0.1686  loss_bbox: 0.0342  loss_iou: 0.2861  d0.loss_cls: 0.2049  d0.loss_bbox: 0.0365  d0.loss_iou: 0.2970  d1.loss_cls: 0.1855  d1.loss_bbox: 0.0353  d1.loss_iou: 0.2913  d2.loss_cls: 0.1758  d2.loss_bbox: 0.0346  d2.loss_iou: 0.2873  d3.loss_cls: 0.1724  d3.loss_bbox: 0.0342  d3.loss_iou: 0.2852  d4.loss_cls: 0.1685  d4.loss_bbox: 0.0342  d4.loss_iou: 0.2860  enc_loss_cls: 0.2289  enc_loss_bbox: 0.0396  enc_loss_iou: 0.3144  dn_loss_cls: 0.0050  dn_loss_bbox: 0.0291  dn_loss_iou: 0.2362  d0.dn_loss_cls: 0.0302  d0.dn_loss_bbox: 0.0396  d0.dn_loss_iou: 0.3083  d1.dn_loss_cls: 0.0087  d1.dn_loss_bbox: 0.0305  d1.dn_loss_iou: 0.2468  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2376  d3.dn_loss_cls: 0.0053  d3.dn_loss_bbox: 0.0291  d3.dn_loss_iou: 0.2360  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0291  d4.dn_loss_iou: 0.2360  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:08:15 - mmengine - INFO - Epoch(train) [2][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:24:12  time: 1.7903  data_time: 0.0163  memory: 16257  grad_norm: 34.6555  loss: 5.9416  loss_cls: 0.1887  loss_bbox: 0.0367  loss_iou: 0.3338  d0.loss_cls: 0.2205  d0.loss_bbox: 0.0383  d0.loss_iou: 0.3458  d1.loss_cls: 0.2001  d1.loss_bbox: 0.0370  d1.loss_iou: 0.3389  d2.loss_cls: 0.1943  d2.loss_bbox: 0.0371  d2.loss_iou: 0.3369  d3.loss_cls: 0.1913  d3.loss_bbox: 0.0365  d3.loss_iou: 0.3352  d4.loss_cls: 0.1903  d4.loss_bbox: 0.0367  d4.loss_iou: 0.3333  enc_loss_cls: 0.2441  enc_loss_bbox: 0.0414  enc_loss_iou: 0.3652  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0289  dn_loss_iou: 0.2520  d0.dn_loss_cls: 0.0282  d0.dn_loss_bbox: 0.0399  d0.dn_loss_iou: 0.3320  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0305  d1.dn_loss_iou: 0.2654  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2549  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0289  d3.dn_loss_iou: 0.2522  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0289  d4.dn_loss_iou: 0.2518  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:09:45 - mmengine - INFO - Epoch(train) [2][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:22:42  time: 1.8098  data_time: 0.0167  memory: 16269  grad_norm: 41.4292  loss: 5.4106  loss_cls: 0.1693  loss_bbox: 0.0386  loss_iou: 0.2868  d0.loss_cls: 0.2000  d0.loss_bbox: 0.0396  d0.loss_iou: 0.2987  d1.loss_cls: 0.1918  d1.loss_bbox: 0.0377  d1.loss_iou: 0.2877  d2.loss_cls: 0.1816  d2.loss_bbox: 0.0378  d2.loss_iou: 0.2855  d3.loss_cls: 0.1761  d3.loss_bbox: 0.0378  d3.loss_iou: 0.2850  d4.loss_cls: 0.1713  d4.loss_bbox: 0.0386  d4.loss_iou: 0.2866  enc_loss_cls: 0.2269  enc_loss_bbox: 0.0439  enc_loss_iou: 0.3163  dn_loss_cls: 0.0068  dn_loss_bbox: 0.0299  dn_loss_iou: 0.2354  d0.dn_loss_cls: 0.0286  d0.dn_loss_bbox: 0.0430  d0.dn_loss_iou: 0.3156  d1.dn_loss_cls: 0.0118  d1.dn_loss_bbox: 0.0320  d1.dn_loss_iou: 0.2476  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0304  d2.dn_loss_iou: 0.2378  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0299  d3.dn_loss_iou: 0.2353  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0299  d4.dn_loss_iou: 0.2351  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:11:16 - mmengine - INFO - Epoch(train) [2][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:21:12  time: 1.8062  data_time: 0.0170  memory: 16292  grad_norm: 44.3575  loss: 5.8305  loss_cls: 0.1764  loss_bbox: 0.0408  loss_iou: 0.3387  d0.loss_cls: 0.2201  d0.loss_bbox: 0.0420  d0.loss_iou: 0.3453  d1.loss_cls: 0.1956  d1.loss_bbox: 0.0420  d1.loss_iou: 0.3428  d2.loss_cls: 0.1874  d2.loss_bbox: 0.0410  d2.loss_iou: 0.3404  d3.loss_cls: 0.1788  d3.loss_bbox: 0.0409  d3.loss_iou: 0.3402  d4.loss_cls: 0.1769  d4.loss_bbox: 0.0409  d4.loss_iou: 0.3386  enc_loss_cls: 0.2350  enc_loss_bbox: 0.0451  enc_loss_iou: 0.3660  dn_loss_cls: 0.0061  dn_loss_bbox: 0.0280  dn_loss_iou: 0.2364  d0.dn_loss_cls: 0.0277  d0.dn_loss_bbox: 0.0383  d0.dn_loss_iou: 0.3131  d1.dn_loss_cls: 0.0097  d1.dn_loss_bbox: 0.0296  d1.dn_loss_iou: 0.2487  d2.dn_loss_cls: 0.0071  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2384  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0280  d3.dn_loss_iou: 0.2363  d4.dn_loss_cls: 0.0060  d4.dn_loss_bbox: 0.0279  d4.dn_loss_iou: 0.2361  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 10:12:46 - mmengine - INFO - Epoch(train) [2][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:19:42  time: 1.8066  data_time: 0.0177  memory: 16269  grad_norm: 46.1303  loss: 6.6824  loss_cls: 0.2029  loss_bbox: 0.0437  loss_iou: 0.3894  d0.loss_cls: 0.2368  d0.loss_bbox: 0.0473  d0.loss_iou: 0.4121  d1.loss_cls: 0.2154  d1.loss_bbox: 0.0457  d1.loss_iou: 0.3978  d2.loss_cls: 0.2052  d2.loss_bbox: 0.0441  d2.loss_iou: 0.3936  d3.loss_cls: 0.2010  d3.loss_bbox: 0.0438  d3.loss_iou: 0.3915  d4.loss_cls: 0.2049  d4.loss_bbox: 0.0437  d4.loss_iou: 0.3892  enc_loss_cls: 0.2602  enc_loss_bbox: 0.0517  enc_loss_iou: 0.4359  dn_loss_cls: 0.0078  dn_loss_bbox: 0.0311  dn_loss_iou: 0.2723  d0.dn_loss_cls: 0.0339  d0.dn_loss_bbox: 0.0440  d0.dn_loss_iou: 0.3611  d1.dn_loss_cls: 0.0123  d1.dn_loss_bbox: 0.0334  d1.dn_loss_iou: 0.2884  d2.dn_loss_cls: 0.0091  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2758  d3.dn_loss_cls: 0.0081  d3.dn_loss_bbox: 0.0312  d3.dn_loss_iou: 0.2722  d4.dn_loss_cls: 0.0079  d4.dn_loss_bbox: 0.0311  d4.dn_loss_iou: 0.2719  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:14:16 - mmengine - INFO - Epoch(train) [2][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:18:11  time: 1.7994  data_time: 0.0166  memory: 16267  grad_norm: 38.0924  loss: 6.0098  loss_cls: 0.1865  loss_bbox: 0.0390  loss_iou: 0.3399  d0.loss_cls: 0.2226  d0.loss_bbox: 0.0426  d0.loss_iou: 0.3609  d1.loss_cls: 0.2036  d1.loss_bbox: 0.0405  d1.loss_iou: 0.3495  d2.loss_cls: 0.1926  d2.loss_bbox: 0.0392  d2.loss_iou: 0.3427  d3.loss_cls: 0.1890  d3.loss_bbox: 0.0392  d3.loss_iou: 0.3407  d4.loss_cls: 0.1865  d4.loss_bbox: 0.0391  d4.loss_iou: 0.3404  enc_loss_cls: 0.2433  enc_loss_bbox: 0.0445  enc_loss_iou: 0.3732  dn_loss_cls: 0.0060  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2484  d0.dn_loss_cls: 0.0305  d0.dn_loss_bbox: 0.0414  d0.dn_loss_iou: 0.3304  d1.dn_loss_cls: 0.0104  d1.dn_loss_bbox: 0.0318  d1.dn_loss_iou: 0.2628  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2518  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2485  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2481  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0004  d4.loss_num: 0.0005
2025/10/29 10:15:47 - mmengine - INFO - Epoch(train) [2][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:16:41  time: 1.8088  data_time: 0.0167  memory: 16282  grad_norm: 45.6273  loss: 5.7756  loss_cls: 0.1814  loss_bbox: 0.0363  loss_iou: 0.3091  d0.loss_cls: 0.2342  d0.loss_bbox: 0.0379  d0.loss_iou: 0.3213  d1.loss_cls: 0.1994  d1.loss_bbox: 0.0368  d1.loss_iou: 0.3141  d2.loss_cls: 0.1916  d2.loss_bbox: 0.0363  d2.loss_iou: 0.3086  d3.loss_cls: 0.1843  d3.loss_bbox: 0.0365  d3.loss_iou: 0.3107  d4.loss_cls: 0.1810  d4.loss_bbox: 0.0363  d4.loss_iou: 0.3089  enc_loss_cls: 0.2402  enc_loss_bbox: 0.0407  enc_loss_iou: 0.3333  dn_loss_cls: 0.0067  dn_loss_bbox: 0.0294  dn_loss_iou: 0.2556  d0.dn_loss_cls: 0.0344  d0.dn_loss_bbox: 0.0411  d0.dn_loss_iou: 0.3377  d1.dn_loss_cls: 0.0118  d1.dn_loss_bbox: 0.0312  d1.dn_loss_iou: 0.2680  d2.dn_loss_cls: 0.0077  d2.dn_loss_bbox: 0.0298  d2.dn_loss_iou: 0.2578  d3.dn_loss_cls: 0.0069  d3.dn_loss_bbox: 0.0294  d3.dn_loss_iou: 0.2551  d4.dn_loss_cls: 0.0066  d4.dn_loss_bbox: 0.0294  d4.dn_loss_iou: 0.2551  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:17:16 - mmengine - INFO - Epoch(train) [2][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:15:11  time: 1.7986  data_time: 0.0163  memory: 16263  grad_norm: 37.3856  loss: 6.1028  loss_cls: 0.1717  loss_bbox: 0.0410  loss_iou: 0.3448  d0.loss_cls: 0.2121  d0.loss_bbox: 0.0429  d0.loss_iou: 0.3563  d1.loss_cls: 0.1882  d1.loss_bbox: 0.0403  d1.loss_iou: 0.3458  d2.loss_cls: 0.1771  d2.loss_bbox: 0.0411  d2.loss_iou: 0.3463  d3.loss_cls: 0.1705  d3.loss_bbox: 0.0411  d3.loss_iou: 0.3460  d4.loss_cls: 0.1690  d4.loss_bbox: 0.0411  d4.loss_iou: 0.3468  enc_loss_cls: 0.2301  enc_loss_bbox: 0.0438  enc_loss_iou: 0.3684  dn_loss_cls: 0.0064  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2788  d0.dn_loss_cls: 0.0337  d0.dn_loss_bbox: 0.0408  d0.dn_loss_iou: 0.3599  d1.dn_loss_cls: 0.0111  d1.dn_loss_bbox: 0.0317  d1.dn_loss_iou: 0.2920  d2.dn_loss_cls: 0.0078  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2807  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2788  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2786  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:18:47 - mmengine - INFO - Epoch(train) [2][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:13:40  time: 1.8031  data_time: 0.0165  memory: 16269  grad_norm: 37.9002  loss: 6.1319  loss_cls: 0.1967  loss_bbox: 0.0392  loss_iou: 0.3362  d0.loss_cls: 0.2410  d0.loss_bbox: 0.0424  d0.loss_iou: 0.3557  d1.loss_cls: 0.2132  d1.loss_bbox: 0.0399  d1.loss_iou: 0.3441  d2.loss_cls: 0.2033  d2.loss_bbox: 0.0395  d2.loss_iou: 0.3379  d3.loss_cls: 0.1980  d3.loss_bbox: 0.0393  d3.loss_iou: 0.3367  d4.loss_cls: 0.1943  d4.loss_bbox: 0.0392  d4.loss_iou: 0.3360  enc_loss_cls: 0.2701  enc_loss_bbox: 0.0450  enc_loss_iou: 0.3750  dn_loss_cls: 0.0070  dn_loss_bbox: 0.0306  dn_loss_iou: 0.2546  d0.dn_loss_cls: 0.0417  d0.dn_loss_bbox: 0.0425  d0.dn_loss_iou: 0.3321  d1.dn_loss_cls: 0.0142  d1.dn_loss_bbox: 0.0322  d1.dn_loss_iou: 0.2668  d2.dn_loss_cls: 0.0116  d2.dn_loss_bbox: 0.0309  d2.dn_loss_iou: 0.2571  d3.dn_loss_cls: 0.0075  d3.dn_loss_bbox: 0.0306  d3.dn_loss_iou: 0.2548  d4.dn_loss_cls: 0.0071  d4.dn_loss_bbox: 0.0306  d4.dn_loss_iou: 0.2542  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:20:17 - mmengine - INFO - Epoch(train) [2][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:12:11  time: 1.8121  data_time: 0.0165  memory: 16285  grad_norm: 41.5301  loss: 6.0961  loss_cls: 0.1780  loss_bbox: 0.0384  loss_iou: 0.3335  d0.loss_cls: 0.2148  d0.loss_bbox: 0.0451  d0.loss_iou: 0.3556  d1.loss_cls: 0.1936  d1.loss_bbox: 0.0415  d1.loss_iou: 0.3448  d2.loss_cls: 0.1846  d2.loss_bbox: 0.0397  d2.loss_iou: 0.3389  d3.loss_cls: 0.1792  d3.loss_bbox: 0.0396  d3.loss_iou: 0.3384  d4.loss_cls: 0.1760  d4.loss_bbox: 0.0385  d4.loss_iou: 0.3347  enc_loss_cls: 0.2475  enc_loss_bbox: 0.0446  enc_loss_iou: 0.3703  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0317  dn_loss_iou: 0.2749  d0.dn_loss_cls: 0.0306  d0.dn_loss_bbox: 0.0439  d0.dn_loss_iou: 0.3601  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0333  d1.dn_loss_iou: 0.2879  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0320  d2.dn_loss_iou: 0.2768  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0317  d3.dn_loss_iou: 0.2751  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2747  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0007  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:21:47 - mmengine - INFO - Epoch(train) [2][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:10:40  time: 1.7998  data_time: 0.0162  memory: 16284  grad_norm: 39.1782  loss: 5.7932  loss_cls: 0.1705  loss_bbox: 0.0397  loss_iou: 0.3217  d0.loss_cls: 0.1980  d0.loss_bbox: 0.0439  d0.loss_iou: 0.3478  d1.loss_cls: 0.1812  d1.loss_bbox: 0.0404  d1.loss_iou: 0.3269  d2.loss_cls: 0.1741  d2.loss_bbox: 0.0398  d2.loss_iou: 0.3219  d3.loss_cls: 0.1712  d3.loss_bbox: 0.0400  d3.loss_iou: 0.3228  d4.loss_cls: 0.1684  d4.loss_bbox: 0.0400  d4.loss_iou: 0.3237  enc_loss_cls: 0.2333  enc_loss_bbox: 0.0464  enc_loss_iou: 0.3656  dn_loss_cls: 0.0060  dn_loss_bbox: 0.0299  dn_loss_iou: 0.2527  d0.dn_loss_cls: 0.0314  d0.dn_loss_bbox: 0.0425  d0.dn_loss_iou: 0.3343  d1.dn_loss_cls: 0.0102  d1.dn_loss_bbox: 0.0317  d1.dn_loss_iou: 0.2650  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0301  d2.dn_loss_iou: 0.2548  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0300  d3.dn_loss_iou: 0.2523  d4.dn_loss_cls: 0.0061  d4.dn_loss_bbox: 0.0299  d4.dn_loss_iou: 0.2524  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:23:18 - mmengine - INFO - Epoch(train) [2][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:09:10  time: 1.8111  data_time: 0.0220  memory: 16274  grad_norm: 38.2831  loss: 5.3504  loss_cls: 0.1608  loss_bbox: 0.0344  loss_iou: 0.2955  d0.loss_cls: 0.2013  d0.loss_bbox: 0.0365  d0.loss_iou: 0.3067  d1.loss_cls: 0.1809  d1.loss_bbox: 0.0348  d1.loss_iou: 0.2975  d2.loss_cls: 0.1711  d2.loss_bbox: 0.0342  d2.loss_iou: 0.2939  d3.loss_cls: 0.1644  d3.loss_bbox: 0.0344  d3.loss_iou: 0.2952  d4.loss_cls: 0.1620  d4.loss_bbox: 0.0344  d4.loss_iou: 0.2954  enc_loss_cls: 0.2289  enc_loss_bbox: 0.0386  enc_loss_iou: 0.3226  dn_loss_cls: 0.0070  dn_loss_bbox: 0.0271  dn_loss_iou: 0.2304  d0.dn_loss_cls: 0.0304  d0.dn_loss_bbox: 0.0382  d0.dn_loss_iou: 0.3089  d1.dn_loss_cls: 0.0112  d1.dn_loss_bbox: 0.0288  d1.dn_loss_iou: 0.2442  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2337  d3.dn_loss_cls: 0.0072  d3.dn_loss_bbox: 0.0271  d3.dn_loss_iou: 0.2304  d4.dn_loss_cls: 0.0070  d4.dn_loss_bbox: 0.0271  d4.dn_loss_iou: 0.2303  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:24:49 - mmengine - INFO - Epoch(train) [2][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:07:41  time: 1.8163  data_time: 0.0170  memory: 16275  grad_norm: 37.4343  loss: 5.7703  loss_cls: 0.1719  loss_bbox: 0.0413  loss_iou: 0.3132  d0.loss_cls: 0.2048  d0.loss_bbox: 0.0429  d0.loss_iou: 0.3286  d1.loss_cls: 0.1855  d1.loss_bbox: 0.0401  d1.loss_iou: 0.3150  d2.loss_cls: 0.1826  d2.loss_bbox: 0.0394  d2.loss_iou: 0.3111  d3.loss_cls: 0.1784  d3.loss_bbox: 0.0402  d3.loss_iou: 0.3090  d4.loss_cls: 0.1739  d4.loss_bbox: 0.0413  d4.loss_iou: 0.3124  enc_loss_cls: 0.2465  enc_loss_bbox: 0.0473  enc_loss_iou: 0.3503  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2538  d0.dn_loss_cls: 0.0343  d0.dn_loss_bbox: 0.0431  d0.dn_loss_iou: 0.3416  d1.dn_loss_cls: 0.0098  d1.dn_loss_bbox: 0.0322  d1.dn_loss_iou: 0.2688  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2570  d3.dn_loss_cls: 0.0055  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2536  d4.dn_loss_cls: 0.0052  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2536  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:26:18 - mmengine - INFO - Epoch(train) [2][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:06:10  time: 1.7882  data_time: 0.0170  memory: 16269  grad_norm: 38.9487  loss: 5.8718  loss_cls: 0.1546  loss_bbox: 0.0372  loss_iou: 0.3390  d0.loss_cls: 0.1941  d0.loss_bbox: 0.0389  d0.loss_iou: 0.3540  d1.loss_cls: 0.1742  d1.loss_bbox: 0.0375  d1.loss_iou: 0.3438  d2.loss_cls: 0.1647  d2.loss_bbox: 0.0372  d2.loss_iou: 0.3408  d3.loss_cls: 0.1562  d3.loss_bbox: 0.0374  d3.loss_iou: 0.3414  d4.loss_cls: 0.1538  d4.loss_bbox: 0.0373  d4.loss_iou: 0.3394  enc_loss_cls: 0.2190  enc_loss_bbox: 0.0410  enc_loss_iou: 0.3684  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0298  dn_loss_iou: 0.2655  d0.dn_loss_cls: 0.0309  d0.dn_loss_bbox: 0.0419  d0.dn_loss_iou: 0.3535  d1.dn_loss_cls: 0.0102  d1.dn_loss_bbox: 0.0318  d1.dn_loss_iou: 0.2817  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0302  d2.dn_loss_iou: 0.2682  d3.dn_loss_cls: 0.0060  d3.dn_loss_bbox: 0.0298  d3.dn_loss_iou: 0.2656  d4.dn_loss_cls: 0.0057  d4.dn_loss_bbox: 0.0297  d4.dn_loss_iou: 0.2654  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:26:45 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 10:27:48 - mmengine - INFO - Epoch(train) [2][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:04:39  time: 1.8037  data_time: 0.0163  memory: 16269  grad_norm: 37.4692  loss: 5.0891  loss_cls: 0.1459  loss_bbox: 0.0379  loss_iou: 0.2876  d0.loss_cls: 0.1748  d0.loss_bbox: 0.0400  d0.loss_iou: 0.2993  d1.loss_cls: 0.1595  d1.loss_bbox: 0.0382  d1.loss_iou: 0.2909  d2.loss_cls: 0.1506  d2.loss_bbox: 0.0380  d2.loss_iou: 0.2883  d3.loss_cls: 0.1462  d3.loss_bbox: 0.0378  d3.loss_iou: 0.2883  d4.loss_cls: 0.1441  d4.loss_bbox: 0.0379  d4.loss_iou: 0.2878  enc_loss_cls: 0.2018  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3222  dn_loss_cls: 0.0032  dn_loss_bbox: 0.0271  dn_loss_iou: 0.2198  d0.dn_loss_cls: 0.0265  d0.dn_loss_bbox: 0.0371  d0.dn_loss_iou: 0.2923  d1.dn_loss_cls: 0.0065  d1.dn_loss_bbox: 0.0283  d1.dn_loss_iou: 0.2312  d2.dn_loss_cls: 0.0042  d2.dn_loss_bbox: 0.0272  d2.dn_loss_iou: 0.2221  d3.dn_loss_cls: 0.0035  d3.dn_loss_bbox: 0.0271  d3.dn_loss_iou: 0.2199  d4.dn_loss_cls: 0.0033  d4.dn_loss_bbox: 0.0270  d4.dn_loss_iou: 0.2196  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:28:51 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 10:28:51 - mmengine - INFO - Saving checkpoint at 2 epochs
2025/10/29 10:29:03 - mmengine - INFO - Epoch(val) [2][ 50/429]    eta: 0:00:34  time: 0.0903  data_time: 0.0031  memory: 16262  
2025/10/29 10:29:08 - mmengine - INFO - Epoch(val) [2][100/429]    eta: 0:00:29  time: 0.0869  data_time: 0.0022  memory: 3164  
2025/10/29 10:29:12 - mmengine - INFO - Epoch(val) [2][150/429]    eta: 0:00:24  time: 0.0868  data_time: 0.0022  memory: 3169  
2025/10/29 10:29:16 - mmengine - INFO - Epoch(val) [2][200/429]    eta: 0:00:20  time: 0.0880  data_time: 0.0022  memory: 3165  
2025/10/29 10:29:21 - mmengine - INFO - Epoch(val) [2][250/429]    eta: 0:00:15  time: 0.0884  data_time: 0.0022  memory: 3167  
2025/10/29 10:29:25 - mmengine - INFO - Epoch(val) [2][300/429]    eta: 0:00:11  time: 0.0882  data_time: 0.0022  memory: 3175  
2025/10/29 10:29:30 - mmengine - INFO - Epoch(val) [2][350/429]    eta: 0:00:06  time: 0.0882  data_time: 0.0022  memory: 3169  
2025/10/29 10:29:34 - mmengine - INFO - Epoch(val) [2][400/429]    eta: 0:00:02  time: 0.0874  data_time: 0.0022  memory: 3165  
2025/10/29 10:29:38 - mmengine - INFO - {'instance_F1_score': 0.6078051480763909, 'instance_acc': 0.4419941718516185, 'image_F1_score': 0.5143626570915619, 'image_acc': 0.36946386946386944}
2025/10/29 10:29:38 - mmengine - INFO - Epoch(val) [2][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.6078  grefcoco_val/refdrone/instance_acc: 0.4420  grefcoco_val/refdrone/image_F1_score: 0.5144  grefcoco_val/refdrone/image_acc: 0.3695  data_time: 0.0023  time: 0.0880
2025/10/29 10:31:08 - mmengine - INFO - Epoch(train) [3][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:02:05  time: 1.7981  data_time: 0.0180  memory: 16262  grad_norm: 31.2837  loss: 5.3815  loss_cls: 0.1489  loss_bbox: 0.0361  loss_iou: 0.2869  d0.loss_cls: 0.1737  d0.loss_bbox: 0.0375  d0.loss_iou: 0.2986  d1.loss_cls: 0.1588  d1.loss_bbox: 0.0366  d1.loss_iou: 0.2926  d2.loss_cls: 0.1508  d2.loss_bbox: 0.0365  d2.loss_iou: 0.2892  d3.loss_cls: 0.1505  d3.loss_bbox: 0.0361  d3.loss_iou: 0.2866  d4.loss_cls: 0.1480  d4.loss_bbox: 0.0361  d4.loss_iou: 0.2868  enc_loss_cls: 0.2043  enc_loss_bbox: 0.0394  enc_loss_iou: 0.3196  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2590  d0.dn_loss_cls: 0.0299  d0.dn_loss_bbox: 0.0438  d0.dn_loss_iou: 0.3415  d1.dn_loss_cls: 0.0090  d1.dn_loss_bbox: 0.0336  d1.dn_loss_iou: 0.2744  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0322  d2.dn_loss_iou: 0.2635  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0317  d3.dn_loss_iou: 0.2595  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2588  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:32:39 - mmengine - INFO - Epoch(train) [3][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:00:35  time: 1.8043  data_time: 0.0170  memory: 16267  grad_norm: 34.9638  loss: 5.9509  loss_cls: 0.1461  loss_bbox: 0.0392  loss_iou: 0.3476  d0.loss_cls: 0.1906  d0.loss_bbox: 0.0409  d0.loss_iou: 0.3560  d1.loss_cls: 0.1630  d1.loss_bbox: 0.0411  d1.loss_iou: 0.3526  d2.loss_cls: 0.1556  d2.loss_bbox: 0.0398  d2.loss_iou: 0.3474  d3.loss_cls: 0.1530  d3.loss_bbox: 0.0400  d3.loss_iou: 0.3487  d4.loss_cls: 0.1464  d4.loss_bbox: 0.0392  d4.loss_iou: 0.3471  enc_loss_cls: 0.2095  enc_loss_bbox: 0.0439  enc_loss_iou: 0.3800  dn_loss_cls: 0.0114  dn_loss_bbox: 0.0299  dn_loss_iou: 0.2706  d0.dn_loss_cls: 0.0363  d0.dn_loss_bbox: 0.0410  d0.dn_loss_iou: 0.3576  d1.dn_loss_cls: 0.0159  d1.dn_loss_bbox: 0.0315  d1.dn_loss_iou: 0.2850  d2.dn_loss_cls: 0.0133  d2.dn_loss_bbox: 0.0302  d2.dn_loss_iou: 0.2738  d3.dn_loss_cls: 0.0120  d3.dn_loss_bbox: 0.0299  d3.dn_loss_iou: 0.2709  d4.dn_loss_cls: 0.0111  d4.dn_loss_bbox: 0.0299  d4.dn_loss_iou: 0.2705  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:34:09 - mmengine - INFO - Epoch(train) [3][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:59:05  time: 1.8110  data_time: 0.0175  memory: 16280  grad_norm: 41.0146  loss: 5.3101  loss_cls: 0.1564  loss_bbox: 0.0310  loss_iou: 0.2777  d0.loss_cls: 0.1966  d0.loss_bbox: 0.0346  d0.loss_iou: 0.2955  d1.loss_cls: 0.1720  d1.loss_bbox: 0.0324  d1.loss_iou: 0.2849  d2.loss_cls: 0.1654  d2.loss_bbox: 0.0313  d2.loss_iou: 0.2787  d3.loss_cls: 0.1648  d3.loss_bbox: 0.0307  d3.loss_iou: 0.2734  d4.loss_cls: 0.1523  d4.loss_bbox: 0.0318  d4.loss_iou: 0.2816  enc_loss_cls: 0.2221  enc_loss_bbox: 0.0376  enc_loss_iou: 0.3167  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2511  d0.dn_loss_cls: 0.0298  d0.dn_loss_bbox: 0.0378  d0.dn_loss_iou: 0.3294  d1.dn_loss_cls: 0.0096  d1.dn_loss_bbox: 0.0292  d1.dn_loss_iou: 0.2640  d2.dn_loss_cls: 0.0064  d2.dn_loss_bbox: 0.0280  d2.dn_loss_iou: 0.2540  d3.dn_loss_cls: 0.0056  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2510  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2507  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:35:39 - mmengine - INFO - Epoch(train) [3][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:57:35  time: 1.8021  data_time: 0.0171  memory: 16265  grad_norm: 34.8344  loss: 5.3557  loss_cls: 0.1545  loss_bbox: 0.0399  loss_iou: 0.3101  d0.loss_cls: 0.1857  d0.loss_bbox: 0.0402  d0.loss_iou: 0.3268  d1.loss_cls: 0.1660  d1.loss_bbox: 0.0416  d1.loss_iou: 0.3198  d2.loss_cls: 0.1581  d2.loss_bbox: 0.0393  d2.loss_iou: 0.3105  d3.loss_cls: 0.1574  d3.loss_bbox: 0.0396  d3.loss_iou: 0.3091  d4.loss_cls: 0.1566  d4.loss_bbox: 0.0397  d4.loss_iou: 0.3089  enc_loss_cls: 0.2168  enc_loss_bbox: 0.0448  enc_loss_iou: 0.3488  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0270  dn_loss_iou: 0.2195  d0.dn_loss_cls: 0.0288  d0.dn_loss_bbox: 0.0383  d0.dn_loss_iou: 0.2958  d1.dn_loss_cls: 0.0080  d1.dn_loss_bbox: 0.0288  d1.dn_loss_iou: 0.2322  d2.dn_loss_cls: 0.0050  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2225  d3.dn_loss_cls: 0.0043  d3.dn_loss_bbox: 0.0270  d3.dn_loss_iou: 0.2194  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0270  d4.dn_loss_iou: 0.2193  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:37:09 - mmengine - INFO - Epoch(train) [3][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:56:05  time: 1.8043  data_time: 0.0166  memory: 16264  grad_norm: 39.0413  loss: 5.3621  loss_cls: 0.1446  loss_bbox: 0.0354  loss_iou: 0.2975  d0.loss_cls: 0.1905  d0.loss_bbox: 0.0373  d0.loss_iou: 0.3101  d1.loss_cls: 0.1626  d1.loss_bbox: 0.0360  d1.loss_iou: 0.3016  d2.loss_cls: 0.1522  d2.loss_bbox: 0.0356  d2.loss_iou: 0.2998  d3.loss_cls: 0.1476  d3.loss_bbox: 0.0353  d3.loss_iou: 0.2972  d4.loss_cls: 0.1464  d4.loss_bbox: 0.0354  d4.loss_iou: 0.2978  enc_loss_cls: 0.2027  enc_loss_bbox: 0.0409  enc_loss_iou: 0.3317  dn_loss_cls: 0.0043  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2494  d0.dn_loss_cls: 0.0300  d0.dn_loss_bbox: 0.0381  d0.dn_loss_iou: 0.3267  d1.dn_loss_cls: 0.0084  d1.dn_loss_bbox: 0.0289  d1.dn_loss_iou: 0.2606  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2516  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2492  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2492  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0004  d4.loss_num: 0.0005
2025/10/29 10:38:40 - mmengine - INFO - Epoch(train) [3][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:54:35  time: 1.8123  data_time: 0.0181  memory: 16269  grad_norm: 39.3422  loss: 5.4395  loss_cls: 0.1553  loss_bbox: 0.0338  loss_iou: 0.3161  d0.loss_cls: 0.1864  d0.loss_bbox: 0.0360  d0.loss_iou: 0.3319  d1.loss_cls: 0.1638  d1.loss_bbox: 0.0347  d1.loss_iou: 0.3215  d2.loss_cls: 0.1559  d2.loss_bbox: 0.0343  d2.loss_iou: 0.3183  d3.loss_cls: 0.1577  d3.loss_bbox: 0.0340  d3.loss_iou: 0.3163  d4.loss_cls: 0.1546  d4.loss_bbox: 0.0343  d4.loss_iou: 0.3170  enc_loss_cls: 0.2153  enc_loss_bbox: 0.0404  enc_loss_iou: 0.3563  dn_loss_cls: 0.0043  dn_loss_bbox: 0.0268  dn_loss_iou: 0.2341  d0.dn_loss_cls: 0.0263  d0.dn_loss_bbox: 0.0371  d0.dn_loss_iou: 0.3109  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0284  d1.dn_loss_iou: 0.2475  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0272  d2.dn_loss_iou: 0.2370  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0268  d3.dn_loss_iou: 0.2339  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.0268  d4.dn_loss_iou: 0.2338  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0004
2025/10/29 10:40:10 - mmengine - INFO - Epoch(train) [3][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:53:04  time: 1.7993  data_time: 0.0178  memory: 16271  grad_norm: 46.2005  loss: 5.6418  loss_cls: 0.1448  loss_bbox: 0.0406  loss_iou: 0.3322  d0.loss_cls: 0.1979  d0.loss_bbox: 0.0432  d0.loss_iou: 0.3479  d1.loss_cls: 0.1616  d1.loss_bbox: 0.0425  d1.loss_iou: 0.3394  d2.loss_cls: 0.1535  d2.loss_bbox: 0.0403  d2.loss_iou: 0.3312  d3.loss_cls: 0.1519  d3.loss_bbox: 0.0400  d3.loss_iou: 0.3282  d4.loss_cls: 0.1481  d4.loss_bbox: 0.0403  d4.loss_iou: 0.3295  enc_loss_cls: 0.2149  enc_loss_bbox: 0.0484  enc_loss_iou: 0.3760  dn_loss_cls: 0.0079  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2392  d0.dn_loss_cls: 0.0287  d0.dn_loss_bbox: 0.0414  d0.dn_loss_iou: 0.3103  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0321  d1.dn_loss_iou: 0.2514  d2.dn_loss_cls: 0.0086  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2413  d3.dn_loss_cls: 0.0077  d3.dn_loss_bbox: 0.0304  d3.dn_loss_iou: 0.2389  d4.dn_loss_cls: 0.0078  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2389  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:41:40 - mmengine - INFO - Epoch(train) [3][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:51:34  time: 1.7995  data_time: 0.0173  memory: 16263  grad_norm: 41.8320  loss: 5.4869  loss_cls: 0.1573  loss_bbox: 0.0371  loss_iou: 0.3119  d0.loss_cls: 0.2079  d0.loss_bbox: 0.0394  d0.loss_iou: 0.3285  d1.loss_cls: 0.1785  d1.loss_bbox: 0.0384  d1.loss_iou: 0.3200  d2.loss_cls: 0.1722  d2.loss_bbox: 0.0367  d2.loss_iou: 0.3115  d3.loss_cls: 0.1618  d3.loss_bbox: 0.0372  d3.loss_iou: 0.3124  d4.loss_cls: 0.1583  d4.loss_bbox: 0.0371  d4.loss_iou: 0.3114  enc_loss_cls: 0.2372  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3512  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2278  d0.dn_loss_cls: 0.0283  d0.dn_loss_bbox: 0.0394  d0.dn_loss_iou: 0.3063  d1.dn_loss_cls: 0.0083  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2405  d2.dn_loss_cls: 0.0049  d2.dn_loss_bbox: 0.0280  d2.dn_loss_iou: 0.2308  d3.dn_loss_cls: 0.0043  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2276  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2276  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:43:10 - mmengine - INFO - Epoch(train) [3][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:50:04  time: 1.8100  data_time: 0.0171  memory: 16256  grad_norm: 45.8772  loss: 5.1273  loss_cls: 0.1345  loss_bbox: 0.0331  loss_iou: 0.2694  d0.loss_cls: 0.1689  d0.loss_bbox: 0.0350  d0.loss_iou: 0.2844  d1.loss_cls: 0.1528  d1.loss_bbox: 0.0345  d1.loss_iou: 0.2767  d2.loss_cls: 0.1454  d2.loss_bbox: 0.0329  d2.loss_iou: 0.2695  d3.loss_cls: 0.1381  d3.loss_bbox: 0.0335  d3.loss_iou: 0.2705  d4.loss_cls: 0.1361  d4.loss_bbox: 0.0331  d4.loss_iou: 0.2692  enc_loss_cls: 0.1966  enc_loss_bbox: 0.0375  enc_loss_iou: 0.2958  dn_loss_cls: 0.0057  dn_loss_bbox: 0.0306  dn_loss_iou: 0.2529  d0.dn_loss_cls: 0.0324  d0.dn_loss_bbox: 0.0421  d0.dn_loss_iou: 0.3362  d1.dn_loss_cls: 0.0088  d1.dn_loss_bbox: 0.0323  d1.dn_loss_iou: 0.2664  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0308  d2.dn_loss_iou: 0.2549  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0305  d3.dn_loss_iou: 0.2523  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0305  d4.dn_loss_iou: 0.2526  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 10:44:41 - mmengine - INFO - Epoch(train) [3][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:48:34  time: 1.8087  data_time: 0.0170  memory: 16282  grad_norm: 40.1208  loss: 5.1549  loss_cls: 0.1360  loss_bbox: 0.0361  loss_iou: 0.3048  d0.loss_cls: 0.1721  d0.loss_bbox: 0.0382  d0.loss_iou: 0.3182  d1.loss_cls: 0.1543  d1.loss_bbox: 0.0368  d1.loss_iou: 0.3048  d2.loss_cls: 0.1430  d2.loss_bbox: 0.0362  d2.loss_iou: 0.3025  d3.loss_cls: 0.1402  d3.loss_bbox: 0.0359  d3.loss_iou: 0.3004  d4.loss_cls: 0.1364  d4.loss_bbox: 0.0358  d4.loss_iou: 0.3031  enc_loss_cls: 0.1944  enc_loss_bbox: 0.0427  enc_loss_iou: 0.3378  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0261  dn_loss_iou: 0.2218  d0.dn_loss_cls: 0.0265  d0.dn_loss_bbox: 0.0358  d0.dn_loss_iou: 0.2911  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0276  d1.dn_loss_iou: 0.2341  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0264  d2.dn_loss_iou: 0.2246  d3.dn_loss_cls: 0.0061  d3.dn_loss_bbox: 0.0261  d3.dn_loss_iou: 0.2218  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0261  d4.dn_loss_iou: 0.2216  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:46:11 - mmengine - INFO - Epoch(train) [3][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:47:04  time: 1.8048  data_time: 0.0165  memory: 16265  grad_norm: 37.2365  loss: 5.6160  loss_cls: 0.1426  loss_bbox: 0.0377  loss_iou: 0.3426  d0.loss_cls: 0.1739  d0.loss_bbox: 0.0413  d0.loss_iou: 0.3593  d1.loss_cls: 0.1550  d1.loss_bbox: 0.0386  d1.loss_iou: 0.3458  d2.loss_cls: 0.1453  d2.loss_bbox: 0.0380  d2.loss_iou: 0.3450  d3.loss_cls: 0.1444  d3.loss_bbox: 0.0381  d3.loss_iou: 0.3443  d4.loss_cls: 0.1427  d4.loss_bbox: 0.0377  d4.loss_iou: 0.3423  enc_loss_cls: 0.1962  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3769  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0269  dn_loss_iou: 0.2426  d0.dn_loss_cls: 0.0268  d0.dn_loss_bbox: 0.0369  d0.dn_loss_iou: 0.3186  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0282  d1.dn_loss_iou: 0.2547  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0272  d2.dn_loss_iou: 0.2448  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0269  d3.dn_loss_iou: 0.2426  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0269  d4.dn_loss_iou: 0.2424  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:47:42 - mmengine - INFO - Epoch(train) [3][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:45:34  time: 1.8131  data_time: 0.0168  memory: 16272  grad_norm: 36.8803  loss: 5.1180  loss_cls: 0.1255  loss_bbox: 0.0345  loss_iou: 0.2921  d0.loss_cls: 0.1704  d0.loss_bbox: 0.0383  d0.loss_iou: 0.3159  d1.loss_cls: 0.1460  d1.loss_bbox: 0.0356  d1.loss_iou: 0.3016  d2.loss_cls: 0.1335  d2.loss_bbox: 0.0348  d2.loss_iou: 0.2946  d3.loss_cls: 0.1286  d3.loss_bbox: 0.0344  d3.loss_iou: 0.2917  d4.loss_cls: 0.1272  d4.loss_bbox: 0.0345  d4.loss_iou: 0.2918  enc_loss_cls: 0.1900  enc_loss_bbox: 0.0424  enc_loss_iou: 0.3396  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0277  dn_loss_iou: 0.2310  d0.dn_loss_cls: 0.0262  d0.dn_loss_bbox: 0.0386  d0.dn_loss_iou: 0.3055  d1.dn_loss_cls: 0.0093  d1.dn_loss_bbox: 0.0294  d1.dn_loss_iou: 0.2440  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0279  d2.dn_loss_iou: 0.2336  d3.dn_loss_cls: 0.0056  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2306  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0277  d4.dn_loss_iou: 0.2308  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 10:49:12 - mmengine - INFO - Epoch(train) [3][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:44:04  time: 1.8097  data_time: 0.0178  memory: 16269  grad_norm: 46.1005  loss: 5.0389  loss_cls: 0.1253  loss_bbox: 0.0311  loss_iou: 0.2659  d0.loss_cls: 0.1664  d0.loss_bbox: 0.0327  d0.loss_iou: 0.2775  d1.loss_cls: 0.1464  d1.loss_bbox: 0.0318  d1.loss_iou: 0.2715  d2.loss_cls: 0.1377  d2.loss_bbox: 0.0312  d2.loss_iou: 0.2657  d3.loss_cls: 0.1313  d3.loss_bbox: 0.0311  d3.loss_iou: 0.2648  d4.loss_cls: 0.1266  d4.loss_bbox: 0.0311  d4.loss_iou: 0.2656  enc_loss_cls: 0.1899  enc_loss_bbox: 0.0350  enc_loss_iou: 0.2928  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2558  d0.dn_loss_cls: 0.0268  d0.dn_loss_bbox: 0.0422  d0.dn_loss_iou: 0.3392  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0320  d1.dn_loss_iou: 0.2691  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2590  d3.dn_loss_cls: 0.0050  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2554  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2554  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:50:42 - mmengine - INFO - Epoch(train) [3][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:42:34  time: 1.8022  data_time: 0.0165  memory: 16272  grad_norm: 37.2419  loss: 5.6892  loss_cls: 0.1451  loss_bbox: 0.0406  loss_iou: 0.3224  d0.loss_cls: 0.1804  d0.loss_bbox: 0.0431  d0.loss_iou: 0.3427  d1.loss_cls: 0.1580  d1.loss_bbox: 0.0415  d1.loss_iou: 0.3295  d2.loss_cls: 0.1534  d2.loss_bbox: 0.0404  d2.loss_iou: 0.3250  d3.loss_cls: 0.1455  d3.loss_bbox: 0.0408  d3.loss_iou: 0.3232  d4.loss_cls: 0.1448  d4.loss_bbox: 0.0405  d4.loss_iou: 0.3219  enc_loss_cls: 0.2027  enc_loss_bbox: 0.0461  enc_loss_iou: 0.3618  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2641  d0.dn_loss_cls: 0.0304  d0.dn_loss_bbox: 0.0403  d0.dn_loss_iou: 0.3446  d1.dn_loss_cls: 0.0095  d1.dn_loss_bbox: 0.0311  d1.dn_loss_iou: 0.2780  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0299  d2.dn_loss_iou: 0.2670  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0296  d3.dn_loss_iou: 0.2642  d4.dn_loss_cls: 0.0060  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2639  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:52:13 - mmengine - INFO - Epoch(train) [3][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:41:04  time: 1.8068  data_time: 0.0172  memory: 16267  grad_norm: 36.3390  loss: 5.4817  loss_cls: 0.1276  loss_bbox: 0.0384  loss_iou: 0.3059  d0.loss_cls: 0.1728  d0.loss_bbox: 0.0412  d0.loss_iou: 0.3214  d1.loss_cls: 0.1539  d1.loss_bbox: 0.0389  d1.loss_iou: 0.3102  d2.loss_cls: 0.1395  d2.loss_bbox: 0.0380  d2.loss_iou: 0.3096  d3.loss_cls: 0.1319  d3.loss_bbox: 0.0384  d3.loss_iou: 0.3062  d4.loss_cls: 0.1274  d4.loss_bbox: 0.0384  d4.loss_iou: 0.3058  enc_loss_cls: 0.2010  enc_loss_bbox: 0.0448  enc_loss_iou: 0.3427  dn_loss_cls: 0.0062  dn_loss_bbox: 0.0316  dn_loss_iou: 0.2615  d0.dn_loss_cls: 0.0340  d0.dn_loss_bbox: 0.0443  d0.dn_loss_iou: 0.3432  d1.dn_loss_cls: 0.0102  d1.dn_loss_bbox: 0.0340  d1.dn_loss_iou: 0.2766  d2.dn_loss_cls: 0.0073  d2.dn_loss_bbox: 0.0321  d2.dn_loss_iou: 0.2646  d3.dn_loss_cls: 0.0064  d3.dn_loss_bbox: 0.0317  d3.dn_loss_iou: 0.2620  d4.dn_loss_cls: 0.0062  d4.dn_loss_bbox: 0.0316  d4.dn_loss_iou: 0.2613  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:53:43 - mmengine - INFO - Epoch(train) [3][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:39:33  time: 1.8018  data_time: 0.0170  memory: 16268  grad_norm: 42.8082  loss: 5.1117  loss_cls: 0.1348  loss_bbox: 0.0311  loss_iou: 0.2880  d0.loss_cls: 0.1679  d0.loss_bbox: 0.0334  d0.loss_iou: 0.3051  d1.loss_cls: 0.1484  d1.loss_bbox: 0.0321  d1.loss_iou: 0.2927  d2.loss_cls: 0.1410  d2.loss_bbox: 0.0313  d2.loss_iou: 0.2881  d3.loss_cls: 0.1357  d3.loss_bbox: 0.0312  d3.loss_iou: 0.2892  d4.loss_cls: 0.1349  d4.loss_bbox: 0.0311  d4.loss_iou: 0.2884  enc_loss_cls: 0.1979  enc_loss_bbox: 0.0369  enc_loss_iou: 0.3272  dn_loss_cls: 0.0050  dn_loss_bbox: 0.0260  dn_loss_iou: 0.2365  d0.dn_loss_cls: 0.0321  d0.dn_loss_bbox: 0.0368  d0.dn_loss_iou: 0.3135  d1.dn_loss_cls: 0.0098  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2488  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0264  d2.dn_loss_iou: 0.2390  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2363  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0260  d4.dn_loss_iou: 0.2361  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:55:13 - mmengine - INFO - Epoch(train) [3][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:38:03  time: 1.8005  data_time: 0.0175  memory: 16256  grad_norm: 35.6575  loss: 5.8794  loss_cls: 0.1564  loss_bbox: 0.0385  loss_iou: 0.3456  d0.loss_cls: 0.2118  d0.loss_bbox: 0.0403  d0.loss_iou: 0.3538  d1.loss_cls: 0.1796  d1.loss_bbox: 0.0386  d1.loss_iou: 0.3468  d2.loss_cls: 0.1655  d2.loss_bbox: 0.0384  d2.loss_iou: 0.3470  d3.loss_cls: 0.1630  d3.loss_bbox: 0.0386  d3.loss_iou: 0.3453  d4.loss_cls: 0.1587  d4.loss_bbox: 0.0384  d4.loss_iou: 0.3440  enc_loss_cls: 0.2350  enc_loss_bbox: 0.0443  enc_loss_iou: 0.3733  dn_loss_cls: 0.0062  dn_loss_bbox: 0.0309  dn_loss_iou: 0.2502  d0.dn_loss_cls: 0.0333  d0.dn_loss_bbox: 0.0433  d0.dn_loss_iou: 0.3343  d1.dn_loss_cls: 0.0107  d1.dn_loss_bbox: 0.0328  d1.dn_loss_iou: 0.2641  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0313  d2.dn_loss_iou: 0.2541  d3.dn_loss_cls: 0.0067  d3.dn_loss_bbox: 0.0310  d3.dn_loss_iou: 0.2503  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0309  d4.dn_loss_iou: 0.2500  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0003  d4.loss_num: 0.0004
2025/10/29 10:56:43 - mmengine - INFO - Epoch(train) [3][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:36:32  time: 1.8008  data_time: 0.0182  memory: 16255  grad_norm: 38.3975  loss: 6.2346  loss_cls: 0.1682  loss_bbox: 0.0431  loss_iou: 0.3888  d0.loss_cls: 0.2030  d0.loss_bbox: 0.0477  d0.loss_iou: 0.4099  d1.loss_cls: 0.1813  d1.loss_bbox: 0.0438  d1.loss_iou: 0.3959  d2.loss_cls: 0.1744  d2.loss_bbox: 0.0433  d2.loss_iou: 0.3906  d3.loss_cls: 0.1777  d3.loss_bbox: 0.0429  d3.loss_iou: 0.3868  d4.loss_cls: 0.1702  d4.loss_bbox: 0.0429  d4.loss_iou: 0.3871  enc_loss_cls: 0.2301  enc_loss_bbox: 0.0509  enc_loss_iou: 0.4315  dn_loss_cls: 0.0065  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2462  d0.dn_loss_cls: 0.0348  d0.dn_loss_bbox: 0.0386  d0.dn_loss_iou: 0.3271  d1.dn_loss_cls: 0.0110  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2580  d2.dn_loss_cls: 0.0078  d2.dn_loss_bbox: 0.0275  d2.dn_loss_iou: 0.2487  d3.dn_loss_cls: 0.0069  d3.dn_loss_bbox: 0.0273  d3.dn_loss_iou: 0.2460  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2459  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0005  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 10:57:38 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 10:58:14 - mmengine - INFO - Epoch(train) [3][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:35:03  time: 1.8177  data_time: 0.0166  memory: 16269  grad_norm: 36.7362  loss: 5.2921  loss_cls: 0.1389  loss_bbox: 0.0328  loss_iou: 0.3155  d0.loss_cls: 0.1691  d0.loss_bbox: 0.0351  d0.loss_iou: 0.3346  d1.loss_cls: 0.1524  d1.loss_bbox: 0.0332  d1.loss_iou: 0.3244  d2.loss_cls: 0.1457  d2.loss_bbox: 0.0327  d2.loss_iou: 0.3202  d3.loss_cls: 0.1443  d3.loss_bbox: 0.0323  d3.loss_iou: 0.3149  d4.loss_cls: 0.1397  d4.loss_bbox: 0.0323  d4.loss_iou: 0.3148  enc_loss_cls: 0.1915  enc_loss_bbox: 0.0388  enc_loss_iou: 0.3603  dn_loss_cls: 0.0053  dn_loss_bbox: 0.0248  dn_loss_iou: 0.2287  d0.dn_loss_cls: 0.0277  d0.dn_loss_bbox: 0.0347  d0.dn_loss_iou: 0.3051  d1.dn_loss_cls: 0.0089  d1.dn_loss_bbox: 0.0265  d1.dn_loss_iou: 0.2427  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0252  d2.dn_loss_iou: 0.2318  d3.dn_loss_cls: 0.0056  d3.dn_loss_bbox: 0.0248  d3.dn_loss_iou: 0.2288  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0248  d4.dn_loss_iou: 0.2285  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 10:59:44 - mmengine - INFO - Epoch(train) [3][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:33:32  time: 1.8005  data_time: 0.0165  memory: 16269  grad_norm: 39.3938  loss: 5.2350  loss_cls: 0.1253  loss_bbox: 0.0362  loss_iou: 0.2985  d0.loss_cls: 0.1631  d0.loss_bbox: 0.0329  d0.loss_iou: 0.3151  d1.loss_cls: 0.1379  d1.loss_bbox: 0.0328  d1.loss_iou: 0.3085  d2.loss_cls: 0.1336  d2.loss_bbox: 0.0312  d2.loss_iou: 0.3001  d3.loss_cls: 0.1299  d3.loss_bbox: 0.0309  d3.loss_iou: 0.2950  d4.loss_cls: 0.1299  d4.loss_bbox: 0.0315  d4.loss_iou: 0.2959  enc_loss_cls: 0.1858  enc_loss_bbox: 0.0357  enc_loss_iou: 0.3314  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2543  d0.dn_loss_cls: 0.0277  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.3292  d1.dn_loss_cls: 0.0093  d1.dn_loss_bbox: 0.0288  d1.dn_loss_iou: 0.2648  d2.dn_loss_cls: 0.0069  d2.dn_loss_bbox: 0.0278  d2.dn_loss_iou: 0.2563  d3.dn_loss_cls: 0.0061  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2540  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2540  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 11:01:14 - mmengine - INFO - Epoch(train) [3][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:32:02  time: 1.7987  data_time: 0.0168  memory: 16264  grad_norm: 37.5795  loss: 4.8913  loss_cls: 0.1256  loss_bbox: 0.0323  loss_iou: 0.2825  d0.loss_cls: 0.1656  d0.loss_bbox: 0.0343  d0.loss_iou: 0.2974  d1.loss_cls: 0.1430  d1.loss_bbox: 0.0332  d1.loss_iou: 0.2896  d2.loss_cls: 0.1338  d2.loss_bbox: 0.0325  d2.loss_iou: 0.2857  d3.loss_cls: 0.1313  d3.loss_bbox: 0.0323  d3.loss_iou: 0.2829  d4.loss_cls: 0.1261  d4.loss_bbox: 0.0322  d4.loss_iou: 0.2825  enc_loss_cls: 0.1918  enc_loss_bbox: 0.0376  enc_loss_iou: 0.3148  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0242  dn_loss_iou: 0.2178  d0.dn_loss_cls: 0.0254  d0.dn_loss_bbox: 0.0332  d0.dn_loss_iou: 0.2860  d1.dn_loss_cls: 0.0085  d1.dn_loss_bbox: 0.0255  d1.dn_loss_iou: 0.2291  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0244  d2.dn_loss_iou: 0.2200  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0242  d3.dn_loss_iou: 0.2177  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0242  d4.dn_loss_iou: 0.2176  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:02:44 - mmengine - INFO - Epoch(train) [3][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:30:31  time: 1.8006  data_time: 0.0173  memory: 16255  grad_norm: 44.5789  loss: 5.2639  loss_cls: 0.1135  loss_bbox: 0.0357  loss_iou: 0.3048  d0.loss_cls: 0.1437  d0.loss_bbox: 0.0396  d0.loss_iou: 0.3250  d1.loss_cls: 0.1322  d1.loss_bbox: 0.0360  d1.loss_iou: 0.3043  d2.loss_cls: 0.1252  d2.loss_bbox: 0.0356  d2.loss_iou: 0.3019  d3.loss_cls: 0.1197  d3.loss_bbox: 0.0355  d3.loss_iou: 0.3019  d4.loss_cls: 0.1151  d4.loss_bbox: 0.0356  d4.loss_iou: 0.3027  enc_loss_cls: 0.1684  enc_loss_bbox: 0.0428  enc_loss_iou: 0.3402  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0293  dn_loss_iou: 0.2597  d0.dn_loss_cls: 0.0289  d0.dn_loss_bbox: 0.0406  d0.dn_loss_iou: 0.3394  d1.dn_loss_cls: 0.0095  d1.dn_loss_bbox: 0.0309  d1.dn_loss_iou: 0.2724  d2.dn_loss_cls: 0.0063  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2616  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0293  d3.dn_loss_iou: 0.2595  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0293  d4.dn_loss_iou: 0.2592  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0003  d4.loss_num: 0.0004
2025/10/29 11:04:14 - mmengine - INFO - Epoch(train) [3][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:29:01  time: 1.8023  data_time: 0.0173  memory: 16286  grad_norm: 49.7568  loss: 5.0388  loss_cls: 0.1285  loss_bbox: 0.0368  loss_iou: 0.2781  d0.loss_cls: 0.1663  d0.loss_bbox: 0.0386  d0.loss_iou: 0.2912  d1.loss_cls: 0.1392  d1.loss_bbox: 0.0373  d1.loss_iou: 0.2841  d2.loss_cls: 0.1310  d2.loss_bbox: 0.0366  d2.loss_iou: 0.2794  d3.loss_cls: 0.1299  d3.loss_bbox: 0.0365  d3.loss_iou: 0.2782  d4.loss_cls: 0.1267  d4.loss_bbox: 0.0365  d4.loss_iou: 0.2782  enc_loss_cls: 0.1894  enc_loss_bbox: 0.0419  enc_loss_iou: 0.3118  dn_loss_cls: 0.0159  dn_loss_bbox: 0.0269  dn_loss_iou: 0.2339  d0.dn_loss_cls: 0.0308  d0.dn_loss_bbox: 0.0375  d0.dn_loss_iou: 0.3100  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0284  d1.dn_loss_iou: 0.2455  d2.dn_loss_cls: 0.0114  d2.dn_loss_bbox: 0.0272  d2.dn_loss_iou: 0.2355  d3.dn_loss_cls: 0.0110  d3.dn_loss_bbox: 0.0269  d3.dn_loss_iou: 0.2334  d4.dn_loss_cls: 0.0137  d4.dn_loss_bbox: 0.0269  d4.dn_loss_iou: 0.2333  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 11:05:44 - mmengine - INFO - Epoch(train) [3][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:27:30  time: 1.7934  data_time: 0.0173  memory: 16259  grad_norm: 40.5912  loss: 4.9804  loss_cls: 0.1127  loss_bbox: 0.0358  loss_iou: 0.2765  d0.loss_cls: 0.1501  d0.loss_bbox: 0.0376  d0.loss_iou: 0.2949  d1.loss_cls: 0.1299  d1.loss_bbox: 0.0360  d1.loss_iou: 0.2838  d2.loss_cls: 0.1220  d2.loss_bbox: 0.0366  d2.loss_iou: 0.2799  d3.loss_cls: 0.1214  d3.loss_bbox: 0.0347  d3.loss_iou: 0.2731  d4.loss_cls: 0.1107  d4.loss_bbox: 0.0361  d4.loss_iou: 0.2780  enc_loss_cls: 0.1823  enc_loss_bbox: 0.0433  enc_loss_iou: 0.3186  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2416  d0.dn_loss_cls: 0.0313  d0.dn_loss_bbox: 0.0409  d0.dn_loss_iou: 0.3147  d1.dn_loss_cls: 0.0083  d1.dn_loss_bbox: 0.0313  d1.dn_loss_iou: 0.2525  d2.dn_loss_cls: 0.0055  d2.dn_loss_bbox: 0.0298  d2.dn_loss_iou: 0.2429  d3.dn_loss_cls: 0.0048  d3.dn_loss_bbox: 0.0296  d3.dn_loss_iou: 0.2412  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2414  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0003
2025/10/29 11:07:14 - mmengine - INFO - Epoch(train) [3][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:26:00  time: 1.8103  data_time: 0.0172  memory: 16268  grad_norm: 39.7098  loss: 5.1251  loss_cls: 0.1290  loss_bbox: 0.0350  loss_iou: 0.3036  d0.loss_cls: 0.1553  d0.loss_bbox: 0.0376  d0.loss_iou: 0.3217  d1.loss_cls: 0.1412  d1.loss_bbox: 0.0354  d1.loss_iou: 0.3133  d2.loss_cls: 0.1354  d2.loss_bbox: 0.0345  d2.loss_iou: 0.3122  d3.loss_cls: 0.1345  d3.loss_bbox: 0.0344  d3.loss_iou: 0.3032  d4.loss_cls: 0.1311  d4.loss_bbox: 0.0343  d4.loss_iou: 0.3039  enc_loss_cls: 0.1851  enc_loss_bbox: 0.0393  enc_loss_iou: 0.3400  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0270  dn_loss_iou: 0.2238  d0.dn_loss_cls: 0.0295  d0.dn_loss_bbox: 0.0375  d0.dn_loss_iou: 0.2926  d1.dn_loss_cls: 0.0094  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2357  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0273  d2.dn_loss_iou: 0.2260  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0271  d3.dn_loss_iou: 0.2240  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0270  d4.dn_loss_iou: 0.2235  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 11:08:44 - mmengine - INFO - Epoch(train) [3][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:24:30  time: 1.8063  data_time: 0.0177  memory: 16280  grad_norm: 40.5717  loss: 4.5691  loss_cls: 0.1056  loss_bbox: 0.0299  loss_iou: 0.2468  d0.loss_cls: 0.1447  d0.loss_bbox: 0.0311  d0.loss_iou: 0.2556  d1.loss_cls: 0.1181  d1.loss_bbox: 0.0312  d1.loss_iou: 0.2537  d2.loss_cls: 0.1117  d2.loss_bbox: 0.0299  d2.loss_iou: 0.2472  d3.loss_cls: 0.1055  d3.loss_bbox: 0.0298  d3.loss_iou: 0.2466  d4.loss_cls: 0.1015  d4.loss_bbox: 0.0342  d4.loss_iou: 0.2496  enc_loss_cls: 0.1620  enc_loss_bbox: 0.0343  enc_loss_iou: 0.2748  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0282  dn_loss_iou: 0.2333  d0.dn_loss_cls: 0.0246  d0.dn_loss_bbox: 0.0391  d0.dn_loss_iou: 0.3080  d1.dn_loss_cls: 0.0079  d1.dn_loss_bbox: 0.0298  d1.dn_loss_iou: 0.2457  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0285  d2.dn_loss_iou: 0.2359  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0282  d3.dn_loss_iou: 0.2329  d4.dn_loss_cls: 0.0047  d4.dn_loss_bbox: 0.0282  d4.dn_loss_iou: 0.2330  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:10:15 - mmengine - INFO - Epoch(train) [3][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:23:00  time: 1.8051  data_time: 0.0167  memory: 16256  grad_norm: 39.1816  loss: 5.1039  loss_cls: 0.1287  loss_bbox: 0.0322  loss_iou: 0.2840  d0.loss_cls: 0.1615  d0.loss_bbox: 0.0337  d0.loss_iou: 0.2992  d1.loss_cls: 0.1392  d1.loss_bbox: 0.0324  d1.loss_iou: 0.2890  d2.loss_cls: 0.1299  d2.loss_bbox: 0.0323  d2.loss_iou: 0.2868  d3.loss_cls: 0.1283  d3.loss_bbox: 0.0321  d3.loss_iou: 0.2835  d4.loss_cls: 0.1287  d4.loss_bbox: 0.0321  d4.loss_iou: 0.2834  enc_loss_cls: 0.1858  enc_loss_bbox: 0.0380  enc_loss_iou: 0.3248  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0277  dn_loss_iou: 0.2460  d0.dn_loss_cls: 0.0294  d0.dn_loss_bbox: 0.0400  d0.dn_loss_iou: 0.3291  d1.dn_loss_cls: 0.0084  d1.dn_loss_bbox: 0.0298  d1.dn_loss_iou: 0.2601  d2.dn_loss_cls: 0.0058  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2495  d3.dn_loss_cls: 0.0050  d3.dn_loss_bbox: 0.0278  d3.dn_loss_iou: 0.2461  d4.dn_loss_cls: 0.0047  d4.dn_loss_bbox: 0.0277  d4.dn_loss_iou: 0.2458  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:11:45 - mmengine - INFO - Epoch(train) [3][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:21:30  time: 1.8111  data_time: 0.0170  memory: 16269  grad_norm: 38.3305  loss: 5.8150  loss_cls: 0.1359  loss_bbox: 0.0404  loss_iou: 0.3696  d0.loss_cls: 0.1776  d0.loss_bbox: 0.0434  d0.loss_iou: 0.3914  d1.loss_cls: 0.1552  d1.loss_bbox: 0.0413  d1.loss_iou: 0.3760  d2.loss_cls: 0.1453  d2.loss_bbox: 0.0409  d2.loss_iou: 0.3722  d3.loss_cls: 0.1406  d3.loss_bbox: 0.0405  d3.loss_iou: 0.3703  d4.loss_cls: 0.1363  d4.loss_bbox: 0.0404  d4.loss_iou: 0.3695  enc_loss_cls: 0.2038  enc_loss_bbox: 0.0470  enc_loss_iou: 0.4130  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2397  d0.dn_loss_cls: 0.0270  d0.dn_loss_bbox: 0.0378  d0.dn_loss_iou: 0.3138  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2544  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0279  d2.dn_loss_iou: 0.2426  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2397  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2395  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:13:15 - mmengine - INFO - Epoch(train) [3][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:20:00  time: 1.8033  data_time: 0.0173  memory: 16265  grad_norm: 38.2363  loss: 5.2521  loss_cls: 0.1230  loss_bbox: 0.0347  loss_iou: 0.2980  d0.loss_cls: 0.1563  d0.loss_bbox: 0.0378  d0.loss_iou: 0.3160  d1.loss_cls: 0.1348  d1.loss_bbox: 0.0367  d1.loss_iou: 0.3091  d2.loss_cls: 0.1242  d2.loss_bbox: 0.0355  d2.loss_iou: 0.3060  d3.loss_cls: 0.1251  d3.loss_bbox: 0.0346  d3.loss_iou: 0.2980  d4.loss_cls: 0.1222  d4.loss_bbox: 0.0348  d4.loss_iou: 0.2995  enc_loss_cls: 0.1881  enc_loss_bbox: 0.0413  enc_loss_iou: 0.3368  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0306  dn_loss_iou: 0.2485  d0.dn_loss_cls: 0.0310  d0.dn_loss_bbox: 0.0426  d0.dn_loss_iou: 0.3305  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0329  d1.dn_loss_iou: 0.2633  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0313  d2.dn_loss_iou: 0.2517  d3.dn_loss_cls: 0.0066  d3.dn_loss_bbox: 0.0308  d3.dn_loss_iou: 0.2487  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0306  d4.dn_loss_iou: 0.2484  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:14:45 - mmengine - INFO - Epoch(train) [3][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:18:29  time: 1.7956  data_time: 0.0170  memory: 16269  grad_norm: 35.7471  loss: 4.7035  loss_cls: 0.1074  loss_bbox: 0.0308  loss_iou: 0.2707  d0.loss_cls: 0.1457  d0.loss_bbox: 0.0331  d0.loss_iou: 0.2889  d1.loss_cls: 0.1279  d1.loss_bbox: 0.0315  d1.loss_iou: 0.2750  d2.loss_cls: 0.1187  d2.loss_bbox: 0.0310  d2.loss_iou: 0.2713  d3.loss_cls: 0.1131  d3.loss_bbox: 0.0309  d3.loss_iou: 0.2711  d4.loss_cls: 0.1064  d4.loss_bbox: 0.0309  d4.loss_iou: 0.2714  enc_loss_cls: 0.1687  enc_loss_bbox: 0.0369  enc_loss_iou: 0.3094  dn_loss_cls: 0.0023  dn_loss_bbox: 0.0272  dn_loss_iou: 0.2218  d0.dn_loss_cls: 0.0238  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.2927  d1.dn_loss_cls: 0.0056  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2333  d2.dn_loss_cls: 0.0031  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2244  d3.dn_loss_cls: 0.0025  d3.dn_loss_bbox: 0.0272  d3.dn_loss_iou: 0.2217  d4.dn_loss_cls: 0.0023  d4.dn_loss_bbox: 0.0272  d4.dn_loss_iou: 0.2216  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0004
2025/10/29 11:16:16 - mmengine - INFO - Epoch(train) [3][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:16:59  time: 1.8074  data_time: 0.0171  memory: 16265  grad_norm: 38.8088  loss: 5.3868  loss_cls: 0.1222  loss_bbox: 0.0357  loss_iou: 0.3286  d0.loss_cls: 0.1588  d0.loss_bbox: 0.0377  d0.loss_iou: 0.3424  d1.loss_cls: 0.1350  d1.loss_bbox: 0.0362  d1.loss_iou: 0.3333  d2.loss_cls: 0.1264  d2.loss_bbox: 0.0360  d2.loss_iou: 0.3300  d3.loss_cls: 0.1247  d3.loss_bbox: 0.0360  d3.loss_iou: 0.3299  d4.loss_cls: 0.1239  d4.loss_bbox: 0.0356  d4.loss_iou: 0.3272  enc_loss_cls: 0.1847  enc_loss_bbox: 0.0400  enc_loss_iou: 0.3606  dn_loss_cls: 0.0069  dn_loss_bbox: 0.0272  dn_loss_iou: 0.2450  d0.dn_loss_cls: 0.0265  d0.dn_loss_bbox: 0.0375  d0.dn_loss_iou: 0.3240  d1.dn_loss_cls: 0.0088  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2573  d2.dn_loss_cls: 0.0069  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2465  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0272  d3.dn_loss_iou: 0.2447  d4.dn_loss_cls: 0.0067  d4.dn_loss_bbox: 0.0272  d4.dn_loss_iou: 0.2448  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 11:17:46 - mmengine - INFO - Epoch(train) [3][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:15:29  time: 1.8048  data_time: 0.0177  memory: 16256  grad_norm: 35.2111  loss: 5.7062  loss_cls: 0.1370  loss_bbox: 0.0369  loss_iou: 0.3449  d0.loss_cls: 0.1891  d0.loss_bbox: 0.0407  d0.loss_iou: 0.3691  d1.loss_cls: 0.1562  d1.loss_bbox: 0.0389  d1.loss_iou: 0.3586  d2.loss_cls: 0.1487  d2.loss_bbox: 0.0372  d2.loss_iou: 0.3484  d3.loss_cls: 0.1421  d3.loss_bbox: 0.0371  d3.loss_iou: 0.3447  d4.loss_cls: 0.1414  d4.loss_bbox: 0.0366  d4.loss_iou: 0.3411  enc_loss_cls: 0.2174  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3879  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2455  d0.dn_loss_cls: 0.0317  d0.dn_loss_bbox: 0.0369  d0.dn_loss_iou: 0.3187  d1.dn_loss_cls: 0.0109  d1.dn_loss_bbox: 0.0288  d1.dn_loss_iou: 0.2579  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0276  d2.dn_loss_iou: 0.2481  d3.dn_loss_cls: 0.0065  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2457  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2453  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0006  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 11:19:16 - mmengine - INFO - Epoch(train) [3][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:13:58  time: 1.8000  data_time: 0.0167  memory: 16252  grad_norm: 35.7073  loss: 5.6157  loss_cls: 0.1441  loss_bbox: 0.0342  loss_iou: 0.3357  d0.loss_cls: 0.1916  d0.loss_bbox: 0.0390  d0.loss_iou: 0.3584  d1.loss_cls: 0.1652  d1.loss_bbox: 0.0358  d1.loss_iou: 0.3453  d2.loss_cls: 0.1552  d2.loss_bbox: 0.0356  d2.loss_iou: 0.3412  d3.loss_cls: 0.1480  d3.loss_bbox: 0.0350  d3.loss_iou: 0.3404  d4.loss_cls: 0.1445  d4.loss_bbox: 0.0342  d4.loss_iou: 0.3372  enc_loss_cls: 0.2022  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3873  dn_loss_cls: 0.0050  dn_loss_bbox: 0.0263  dn_loss_iou: 0.2404  d0.dn_loss_cls: 0.0274  d0.dn_loss_bbox: 0.0364  d0.dn_loss_iou: 0.3176  d1.dn_loss_cls: 0.0085  d1.dn_loss_bbox: 0.0277  d1.dn_loss_iou: 0.2529  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0266  d2.dn_loss_iou: 0.2427  d3.dn_loss_cls: 0.0051  d3.dn_loss_bbox: 0.0263  d3.dn_loss_iou: 0.2402  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0262  d4.dn_loss_iou: 0.2401  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:20:46 - mmengine - INFO - Epoch(train) [3][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:12:28  time: 1.8102  data_time: 0.0172  memory: 16256  grad_norm: 38.0489  loss: 4.1660  loss_cls: 0.0971  loss_bbox: 0.0269  loss_iou: 0.2366  d0.loss_cls: 0.1317  d0.loss_bbox: 0.0284  d0.loss_iou: 0.2474  d1.loss_cls: 0.1094  d1.loss_bbox: 0.0273  d1.loss_iou: 0.2413  d2.loss_cls: 0.1025  d2.loss_bbox: 0.0272  d2.loss_iou: 0.2383  d3.loss_cls: 0.0988  d3.loss_bbox: 0.0271  d3.loss_iou: 0.2372  d4.loss_cls: 0.0960  d4.loss_bbox: 0.0271  d4.loss_iou: 0.2370  enc_loss_cls: 0.1561  enc_loss_bbox: 0.0305  enc_loss_iou: 0.2633  dn_loss_cls: 0.0034  dn_loss_bbox: 0.0239  dn_loss_iou: 0.2003  d0.dn_loss_cls: 0.0212  d0.dn_loss_bbox: 0.0333  d0.dn_loss_iou: 0.2657  d1.dn_loss_cls: 0.0069  d1.dn_loss_bbox: 0.0252  d1.dn_loss_iou: 0.2102  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0242  d2.dn_loss_iou: 0.2025  d3.dn_loss_cls: 0.0037  d3.dn_loss_bbox: 0.0239  d3.dn_loss_iou: 0.2003  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0239  d4.dn_loss_iou: 0.2000  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:22:17 - mmengine - INFO - Epoch(train) [3][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:10:58  time: 1.8066  data_time: 0.0167  memory: 16256  grad_norm: 42.5425  loss: 5.0111  loss_cls: 0.1086  loss_bbox: 0.0340  loss_iou: 0.2936  d0.loss_cls: 0.1431  d0.loss_bbox: 0.0365  d0.loss_iou: 0.3068  d1.loss_cls: 0.1233  d1.loss_bbox: 0.0351  d1.loss_iou: 0.2960  d2.loss_cls: 0.1143  d2.loss_bbox: 0.0347  d2.loss_iou: 0.2956  d3.loss_cls: 0.1116  d3.loss_bbox: 0.0342  d3.loss_iou: 0.2925  d4.loss_cls: 0.1099  d4.loss_bbox: 0.0339  d4.loss_iou: 0.2936  enc_loss_cls: 0.1687  enc_loss_bbox: 0.0396  enc_loss_iou: 0.3220  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2415  d0.dn_loss_cls: 0.0290  d0.dn_loss_bbox: 0.0404  d0.dn_loss_iou: 0.3203  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0305  d1.dn_loss_iou: 0.2523  d2.dn_loss_cls: 0.0049  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2435  d3.dn_loss_cls: 0.0043  d3.dn_loss_bbox: 0.0290  d3.dn_loss_iou: 0.2412  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2412  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:23:47 - mmengine - INFO - Epoch(train) [3][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:09:28  time: 1.8097  data_time: 0.0173  memory: 16265  grad_norm: 36.9165  loss: 5.2182  loss_cls: 0.1172  loss_bbox: 0.0348  loss_iou: 0.3214  d0.loss_cls: 0.1587  d0.loss_bbox: 0.0365  d0.loss_iou: 0.3375  d1.loss_cls: 0.1386  d1.loss_bbox: 0.0356  d1.loss_iou: 0.3249  d2.loss_cls: 0.1247  d2.loss_bbox: 0.0354  d2.loss_iou: 0.3238  d3.loss_cls: 0.1200  d3.loss_bbox: 0.0347  d3.loss_iou: 0.3212  d4.loss_cls: 0.1173  d4.loss_bbox: 0.0348  d4.loss_iou: 0.3219  enc_loss_cls: 0.1851  enc_loss_bbox: 0.0401  enc_loss_iou: 0.3616  dn_loss_cls: 0.0079  dn_loss_bbox: 0.0252  dn_loss_iou: 0.2286  d0.dn_loss_cls: 0.0282  d0.dn_loss_bbox: 0.0341  d0.dn_loss_iou: 0.2954  d1.dn_loss_cls: 0.0126  d1.dn_loss_bbox: 0.0267  d1.dn_loss_iou: 0.2418  d2.dn_loss_cls: 0.0095  d2.dn_loss_bbox: 0.0255  d2.dn_loss_iou: 0.2314  d3.dn_loss_cls: 0.0084  d3.dn_loss_bbox: 0.0252  d3.dn_loss_iou: 0.2286  d4.dn_loss_cls: 0.0079  d4.dn_loss_bbox: 0.0251  d4.dn_loss_iou: 0.2283  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:25:17 - mmengine - INFO - Epoch(train) [3][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:07:58  time: 1.8056  data_time: 0.0172  memory: 16258  grad_norm: 40.9231  loss: 4.6146  loss_cls: 0.1114  loss_bbox: 0.0299  loss_iou: 0.2592  d0.loss_cls: 0.1496  d0.loss_bbox: 0.0310  d0.loss_iou: 0.2709  d1.loss_cls: 0.1284  d1.loss_bbox: 0.0304  d1.loss_iou: 0.2645  d2.loss_cls: 0.1195  d2.loss_bbox: 0.0301  d2.loss_iou: 0.2608  d3.loss_cls: 0.1166  d3.loss_bbox: 0.0299  d3.loss_iou: 0.2590  d4.loss_cls: 0.1131  d4.loss_bbox: 0.0299  d4.loss_iou: 0.2590  enc_loss_cls: 0.1677  enc_loss_bbox: 0.0337  enc_loss_iou: 0.2866  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0262  dn_loss_iou: 0.2213  d0.dn_loss_cls: 0.0245  d0.dn_loss_bbox: 0.0363  d0.dn_loss_iou: 0.2912  d1.dn_loss_cls: 0.0074  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2335  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0265  d2.dn_loss_iou: 0.2236  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0262  d3.dn_loss_iou: 0.2212  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0262  d4.dn_loss_iou: 0.2211  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0003  d4.loss_num: 0.0004
2025/10/29 11:26:47 - mmengine - INFO - Epoch(train) [3][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:06:27  time: 1.7899  data_time: 0.0169  memory: 16269  grad_norm: 38.8108  loss: 5.3607  loss_cls: 0.1144  loss_bbox: 0.0345  loss_iou: 0.3287  d0.loss_cls: 0.1896  d0.loss_bbox: 0.0366  d0.loss_iou: 0.3462  d1.loss_cls: 0.1414  d1.loss_bbox: 0.0352  d1.loss_iou: 0.3341  d2.loss_cls: 0.1234  d2.loss_bbox: 0.0348  d2.loss_iou: 0.3314  d3.loss_cls: 0.1205  d3.loss_bbox: 0.0347  d3.loss_iou: 0.3326  d4.loss_cls: 0.1145  d4.loss_bbox: 0.0345  d4.loss_iou: 0.3289  enc_loss_cls: 0.1799  enc_loss_bbox: 0.0405  enc_loss_iou: 0.3706  dn_loss_cls: 0.0061  dn_loss_bbox: 0.0260  dn_loss_iou: 0.2369  d0.dn_loss_cls: 0.0310  d0.dn_loss_bbox: 0.0367  d0.dn_loss_iou: 0.3175  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0276  d1.dn_loss_iou: 0.2491  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0264  d2.dn_loss_iou: 0.2398  d3.dn_loss_cls: 0.0064  d3.dn_loss_bbox: 0.0259  d3.dn_loss_iou: 0.2367  d4.dn_loss_cls: 0.0061  d4.dn_loss_bbox: 0.0259  d4.dn_loss_iou: 0.2366  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:27:41 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 11:28:18 - mmengine - INFO - Epoch(train) [3][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:04:57  time: 1.8137  data_time: 0.0181  memory: 16262  grad_norm: 35.4582  loss: 5.0247  loss_cls: 0.1150  loss_bbox: 0.0376  loss_iou: 0.2997  d0.loss_cls: 0.1683  d0.loss_bbox: 0.0389  d0.loss_iou: 0.3139  d1.loss_cls: 0.1343  d1.loss_bbox: 0.0382  d1.loss_iou: 0.3069  d2.loss_cls: 0.1240  d2.loss_bbox: 0.0380  d2.loss_iou: 0.3017  d3.loss_cls: 0.1170  d3.loss_bbox: 0.0376  d3.loss_iou: 0.3017  d4.loss_cls: 0.1150  d4.loss_bbox: 0.0381  d4.loss_iou: 0.3025  enc_loss_cls: 0.1770  enc_loss_bbox: 0.0443  enc_loss_iou: 0.3406  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0260  dn_loss_iou: 0.2204  d0.dn_loss_cls: 0.0252  d0.dn_loss_bbox: 0.0360  d0.dn_loss_iou: 0.2912  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0276  d1.dn_loss_iou: 0.2330  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0264  d2.dn_loss_iou: 0.2231  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2207  d4.dn_loss_cls: 0.0052  d4.dn_loss_bbox: 0.0260  d4.dn_loss_iou: 0.2202  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:29:48 - mmengine - INFO - Epoch(train) [3][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:03:27  time: 1.8007  data_time: 0.0168  memory: 16265  grad_norm: 43.2825  loss: 5.1314  loss_cls: 0.0971  loss_bbox: 0.0364  loss_iou: 0.3044  d0.loss_cls: 0.1509  d0.loss_bbox: 0.0403  d0.loss_iou: 0.3170  d1.loss_cls: 0.1193  d1.loss_bbox: 0.0374  d1.loss_iou: 0.3119  d2.loss_cls: 0.1074  d2.loss_bbox: 0.0365  d2.loss_iou: 0.3051  d3.loss_cls: 0.1046  d3.loss_bbox: 0.0364  d3.loss_iou: 0.3042  d4.loss_cls: 0.0982  d4.loss_bbox: 0.0364  d4.loss_iou: 0.3040  enc_loss_cls: 0.1667  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3458  dn_loss_cls: 0.0060  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2466  d0.dn_loss_cls: 0.0281  d0.dn_loss_bbox: 0.0398  d0.dn_loss_iou: 0.3240  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0309  d1.dn_loss_iou: 0.2606  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2500  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2467  d4.dn_loss_cls: 0.0060  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2463  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:30:50 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 11:30:50 - mmengine - INFO - Saving checkpoint at 3 epochs
2025/10/29 11:31:01 - mmengine - INFO - Epoch(val) [3][ 50/429]    eta: 0:00:34  time: 0.0921  data_time: 0.0030  memory: 16255  
2025/10/29 11:31:06 - mmengine - INFO - Epoch(val) [3][100/429]    eta: 0:00:29  time: 0.0886  data_time: 0.0022  memory: 3164  
2025/10/29 11:31:10 - mmengine - INFO - Epoch(val) [3][150/429]    eta: 0:00:25  time: 0.0886  data_time: 0.0022  memory: 3169  
2025/10/29 11:31:14 - mmengine - INFO - Epoch(val) [3][200/429]    eta: 0:00:20  time: 0.0887  data_time: 0.0022  memory: 3165  
2025/10/29 11:31:19 - mmengine - INFO - Epoch(val) [3][250/429]    eta: 0:00:15  time: 0.0888  data_time: 0.0022  memory: 3167  
2025/10/29 11:31:23 - mmengine - INFO - Epoch(val) [3][300/429]    eta: 0:00:11  time: 0.0885  data_time: 0.0022  memory: 3175  
2025/10/29 11:31:28 - mmengine - INFO - Epoch(val) [3][350/429]    eta: 0:00:07  time: 0.0884  data_time: 0.0022  memory: 3169  
2025/10/29 11:31:32 - mmengine - INFO - Epoch(val) [3][400/429]    eta: 0:00:02  time: 0.0881  data_time: 0.0023  memory: 3165  
2025/10/29 11:31:37 - mmengine - INFO - {'instance_F1_score': 0.6886055934380868, 'instance_acc': 0.5286109656668412, 'image_F1_score': 0.5615019421665948, 'image_acc': 0.40792540792540793}
2025/10/29 11:31:37 - mmengine - INFO - Epoch(val) [3][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.6886  grefcoco_val/refdrone/instance_acc: 0.5286  grefcoco_val/refdrone/image_F1_score: 0.5615  grefcoco_val/refdrone/image_acc: 0.4079  data_time: 0.0023  time: 0.0888
2025/10/29 11:33:07 - mmengine - INFO - Epoch(train) [4][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:00:53  time: 1.8044  data_time: 0.0169  memory: 16269  grad_norm: 33.8316  loss: 5.2275  loss_cls: 0.1112  loss_bbox: 0.0395  loss_iou: 0.3179  d0.loss_cls: 0.1427  d0.loss_bbox: 0.0414  d0.loss_iou: 0.3363  d1.loss_cls: 0.1272  d1.loss_bbox: 0.0399  d1.loss_iou: 0.3214  d2.loss_cls: 0.1213  d2.loss_bbox: 0.0384  d2.loss_iou: 0.3148  d3.loss_cls: 0.1168  d3.loss_bbox: 0.0383  d3.loss_iou: 0.3148  d4.loss_cls: 0.1124  d4.loss_bbox: 0.0392  d4.loss_iou: 0.3157  enc_loss_cls: 0.1671  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3522  dn_loss_cls: 0.0046  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2417  d0.dn_loss_cls: 0.0277  d0.dn_loss_bbox: 0.0380  d0.dn_loss_iou: 0.3175  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2541  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.0279  d2.dn_loss_iou: 0.2438  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2421  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0275  d4.dn_loss_iou: 0.2414  loss_num: 0.0003  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:34:38 - mmengine - INFO - Epoch(train) [4][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:59:23  time: 1.8177  data_time: 0.0188  memory: 16269  grad_norm: 35.8684  loss: 4.9014  loss_cls: 0.1181  loss_bbox: 0.0318  loss_iou: 0.2815  d0.loss_cls: 0.1661  d0.loss_bbox: 0.0333  d0.loss_iou: 0.2969  d1.loss_cls: 0.1342  d1.loss_bbox: 0.0319  d1.loss_iou: 0.2854  d2.loss_cls: 0.1224  d2.loss_bbox: 0.0320  d2.loss_iou: 0.2844  d3.loss_cls: 0.1180  d3.loss_bbox: 0.0318  d3.loss_iou: 0.2829  d4.loss_cls: 0.1162  d4.loss_bbox: 0.0318  d4.loss_iou: 0.2818  enc_loss_cls: 0.1804  enc_loss_bbox: 0.0367  enc_loss_iou: 0.3205  dn_loss_cls: 0.0062  dn_loss_bbox: 0.0258  dn_loss_iou: 0.2246  d0.dn_loss_cls: 0.0305  d0.dn_loss_bbox: 0.0366  d0.dn_loss_iou: 0.3048  d1.dn_loss_cls: 0.0110  d1.dn_loss_bbox: 0.0275  d1.dn_loss_iou: 0.2384  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0263  d2.dn_loss_iou: 0.2282  d3.dn_loss_cls: 0.0066  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2252  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0258  d4.dn_loss_iou: 0.2246  loss_num: 0.0004  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 11:36:07 - mmengine - INFO - Epoch(train) [4][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:57:53  time: 1.7905  data_time: 0.0170  memory: 16280  grad_norm: 38.1189  loss: 4.6171  loss_cls: 0.0957  loss_bbox: 0.0315  loss_iou: 0.2894  d0.loss_cls: 0.1389  d0.loss_bbox: 0.0345  d0.loss_iou: 0.3086  d1.loss_cls: 0.1134  d1.loss_bbox: 0.0331  d1.loss_iou: 0.2987  d2.loss_cls: 0.0997  d2.loss_bbox: 0.0330  d2.loss_iou: 0.2955  d3.loss_cls: 0.0993  d3.loss_bbox: 0.0314  d3.loss_iou: 0.2890  d4.loss_cls: 0.0964  d4.loss_bbox: 0.0315  d4.loss_iou: 0.2887  enc_loss_cls: 0.1656  enc_loss_bbox: 0.0377  enc_loss_iou: 0.3291  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0222  dn_loss_iou: 0.2003  d0.dn_loss_cls: 0.0238  d0.dn_loss_bbox: 0.0307  d0.dn_loss_iou: 0.2658  d1.dn_loss_cls: 0.0078  d1.dn_loss_bbox: 0.0235  d1.dn_loss_iou: 0.2112  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0225  d2.dn_loss_iou: 0.2028  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0222  d3.dn_loss_iou: 0.2004  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0222  d4.dn_loss_iou: 0.2001  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:37:37 - mmengine - INFO - Epoch(train) [4][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:56:22  time: 1.8005  data_time: 0.0170  memory: 16274  grad_norm: 40.0508  loss: 5.1559  loss_cls: 0.1079  loss_bbox: 0.0388  loss_iou: 0.3082  d0.loss_cls: 0.1572  d0.loss_bbox: 0.0398  d0.loss_iou: 0.3231  d1.loss_cls: 0.1290  d1.loss_bbox: 0.0405  d1.loss_iou: 0.3155  d2.loss_cls: 0.1170  d2.loss_bbox: 0.0392  d2.loss_iou: 0.3134  d3.loss_cls: 0.1117  d3.loss_bbox: 0.0388  d3.loss_iou: 0.3082  d4.loss_cls: 0.1072  d4.loss_bbox: 0.0388  d4.loss_iou: 0.3076  enc_loss_cls: 0.1787  enc_loss_bbox: 0.0451  enc_loss_iou: 0.3496  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0263  dn_loss_iou: 0.2372  d0.dn_loss_cls: 0.0273  d0.dn_loss_bbox: 0.0362  d0.dn_loss_iou: 0.3134  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2500  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0267  d2.dn_loss_iou: 0.2406  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0264  d3.dn_loss_iou: 0.2379  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0263  d4.dn_loss_iou: 0.2370  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 11:39:08 - mmengine - INFO - Epoch(train) [4][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:54:52  time: 1.8080  data_time: 0.0167  memory: 16264  grad_norm: 39.2993  loss: 5.0756  loss_cls: 0.1032  loss_bbox: 0.0356  loss_iou: 0.3055  d0.loss_cls: 0.1486  d0.loss_bbox: 0.0376  d0.loss_iou: 0.3203  d1.loss_cls: 0.1288  d1.loss_bbox: 0.0361  d1.loss_iou: 0.3094  d2.loss_cls: 0.1147  d2.loss_bbox: 0.0360  d2.loss_iou: 0.3100  d3.loss_cls: 0.1085  d3.loss_bbox: 0.0357  d3.loss_iou: 0.3073  d4.loss_cls: 0.1025  d4.loss_bbox: 0.0356  d4.loss_iou: 0.3051  enc_loss_cls: 0.1673  enc_loss_bbox: 0.0416  enc_loss_iou: 0.3413  dn_loss_cls: 0.0057  dn_loss_bbox: 0.0278  dn_loss_iou: 0.2366  d0.dn_loss_cls: 0.0259  d0.dn_loss_bbox: 0.0375  d0.dn_loss_iou: 0.3063  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0294  d1.dn_loss_iou: 0.2491  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2403  d3.dn_loss_cls: 0.0059  d3.dn_loss_bbox: 0.0279  d3.dn_loss_iou: 0.2372  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0277  d4.dn_loss_iou: 0.2364  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:40:37 - mmengine - INFO - Epoch(train) [4][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:53:22  time: 1.7954  data_time: 0.0168  memory: 16269  grad_norm: 35.7469  loss: 5.4785  loss_cls: 0.1129  loss_bbox: 0.0383  loss_iou: 0.3444  d0.loss_cls: 0.1735  d0.loss_bbox: 0.0405  d0.loss_iou: 0.3559  d1.loss_cls: 0.1345  d1.loss_bbox: 0.0392  d1.loss_iou: 0.3484  d2.loss_cls: 0.1191  d2.loss_bbox: 0.0388  d2.loss_iou: 0.3477  d3.loss_cls: 0.1115  d3.loss_bbox: 0.0390  d3.loss_iou: 0.3472  d4.loss_cls: 0.1144  d4.loss_bbox: 0.0383  d4.loss_iou: 0.3447  enc_loss_cls: 0.1753  enc_loss_bbox: 0.0435  enc_loss_iou: 0.3723  dn_loss_cls: 0.0067  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2431  d0.dn_loss_cls: 0.0306  d0.dn_loss_bbox: 0.0383  d0.dn_loss_iou: 0.3182  d1.dn_loss_cls: 0.0115  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2554  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0279  d2.dn_loss_iou: 0.2455  d3.dn_loss_cls: 0.0071  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2429  d4.dn_loss_cls: 0.0067  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2429  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0004
2025/10/29 11:42:08 - mmengine - INFO - Epoch(train) [4][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:51:51  time: 1.8048  data_time: 0.0177  memory: 16256  grad_norm: 41.5669  loss: 4.8185  loss_cls: 0.0962  loss_bbox: 0.0354  loss_iou: 0.2912  d0.loss_cls: 0.1423  d0.loss_bbox: 0.0370  d0.loss_iou: 0.3054  d1.loss_cls: 0.1090  d1.loss_bbox: 0.0359  d1.loss_iou: 0.2977  d2.loss_cls: 0.1034  d2.loss_bbox: 0.0350  d2.loss_iou: 0.2924  d3.loss_cls: 0.0938  d3.loss_bbox: 0.0360  d3.loss_iou: 0.2928  d4.loss_cls: 0.0948  d4.loss_bbox: 0.0359  d4.loss_iou: 0.2925  enc_loss_cls: 0.1609  enc_loss_bbox: 0.0415  enc_loss_iou: 0.3327  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0263  dn_loss_iou: 0.2229  d0.dn_loss_cls: 0.0263  d0.dn_loss_bbox: 0.0368  d0.dn_loss_iou: 0.2932  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0280  d1.dn_loss_iou: 0.2351  d2.dn_loss_cls: 0.0071  d2.dn_loss_bbox: 0.0267  d2.dn_loss_iou: 0.2256  d3.dn_loss_cls: 0.0066  d3.dn_loss_bbox: 0.0264  d3.dn_loss_iou: 0.2229  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0263  d4.dn_loss_iou: 0.2226  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:43:38 - mmengine - INFO - Epoch(train) [4][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:50:21  time: 1.8064  data_time: 0.0172  memory: 16269  grad_norm: 38.1626  loss: 4.6750  loss_cls: 0.0855  loss_bbox: 0.0315  loss_iou: 0.2671  d0.loss_cls: 0.1238  d0.loss_bbox: 0.0329  d0.loss_iou: 0.2752  d1.loss_cls: 0.1038  d1.loss_bbox: 0.0317  d1.loss_iou: 0.2692  d2.loss_cls: 0.0947  d2.loss_bbox: 0.0314  d2.loss_iou: 0.2664  d3.loss_cls: 0.0883  d3.loss_bbox: 0.0314  d3.loss_iou: 0.2664  d4.loss_cls: 0.0856  d4.loss_bbox: 0.0315  d4.loss_iou: 0.2667  enc_loss_cls: 0.1518  enc_loss_bbox: 0.0354  enc_loss_iou: 0.2914  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0291  dn_loss_iou: 0.2447  d0.dn_loss_cls: 0.0301  d0.dn_loss_bbox: 0.0399  d0.dn_loss_iou: 0.3227  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0309  d1.dn_loss_iou: 0.2584  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0295  d2.dn_loss_iou: 0.2476  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2448  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0291  d4.dn_loss_iou: 0.2443  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:45:08 - mmengine - INFO - Epoch(train) [4][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:48:51  time: 1.7999  data_time: 0.0183  memory: 16269  grad_norm: 37.0336  loss: 5.1535  loss_cls: 0.1142  loss_bbox: 0.0354  loss_iou: 0.3097  d0.loss_cls: 0.1446  d0.loss_bbox: 0.0363  d0.loss_iou: 0.3223  d1.loss_cls: 0.1295  d1.loss_bbox: 0.0349  d1.loss_iou: 0.3130  d2.loss_cls: 0.1192  d2.loss_bbox: 0.0349  d2.loss_iou: 0.3091  d3.loss_cls: 0.1213  d3.loss_bbox: 0.0345  d3.loss_iou: 0.3081  d4.loss_cls: 0.1164  d4.loss_bbox: 0.0344  d4.loss_iou: 0.3068  enc_loss_cls: 0.1725  enc_loss_bbox: 0.0391  enc_loss_iou: 0.3404  dn_loss_cls: 0.0049  dn_loss_bbox: 0.0272  dn_loss_iou: 0.2405  d0.dn_loss_cls: 0.0286  d0.dn_loss_bbox: 0.0377  d0.dn_loss_iou: 0.3205  d1.dn_loss_cls: 0.0090  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2548  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0276  d2.dn_loss_iou: 0.2433  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0272  d3.dn_loss_iou: 0.2403  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0272  d4.dn_loss_iou: 0.2402  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:46:38 - mmengine - INFO - Epoch(train) [4][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:47:21  time: 1.8026  data_time: 0.0190  memory: 16265  grad_norm: 44.7587  loss: 5.1453  loss_cls: 0.1041  loss_bbox: 0.0329  loss_iou: 0.3188  d0.loss_cls: 0.1415  d0.loss_bbox: 0.0347  d0.loss_iou: 0.3331  d1.loss_cls: 0.1179  d1.loss_bbox: 0.0333  d1.loss_iou: 0.3242  d2.loss_cls: 0.1132  d2.loss_bbox: 0.0331  d2.loss_iou: 0.3204  d3.loss_cls: 0.1076  d3.loss_bbox: 0.0331  d3.loss_iou: 0.3191  d4.loss_cls: 0.1036  d4.loss_bbox: 0.0328  d4.loss_iou: 0.3180  enc_loss_cls: 0.1569  enc_loss_bbox: 0.0385  enc_loss_iou: 0.3578  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0263  dn_loss_iou: 0.2428  d0.dn_loss_cls: 0.0238  d0.dn_loss_bbox: 0.0366  d0.dn_loss_iou: 0.3185  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2552  d2.dn_loss_cls: 0.0058  d2.dn_loss_bbox: 0.0265  d2.dn_loss_iou: 0.2446  d3.dn_loss_cls: 0.0051  d3.dn_loss_bbox: 0.0263  d3.dn_loss_iou: 0.2429  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0263  d4.dn_loss_iou: 0.2424  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:48:09 - mmengine - INFO - Epoch(train) [4][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:45:51  time: 1.8165  data_time: 0.0178  memory: 16275  grad_norm: 47.0010  loss: 4.8874  loss_cls: 0.0978  loss_bbox: 0.0314  loss_iou: 0.2926  d0.loss_cls: 0.1447  d0.loss_bbox: 0.0333  d0.loss_iou: 0.3019  d1.loss_cls: 0.1145  d1.loss_bbox: 0.0318  d1.loss_iou: 0.2945  d2.loss_cls: 0.1045  d2.loss_bbox: 0.0317  d2.loss_iou: 0.2934  d3.loss_cls: 0.1017  d3.loss_bbox: 0.0313  d3.loss_iou: 0.2918  d4.loss_cls: 0.0985  d4.loss_bbox: 0.0314  d4.loss_iou: 0.2916  enc_loss_cls: 0.1590  enc_loss_bbox: 0.0373  enc_loss_iou: 0.3293  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0266  dn_loss_iou: 0.2385  d0.dn_loss_cls: 0.0268  d0.dn_loss_bbox: 0.0363  d0.dn_loss_iou: 0.3119  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2507  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0269  d2.dn_loss_iou: 0.2405  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0266  d3.dn_loss_iou: 0.2384  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.0266  d4.dn_loss_iou: 0.2384  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:49:39 - mmengine - INFO - Epoch(train) [4][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:44:21  time: 1.8004  data_time: 0.0180  memory: 16255  grad_norm: 37.9803  loss: 4.9501  loss_cls: 0.0851  loss_bbox: 0.0339  loss_iou: 0.3003  d0.loss_cls: 0.1288  d0.loss_bbox: 0.0358  d0.loss_iou: 0.3168  d1.loss_cls: 0.1029  d1.loss_bbox: 0.0345  d1.loss_iou: 0.3082  d2.loss_cls: 0.0948  d2.loss_bbox: 0.0339  d2.loss_iou: 0.3039  d3.loss_cls: 0.0906  d3.loss_bbox: 0.0337  d3.loss_iou: 0.3008  d4.loss_cls: 0.0865  d4.loss_bbox: 0.0339  d4.loss_iou: 0.3003  enc_loss_cls: 0.1521  enc_loss_bbox: 0.0392  enc_loss_iou: 0.3405  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0281  dn_loss_iou: 0.2414  d0.dn_loss_cls: 0.0280  d0.dn_loss_bbox: 0.0397  d0.dn_loss_iou: 0.3256  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0298  d1.dn_loss_iou: 0.2560  d2.dn_loss_cls: 0.0058  d2.dn_loss_bbox: 0.0285  d2.dn_loss_iou: 0.2452  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0281  d3.dn_loss_iou: 0.2415  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0281  d4.dn_loss_iou: 0.2411  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:51:09 - mmengine - INFO - Epoch(train) [4][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:42:50  time: 1.8058  data_time: 0.0172  memory: 16257  grad_norm: 35.2715  loss: 4.5248  loss_cls: 0.0840  loss_bbox: 0.0284  loss_iou: 0.2644  d0.loss_cls: 0.1197  d0.loss_bbox: 0.0306  d0.loss_iou: 0.2802  d1.loss_cls: 0.0975  d1.loss_bbox: 0.0286  d1.loss_iou: 0.2678  d2.loss_cls: 0.0898  d2.loss_bbox: 0.0285  d2.loss_iou: 0.2649  d3.loss_cls: 0.0878  d3.loss_bbox: 0.0284  d3.loss_iou: 0.2650  d4.loss_cls: 0.0845  d4.loss_bbox: 0.0284  d4.loss_iou: 0.2642  enc_loss_cls: 0.1538  enc_loss_bbox: 0.0327  enc_loss_iou: 0.2955  dn_loss_cls: 0.0034  dn_loss_bbox: 0.0267  dn_loss_iou: 0.2323  d0.dn_loss_cls: 0.0255  d0.dn_loss_bbox: 0.0365  d0.dn_loss_iou: 0.3035  d1.dn_loss_cls: 0.0068  d1.dn_loss_bbox: 0.0281  d1.dn_loss_iou: 0.2447  d2.dn_loss_cls: 0.0041  d2.dn_loss_bbox: 0.0270  d2.dn_loss_iou: 0.2350  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0267  d3.dn_loss_iou: 0.2321  d4.dn_loss_cls: 0.0033  d4.dn_loss_bbox: 0.0267  d4.dn_loss_iou: 0.2321  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:52:40 - mmengine - INFO - Epoch(train) [4][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:41:20  time: 1.8075  data_time: 0.0180  memory: 16256  grad_norm: 34.0884  loss: 4.8092  loss_cls: 0.0926  loss_bbox: 0.0331  loss_iou: 0.2908  d0.loss_cls: 0.1276  d0.loss_bbox: 0.0368  d0.loss_iou: 0.3068  d1.loss_cls: 0.1093  d1.loss_bbox: 0.0339  d1.loss_iou: 0.2984  d2.loss_cls: 0.0965  d2.loss_bbox: 0.0337  d2.loss_iou: 0.2968  d3.loss_cls: 0.0927  d3.loss_bbox: 0.0335  d3.loss_iou: 0.2954  d4.loss_cls: 0.0921  d4.loss_bbox: 0.0332  d4.loss_iou: 0.2914  enc_loss_cls: 0.1596  enc_loss_bbox: 0.0387  enc_loss_iou: 0.3261  dn_loss_cls: 0.0032  dn_loss_bbox: 0.0271  dn_loss_iou: 0.2317  d0.dn_loss_cls: 0.0229  d0.dn_loss_bbox: 0.0372  d0.dn_loss_iou: 0.3005  d1.dn_loss_cls: 0.0059  d1.dn_loss_bbox: 0.0286  d1.dn_loss_iou: 0.2423  d2.dn_loss_cls: 0.0037  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2340  d3.dn_loss_cls: 0.0034  d3.dn_loss_bbox: 0.0271  d3.dn_loss_iou: 0.2316  d4.dn_loss_cls: 0.0032  d4.dn_loss_bbox: 0.0271  d4.dn_loss_iou: 0.2317  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:54:10 - mmengine - INFO - Epoch(train) [4][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:39:50  time: 1.8038  data_time: 0.0174  memory: 16257  grad_norm: 36.4813  loss: 4.9415  loss_cls: 0.0887  loss_bbox: 0.0339  loss_iou: 0.2994  d0.loss_cls: 0.1426  d0.loss_bbox: 0.0340  d0.loss_iou: 0.3056  d1.loss_cls: 0.1083  d1.loss_bbox: 0.0342  d1.loss_iou: 0.3031  d2.loss_cls: 0.1008  d2.loss_bbox: 0.0342  d2.loss_iou: 0.2989  d3.loss_cls: 0.0920  d3.loss_bbox: 0.0339  d3.loss_iou: 0.2990  d4.loss_cls: 0.0907  d4.loss_bbox: 0.0327  d4.loss_iou: 0.2979  enc_loss_cls: 0.1595  enc_loss_bbox: 0.0386  enc_loss_iou: 0.3288  dn_loss_cls: 0.0049  dn_loss_bbox: 0.0288  dn_loss_iou: 0.2421  d0.dn_loss_cls: 0.0240  d0.dn_loss_bbox: 0.0402  d0.dn_loss_iou: 0.3179  d1.dn_loss_cls: 0.0073  d1.dn_loss_bbox: 0.0305  d1.dn_loss_iou: 0.2554  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0291  d2.dn_loss_iou: 0.2451  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.0288  d3.dn_loss_iou: 0.2423  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0288  d4.dn_loss_iou: 0.2419  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:55:40 - mmengine - INFO - Epoch(train) [4][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:38:20  time: 1.7993  data_time: 0.0184  memory: 16269  grad_norm: 40.9573  loss: 4.5776  loss_cls: 0.0744  loss_bbox: 0.0311  loss_iou: 0.2603  d0.loss_cls: 0.1215  d0.loss_bbox: 0.0332  d0.loss_iou: 0.2693  d1.loss_cls: 0.0923  d1.loss_bbox: 0.0318  d1.loss_iou: 0.2616  d2.loss_cls: 0.0847  d2.loss_bbox: 0.0316  d2.loss_iou: 0.2632  d3.loss_cls: 0.0776  d3.loss_bbox: 0.0312  d3.loss_iou: 0.2609  d4.loss_cls: 0.0745  d4.loss_bbox: 0.0311  d4.loss_iou: 0.2598  enc_loss_cls: 0.1425  enc_loss_bbox: 0.0369  enc_loss_iou: 0.2920  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2442  d0.dn_loss_cls: 0.0272  d0.dn_loss_bbox: 0.0427  d0.dn_loss_iou: 0.3262  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0324  d1.dn_loss_iou: 0.2589  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0308  d2.dn_loss_iou: 0.2474  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2444  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2438  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:57:10 - mmengine - INFO - Epoch(train) [4][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:36:50  time: 1.8115  data_time: 0.0176  memory: 16256  grad_norm: 37.9955  loss: 5.2832  loss_cls: 0.1202  loss_bbox: 0.0375  loss_iou: 0.3316  d0.loss_cls: 0.1641  d0.loss_bbox: 0.0400  d0.loss_iou: 0.3492  d1.loss_cls: 0.1384  d1.loss_bbox: 0.0384  d1.loss_iou: 0.3375  d2.loss_cls: 0.1314  d2.loss_bbox: 0.0378  d2.loss_iou: 0.3332  d3.loss_cls: 0.1244  d3.loss_bbox: 0.0368  d3.loss_iou: 0.3286  d4.loss_cls: 0.1237  d4.loss_bbox: 0.0367  d4.loss_iou: 0.3287  enc_loss_cls: 0.1800  enc_loss_bbox: 0.0441  enc_loss_iou: 0.3782  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0251  dn_loss_iou: 0.2218  d0.dn_loss_cls: 0.0280  d0.dn_loss_bbox: 0.0355  d0.dn_loss_iou: 0.2968  d1.dn_loss_cls: 0.0092  d1.dn_loss_bbox: 0.0266  d1.dn_loss_iou: 0.2335  d2.dn_loss_cls: 0.0060  d2.dn_loss_bbox: 0.0254  d2.dn_loss_iou: 0.2241  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0252  d3.dn_loss_iou: 0.2218  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0251  d4.dn_loss_iou: 0.2216  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 11:58:31 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 11:58:40 - mmengine - INFO - Epoch(train) [4][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:35:19  time: 1.8000  data_time: 0.0192  memory: 16256  grad_norm: 39.2933  loss: 4.7418  loss_cls: 0.0769  loss_bbox: 0.0358  loss_iou: 0.2906  d0.loss_cls: 0.1291  d0.loss_bbox: 0.0388  d0.loss_iou: 0.3105  d1.loss_cls: 0.0988  d1.loss_bbox: 0.0371  d1.loss_iou: 0.2965  d2.loss_cls: 0.0824  d2.loss_bbox: 0.0360  d2.loss_iou: 0.2933  d3.loss_cls: 0.0799  d3.loss_bbox: 0.0358  d3.loss_iou: 0.2906  d4.loss_cls: 0.0788  d4.loss_bbox: 0.0356  d4.loss_iou: 0.2891  enc_loss_cls: 0.1474  enc_loss_bbox: 0.0438  enc_loss_iou: 0.3377  dn_loss_cls: 0.0077  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2206  d0.dn_loss_cls: 0.0319  d0.dn_loss_bbox: 0.0387  d0.dn_loss_iou: 0.2975  d1.dn_loss_cls: 0.0131  d1.dn_loss_bbox: 0.0295  d1.dn_loss_iou: 0.2347  d2.dn_loss_cls: 0.0096  d2.dn_loss_bbox: 0.0280  d2.dn_loss_iou: 0.2237  d3.dn_loss_cls: 0.0080  d3.dn_loss_bbox: 0.0277  d3.dn_loss_iou: 0.2210  d4.dn_loss_cls: 0.0080  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2204  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0002
2025/10/29 12:00:11 - mmengine - INFO - Epoch(train) [4][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:33:49  time: 1.8049  data_time: 0.0164  memory: 16256  grad_norm: 38.9484  loss: 4.3516  loss_cls: 0.0690  loss_bbox: 0.0305  loss_iou: 0.2544  d0.loss_cls: 0.1042  d0.loss_bbox: 0.0310  d0.loss_iou: 0.2638  d1.loss_cls: 0.0869  d1.loss_bbox: 0.0308  d1.loss_iou: 0.2553  d2.loss_cls: 0.0739  d2.loss_bbox: 0.0307  d2.loss_iou: 0.2544  d3.loss_cls: 0.0739  d3.loss_bbox: 0.0308  d3.loss_iou: 0.2546  d4.loss_cls: 0.0723  d4.loss_bbox: 0.0305  d4.loss_iou: 0.2537  enc_loss_cls: 0.1274  enc_loss_bbox: 0.0339  enc_loss_iou: 0.2797  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0277  dn_loss_iou: 0.2318  d0.dn_loss_cls: 0.0277  d0.dn_loss_bbox: 0.0380  d0.dn_loss_iou: 0.3042  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2431  d2.dn_loss_cls: 0.0050  d2.dn_loss_bbox: 0.0281  d2.dn_loss_iou: 0.2341  d3.dn_loss_cls: 0.0043  d3.dn_loss_bbox: 0.0278  d3.dn_loss_iou: 0.2319  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0277  d4.dn_loss_iou: 0.2315  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:01:41 - mmengine - INFO - Epoch(train) [4][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:32:19  time: 1.8154  data_time: 0.0173  memory: 16269  grad_norm: 39.0088  loss: 4.8927  loss_cls: 0.0800  loss_bbox: 0.0336  loss_iou: 0.3030  d0.loss_cls: 0.1223  d0.loss_bbox: 0.0354  d0.loss_iou: 0.3179  d1.loss_cls: 0.1019  d1.loss_bbox: 0.0341  d1.loss_iou: 0.3099  d2.loss_cls: 0.0886  d2.loss_bbox: 0.0335  d2.loss_iou: 0.3039  d3.loss_cls: 0.0830  d3.loss_bbox: 0.0338  d3.loss_iou: 0.3032  d4.loss_cls: 0.0800  d4.loss_bbox: 0.0338  d4.loss_iou: 0.3028  enc_loss_cls: 0.1468  enc_loss_bbox: 0.0393  enc_loss_iou: 0.3422  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0265  dn_loss_iou: 0.2399  d0.dn_loss_cls: 0.0280  d0.dn_loss_bbox: 0.0365  d0.dn_loss_iou: 0.3144  d1.dn_loss_cls: 0.0102  d1.dn_loss_bbox: 0.0280  d1.dn_loss_iou: 0.2527  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0267  d2.dn_loss_iou: 0.2427  d3.dn_loss_cls: 0.0060  d3.dn_loss_bbox: 0.0264  d3.dn_loss_iou: 0.2399  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0265  d4.dn_loss_iou: 0.2395  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:03:11 - mmengine - INFO - Epoch(train) [4][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:30:49  time: 1.7978  data_time: 0.0175  memory: 16257  grad_norm: 37.8625  loss: 4.8278  loss_cls: 0.0842  loss_bbox: 0.0329  loss_iou: 0.2845  d0.loss_cls: 0.1307  d0.loss_bbox: 0.0359  d0.loss_iou: 0.2989  d1.loss_cls: 0.1010  d1.loss_bbox: 0.0346  d1.loss_iou: 0.2915  d2.loss_cls: 0.0911  d2.loss_bbox: 0.0340  d2.loss_iou: 0.2872  d3.loss_cls: 0.0868  d3.loss_bbox: 0.0330  d3.loss_iou: 0.2852  d4.loss_cls: 0.0850  d4.loss_bbox: 0.0328  d4.loss_iou: 0.2839  enc_loss_cls: 0.1463  enc_loss_bbox: 0.0388  enc_loss_iou: 0.3201  dn_loss_cls: 0.0084  dn_loss_bbox: 0.0289  dn_loss_iou: 0.2401  d0.dn_loss_cls: 0.0309  d0.dn_loss_bbox: 0.0410  d0.dn_loss_iou: 0.3220  d1.dn_loss_cls: 0.0115  d1.dn_loss_bbox: 0.0308  d1.dn_loss_iou: 0.2544  d2.dn_loss_cls: 0.0105  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2436  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0289  d3.dn_loss_iou: 0.2405  d4.dn_loss_cls: 0.0086  d4.dn_loss_bbox: 0.0289  d4.dn_loss_iou: 0.2399  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:04:41 - mmengine - INFO - Epoch(train) [4][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:29:18  time: 1.7955  data_time: 0.0171  memory: 16259  grad_norm: 44.1664  loss: 4.8245  loss_cls: 0.0773  loss_bbox: 0.0366  loss_iou: 0.3013  d0.loss_cls: 0.1377  d0.loss_bbox: 0.0375  d0.loss_iou: 0.3098  d1.loss_cls: 0.0971  d1.loss_bbox: 0.0371  d1.loss_iou: 0.3060  d2.loss_cls: 0.0889  d2.loss_bbox: 0.0367  d2.loss_iou: 0.3022  d3.loss_cls: 0.0832  d3.loss_bbox: 0.0366  d3.loss_iou: 0.2999  d4.loss_cls: 0.0778  d4.loss_bbox: 0.0368  d4.loss_iou: 0.3023  enc_loss_cls: 0.1431  enc_loss_bbox: 0.0417  enc_loss_iou: 0.3362  dn_loss_cls: 0.0057  dn_loss_bbox: 0.0268  dn_loss_iou: 0.2280  d0.dn_loss_cls: 0.0298  d0.dn_loss_bbox: 0.0375  d0.dn_loss_iou: 0.3047  d1.dn_loss_cls: 0.0104  d1.dn_loss_bbox: 0.0285  d1.dn_loss_iou: 0.2406  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0272  d2.dn_loss_iou: 0.2304  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0269  d3.dn_loss_iou: 0.2277  d4.dn_loss_cls: 0.0057  d4.dn_loss_bbox: 0.0268  d4.dn_loss_iou: 0.2276  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:06:12 - mmengine - INFO - Epoch(train) [4][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:27:48  time: 1.8128  data_time: 0.0178  memory: 16282  grad_norm: 38.9660  loss: 4.4508  loss_cls: 0.0830  loss_bbox: 0.0278  loss_iou: 0.2580  d0.loss_cls: 0.1252  d0.loss_bbox: 0.0302  d0.loss_iou: 0.2735  d1.loss_cls: 0.1039  d1.loss_bbox: 0.0284  d1.loss_iou: 0.2624  d2.loss_cls: 0.0874  d2.loss_bbox: 0.0278  d2.loss_iou: 0.2590  d3.loss_cls: 0.0864  d3.loss_bbox: 0.0278  d3.loss_iou: 0.2580  d4.loss_cls: 0.0832  d4.loss_bbox: 0.0278  d4.loss_iou: 0.2577  enc_loss_cls: 0.1493  enc_loss_bbox: 0.0332  enc_loss_iou: 0.2965  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0255  dn_loss_iou: 0.2260  d0.dn_loss_cls: 0.0266  d0.dn_loss_bbox: 0.0358  d0.dn_loss_iou: 0.3004  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.0270  d1.dn_loss_iou: 0.2380  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0259  d2.dn_loss_iou: 0.2289  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0256  d3.dn_loss_iou: 0.2259  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0255  d4.dn_loss_iou: 0.2257  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:07:42 - mmengine - INFO - Epoch(train) [4][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:26:18  time: 1.7982  data_time: 0.0176  memory: 16285  grad_norm: 36.2708  loss: 4.3367  loss_cls: 0.0757  loss_bbox: 0.0307  loss_iou: 0.2669  d0.loss_cls: 0.1146  d0.loss_bbox: 0.0325  d0.loss_iou: 0.2802  d1.loss_cls: 0.0915  d1.loss_bbox: 0.0311  d1.loss_iou: 0.2690  d2.loss_cls: 0.0839  d2.loss_bbox: 0.0302  d2.loss_iou: 0.2636  d3.loss_cls: 0.0815  d3.loss_bbox: 0.0300  d3.loss_iou: 0.2619  d4.loss_cls: 0.0760  d4.loss_bbox: 0.0305  d4.loss_iou: 0.2652  enc_loss_cls: 0.1366  enc_loss_bbox: 0.0353  enc_loss_iou: 0.2968  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0244  dn_loss_iou: 0.2088  d0.dn_loss_cls: 0.0275  d0.dn_loss_bbox: 0.0335  d0.dn_loss_iou: 0.2759  d1.dn_loss_cls: 0.0096  d1.dn_loss_bbox: 0.0258  d1.dn_loss_iou: 0.2212  d2.dn_loss_cls: 0.0064  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2110  d3.dn_loss_cls: 0.0055  d3.dn_loss_bbox: 0.0244  d3.dn_loss_iou: 0.2092  d4.dn_loss_cls: 0.0052  d4.dn_loss_bbox: 0.0243  d4.dn_loss_iou: 0.2087  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:09:12 - mmengine - INFO - Epoch(train) [4][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:24:48  time: 1.7995  data_time: 0.0175  memory: 16256  grad_norm: 39.4313  loss: 4.3531  loss_cls: 0.0784  loss_bbox: 0.0284  loss_iou: 0.2415  d0.loss_cls: 0.1076  d0.loss_bbox: 0.0296  d0.loss_iou: 0.2500  d1.loss_cls: 0.0855  d1.loss_bbox: 0.0290  d1.loss_iou: 0.2459  d2.loss_cls: 0.0792  d2.loss_bbox: 0.0287  d2.loss_iou: 0.2435  d3.loss_cls: 0.0808  d3.loss_bbox: 0.0287  d3.loss_iou: 0.2426  d4.loss_cls: 0.0787  d4.loss_bbox: 0.0284  d4.loss_iou: 0.2413  enc_loss_cls: 0.1347  enc_loss_bbox: 0.0329  enc_loss_iou: 0.2735  dn_loss_cls: 0.0043  dn_loss_bbox: 0.0279  dn_loss_iou: 0.2402  d0.dn_loss_cls: 0.0264  d0.dn_loss_bbox: 0.0382  d0.dn_loss_iou: 0.3149  d1.dn_loss_cls: 0.0082  d1.dn_loss_bbox: 0.0294  d1.dn_loss_iou: 0.2514  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2424  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0280  d3.dn_loss_iou: 0.2402  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0279  d4.dn_loss_iou: 0.2400  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0005  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:10:42 - mmengine - INFO - Epoch(train) [4][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:23:17  time: 1.8041  data_time: 0.0179  memory: 16275  grad_norm: 51.1599  loss: 4.8770  loss_cls: 0.0902  loss_bbox: 0.0343  loss_iou: 0.2947  d0.loss_cls: 0.1361  d0.loss_bbox: 0.0366  d0.loss_iou: 0.3094  d1.loss_cls: 0.1114  d1.loss_bbox: 0.0353  d1.loss_iou: 0.3013  d2.loss_cls: 0.1014  d2.loss_bbox: 0.0350  d2.loss_iou: 0.2961  d3.loss_cls: 0.0950  d3.loss_bbox: 0.0343  d3.loss_iou: 0.2947  d4.loss_cls: 0.0924  d4.loss_bbox: 0.0343  d4.loss_iou: 0.2944  enc_loss_cls: 0.1542  enc_loss_bbox: 0.0410  enc_loss_iou: 0.3329  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2334  d0.dn_loss_cls: 0.0271  d0.dn_loss_bbox: 0.0373  d0.dn_loss_iou: 0.3058  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0290  d1.dn_loss_iou: 0.2450  d2.dn_loss_cls: 0.0063  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2352  d3.dn_loss_cls: 0.0056  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2328  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0275  d4.dn_loss_iou: 0.2330  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0002
2025/10/29 12:12:11 - mmengine - INFO - Epoch(train) [4][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:21:47  time: 1.7940  data_time: 0.0177  memory: 16269  grad_norm: 41.4424  loss: 4.9555  loss_cls: 0.0984  loss_bbox: 0.0329  loss_iou: 0.2932  d0.loss_cls: 0.1324  d0.loss_bbox: 0.0351  d0.loss_iou: 0.3076  d1.loss_cls: 0.1212  d1.loss_bbox: 0.0332  d1.loss_iou: 0.2948  d2.loss_cls: 0.1119  d2.loss_bbox: 0.0329  d2.loss_iou: 0.2943  d3.loss_cls: 0.1061  d3.loss_bbox: 0.0330  d3.loss_iou: 0.2949  d4.loss_cls: 0.1003  d4.loss_bbox: 0.0329  d4.loss_iou: 0.2927  enc_loss_cls: 0.1644  enc_loss_bbox: 0.0379  enc_loss_iou: 0.3286  dn_loss_cls: 0.0057  dn_loss_bbox: 0.0277  dn_loss_iou: 0.2400  d0.dn_loss_cls: 0.0296  d0.dn_loss_bbox: 0.0380  d0.dn_loss_iou: 0.3159  d1.dn_loss_cls: 0.0100  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2533  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0281  d2.dn_loss_iou: 0.2435  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0277  d3.dn_loss_iou: 0.2406  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2399  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:13:42 - mmengine - INFO - Epoch(train) [4][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:20:17  time: 1.8070  data_time: 0.0177  memory: 16270  grad_norm: 38.6317  loss: 4.6501  loss_cls: 0.0869  loss_bbox: 0.0330  loss_iou: 0.2951  d0.loss_cls: 0.1344  d0.loss_bbox: 0.0352  d0.loss_iou: 0.3108  d1.loss_cls: 0.1064  d1.loss_bbox: 0.0333  d1.loss_iou: 0.2989  d2.loss_cls: 0.0984  d2.loss_bbox: 0.0333  d2.loss_iou: 0.2975  d3.loss_cls: 0.0916  d3.loss_bbox: 0.0330  d3.loss_iou: 0.2950  d4.loss_cls: 0.0871  d4.loss_bbox: 0.0330  d4.loss_iou: 0.2948  enc_loss_cls: 0.1518  enc_loss_bbox: 0.0394  enc_loss_iou: 0.3371  dn_loss_cls: 0.0069  dn_loss_bbox: 0.0243  dn_loss_iou: 0.2041  d0.dn_loss_cls: 0.0265  d0.dn_loss_bbox: 0.0341  d0.dn_loss_iou: 0.2709  d1.dn_loss_cls: 0.0087  d1.dn_loss_bbox: 0.0257  d1.dn_loss_iou: 0.2141  d2.dn_loss_cls: 0.0064  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2064  d3.dn_loss_cls: 0.0066  d3.dn_loss_bbox: 0.0243  d3.dn_loss_iou: 0.2041  d4.dn_loss_cls: 0.0062  d4.dn_loss_bbox: 0.0243  d4.dn_loss_iou: 0.2040  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:15:12 - mmengine - INFO - Epoch(train) [4][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:18:47  time: 1.8107  data_time: 0.0181  memory: 16281  grad_norm: 46.9679  loss: 4.0693  loss_cls: 0.0661  loss_bbox: 0.0256  loss_iou: 0.2384  d0.loss_cls: 0.0983  d0.loss_bbox: 0.0273  d0.loss_iou: 0.2513  d1.loss_cls: 0.0782  d1.loss_bbox: 0.0263  d1.loss_iou: 0.2441  d2.loss_cls: 0.0699  d2.loss_bbox: 0.0258  d2.loss_iou: 0.2406  d3.loss_cls: 0.0685  d3.loss_bbox: 0.0258  d3.loss_iou: 0.2390  d4.loss_cls: 0.0661  d4.loss_bbox: 0.0256  d4.loss_iou: 0.2384  enc_loss_cls: 0.1193  enc_loss_bbox: 0.0306  enc_loss_iou: 0.2707  dn_loss_cls: 0.0029  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2186  d0.dn_loss_cls: 0.0210  d0.dn_loss_bbox: 0.0339  d0.dn_loss_iou: 0.2886  d1.dn_loss_cls: 0.0060  d1.dn_loss_bbox: 0.0256  d1.dn_loss_iou: 0.2303  d2.dn_loss_cls: 0.0036  d2.dn_loss_bbox: 0.0244  d2.dn_loss_iou: 0.2211  d3.dn_loss_cls: 0.0031  d3.dn_loss_bbox: 0.0241  d3.dn_loss_iou: 0.2190  d4.dn_loss_cls: 0.0029  d4.dn_loss_bbox: 0.0241  d4.dn_loss_iou: 0.2184  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:16:43 - mmengine - INFO - Epoch(train) [4][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:17:17  time: 1.8070  data_time: 0.0192  memory: 16257  grad_norm: 40.8666  loss: 4.3686  loss_cls: 0.0694  loss_bbox: 0.0314  loss_iou: 0.2627  d0.loss_cls: 0.1106  d0.loss_bbox: 0.0328  d0.loss_iou: 0.2773  d1.loss_cls: 0.0883  d1.loss_bbox: 0.0319  d1.loss_iou: 0.2689  d2.loss_cls: 0.0752  d2.loss_bbox: 0.0317  d2.loss_iou: 0.2653  d3.loss_cls: 0.0729  d3.loss_bbox: 0.0315  d3.loss_iou: 0.2644  d4.loss_cls: 0.0696  d4.loss_bbox: 0.0314  d4.loss_iou: 0.2625  enc_loss_cls: 0.1266  enc_loss_bbox: 0.0351  enc_loss_iou: 0.2974  dn_loss_cls: 0.0032  dn_loss_bbox: 0.0262  dn_loss_iou: 0.2227  d0.dn_loss_cls: 0.0222  d0.dn_loss_bbox: 0.0355  d0.dn_loss_iou: 0.2914  d1.dn_loss_cls: 0.0061  d1.dn_loss_bbox: 0.0277  d1.dn_loss_iou: 0.2343  d2.dn_loss_cls: 0.0040  d2.dn_loss_bbox: 0.0266  d2.dn_loss_iou: 0.2255  d3.dn_loss_cls: 0.0034  d3.dn_loss_bbox: 0.0262  d3.dn_loss_iou: 0.2228  d4.dn_loss_cls: 0.0032  d4.dn_loss_bbox: 0.0262  d4.dn_loss_iou: 0.2226  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:18:14 - mmengine - INFO - Epoch(train) [4][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:15:47  time: 1.8166  data_time: 0.0175  memory: 16285  grad_norm: 45.3551  loss: 4.5097  loss_cls: 0.0694  loss_bbox: 0.0350  loss_iou: 0.2763  d0.loss_cls: 0.1180  d0.loss_bbox: 0.0369  d0.loss_iou: 0.2910  d1.loss_cls: 0.0834  d1.loss_bbox: 0.0359  d1.loss_iou: 0.2828  d2.loss_cls: 0.0777  d2.loss_bbox: 0.0354  d2.loss_iou: 0.2778  d3.loss_cls: 0.0721  d3.loss_bbox: 0.0350  d3.loss_iou: 0.2763  d4.loss_cls: 0.0688  d4.loss_bbox: 0.0350  d4.loss_iou: 0.2761  enc_loss_cls: 0.1334  enc_loss_bbox: 0.0402  enc_loss_iou: 0.3131  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0248  dn_loss_iou: 0.2234  d0.dn_loss_cls: 0.0258  d0.dn_loss_bbox: 0.0342  d0.dn_loss_iou: 0.2954  d1.dn_loss_cls: 0.0080  d1.dn_loss_bbox: 0.0261  d1.dn_loss_iou: 0.2347  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.0251  d2.dn_loss_iou: 0.2252  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0248  d3.dn_loss_iou: 0.2232  d4.dn_loss_cls: 0.0047  d4.dn_loss_bbox: 0.0248  d4.dn_loss_iou: 0.2231  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:19:44 - mmengine - INFO - Epoch(train) [4][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:14:16  time: 1.8035  data_time: 0.0177  memory: 16269  grad_norm: 39.9451  loss: 4.5643  loss_cls: 0.0729  loss_bbox: 0.0331  loss_iou: 0.2718  d0.loss_cls: 0.1186  d0.loss_bbox: 0.0352  d0.loss_iou: 0.2860  d1.loss_cls: 0.0946  d1.loss_bbox: 0.0335  d1.loss_iou: 0.2761  d2.loss_cls: 0.0813  d2.loss_bbox: 0.0334  d2.loss_iou: 0.2734  d3.loss_cls: 0.0766  d3.loss_bbox: 0.0332  d3.loss_iou: 0.2711  d4.loss_cls: 0.0726  d4.loss_bbox: 0.0333  d4.loss_iou: 0.2722  enc_loss_cls: 0.1319  enc_loss_bbox: 0.0377  enc_loss_iou: 0.3047  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2333  d0.dn_loss_cls: 0.0278  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.3073  d1.dn_loss_cls: 0.0085  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2444  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2359  d3.dn_loss_cls: 0.0050  d3.dn_loss_bbox: 0.0273  d3.dn_loss_iou: 0.2332  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0272  d4.dn_loss_iou: 0.2330  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:21:13 - mmengine - INFO - Epoch(train) [4][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:12:46  time: 1.7951  data_time: 0.0177  memory: 16269  grad_norm: 43.9074  loss: 4.3492  loss_cls: 0.0651  loss_bbox: 0.0320  loss_iou: 0.2686  d0.loss_cls: 0.1070  d0.loss_bbox: 0.0338  d0.loss_iou: 0.2821  d1.loss_cls: 0.0842  d1.loss_bbox: 0.0320  d1.loss_iou: 0.2719  d2.loss_cls: 0.0731  d2.loss_bbox: 0.0318  d2.loss_iou: 0.2693  d3.loss_cls: 0.0688  d3.loss_bbox: 0.0320  d3.loss_iou: 0.2686  d4.loss_cls: 0.0651  d4.loss_bbox: 0.0320  d4.loss_iou: 0.2684  enc_loss_cls: 0.1254  enc_loss_bbox: 0.0374  enc_loss_iou: 0.2979  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0259  dn_loss_iou: 0.2161  d0.dn_loss_cls: 0.0234  d0.dn_loss_bbox: 0.0362  d0.dn_loss_iou: 0.2865  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0277  d1.dn_loss_iou: 0.2290  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.0262  d2.dn_loss_iou: 0.2185  d3.dn_loss_cls: 0.0048  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2159  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0259  d4.dn_loss_iou: 0.2159  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:22:44 - mmengine - INFO - Epoch(train) [4][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:11:16  time: 1.8087  data_time: 0.0172  memory: 16264  grad_norm: 36.8679  loss: 5.3081  loss_cls: 0.0956  loss_bbox: 0.0377  loss_iou: 0.3365  d0.loss_cls: 0.1473  d0.loss_bbox: 0.0390  d0.loss_iou: 0.3488  d1.loss_cls: 0.1158  d1.loss_bbox: 0.0370  d1.loss_iou: 0.3391  d2.loss_cls: 0.1064  d2.loss_bbox: 0.0375  d2.loss_iou: 0.3364  d3.loss_cls: 0.0992  d3.loss_bbox: 0.0382  d3.loss_iou: 0.3376  d4.loss_cls: 0.0976  d4.loss_bbox: 0.0374  d4.loss_iou: 0.3352  enc_loss_cls: 0.1664  enc_loss_bbox: 0.0428  enc_loss_iou: 0.3722  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0266  dn_loss_iou: 0.2456  d0.dn_loss_cls: 0.0296  d0.dn_loss_bbox: 0.0371  d0.dn_loss_iou: 0.3270  d1.dn_loss_cls: 0.0094  d1.dn_loss_bbox: 0.0280  d1.dn_loss_iou: 0.2584  d2.dn_loss_cls: 0.0063  d2.dn_loss_bbox: 0.0269  d2.dn_loss_iou: 0.2476  d3.dn_loss_cls: 0.0055  d3.dn_loss_bbox: 0.0265  d3.dn_loss_iou: 0.2453  d4.dn_loss_cls: 0.0052  d4.dn_loss_bbox: 0.0265  d4.dn_loss_iou: 0.2454  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:24:14 - mmengine - INFO - Epoch(train) [4][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:09:46  time: 1.8038  data_time: 0.0167  memory: 16256  grad_norm: 40.2670  loss: 4.6012  loss_cls: 0.0764  loss_bbox: 0.0306  loss_iou: 0.2693  d0.loss_cls: 0.1313  d0.loss_bbox: 0.0323  d0.loss_iou: 0.2833  d1.loss_cls: 0.0968  d1.loss_bbox: 0.0315  d1.loss_iou: 0.2764  d2.loss_cls: 0.0853  d2.loss_bbox: 0.0310  d2.loss_iou: 0.2722  d3.loss_cls: 0.0806  d3.loss_bbox: 0.0307  d3.loss_iou: 0.2702  d4.loss_cls: 0.0768  d4.loss_bbox: 0.0306  d4.loss_iou: 0.2691  enc_loss_cls: 0.1472  enc_loss_bbox: 0.0354  enc_loss_iou: 0.3031  dn_loss_cls: 0.0078  dn_loss_bbox: 0.0257  dn_loss_iou: 0.2339  d0.dn_loss_cls: 0.0288  d0.dn_loss_bbox: 0.0367  d0.dn_loss_iou: 0.3132  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0274  d1.dn_loss_iou: 0.2474  d2.dn_loss_cls: 0.0081  d2.dn_loss_bbox: 0.0261  d2.dn_loss_iou: 0.2375  d3.dn_loss_cls: 0.0080  d3.dn_loss_bbox: 0.0257  d3.dn_loss_iou: 0.2342  d4.dn_loss_cls: 0.0079  d4.dn_loss_bbox: 0.0256  d4.dn_loss_iou: 0.2337  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:25:44 - mmengine - INFO - Epoch(train) [4][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:08:15  time: 1.8066  data_time: 0.0176  memory: 16269  grad_norm: 35.5889  loss: 4.7807  loss_cls: 0.0846  loss_bbox: 0.0301  loss_iou: 0.2867  d0.loss_cls: 0.1245  d0.loss_bbox: 0.0323  d0.loss_iou: 0.3015  d1.loss_cls: 0.0963  d1.loss_bbox: 0.0310  d1.loss_iou: 0.2946  d2.loss_cls: 0.0922  d2.loss_bbox: 0.0302  d2.loss_iou: 0.2881  d3.loss_cls: 0.0877  d3.loss_bbox: 0.0300  d3.loss_iou: 0.2862  d4.loss_cls: 0.0861  d4.loss_bbox: 0.0301  d4.loss_iou: 0.2866  enc_loss_cls: 0.1558  enc_loss_bbox: 0.0343  enc_loss_iou: 0.3232  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0258  dn_loss_iou: 0.2393  d0.dn_loss_cls: 0.0276  d0.dn_loss_bbox: 0.0367  d0.dn_loss_iou: 0.3223  d1.dn_loss_cls: 0.0098  d1.dn_loss_bbox: 0.0277  d1.dn_loss_iou: 0.2546  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0262  d2.dn_loss_iou: 0.2421  d3.dn_loss_cls: 0.0061  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2396  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0257  d4.dn_loss_iou: 0.2391  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:27:14 - mmengine - INFO - Epoch(train) [4][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:06:45  time: 1.7970  data_time: 0.0180  memory: 16265  grad_norm: 41.4287  loss: 5.5427  loss_cls: 0.1063  loss_bbox: 0.0368  loss_iou: 0.3687  d0.loss_cls: 0.1626  d0.loss_bbox: 0.0398  d0.loss_iou: 0.3899  d1.loss_cls: 0.1285  d1.loss_bbox: 0.0371  d1.loss_iou: 0.3762  d2.loss_cls: 0.1183  d2.loss_bbox: 0.0361  d2.loss_iou: 0.3656  d3.loss_cls: 0.1123  d3.loss_bbox: 0.0373  d3.loss_iou: 0.3686  d4.loss_cls: 0.1060  d4.loss_bbox: 0.0366  d4.loss_iou: 0.3682  enc_loss_cls: 0.1851  enc_loss_bbox: 0.0429  enc_loss_iou: 0.4206  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0246  dn_loss_iou: 0.2317  d0.dn_loss_cls: 0.0282  d0.dn_loss_bbox: 0.0339  d0.dn_loss_iou: 0.3073  d1.dn_loss_cls: 0.0093  d1.dn_loss_bbox: 0.0257  d1.dn_loss_iou: 0.2431  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0248  d2.dn_loss_iou: 0.2339  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0245  d3.dn_loss_iou: 0.2316  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0246  d4.dn_loss_iou: 0.2315  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 12:28:35 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 12:28:45 - mmengine - INFO - Epoch(train) [4][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:05:15  time: 1.8061  data_time: 0.0175  memory: 16268  grad_norm: 36.9711  loss: 4.7950  loss_cls: 0.0861  loss_bbox: 0.0345  loss_iou: 0.2948  d0.loss_cls: 0.1335  d0.loss_bbox: 0.0373  d0.loss_iou: 0.3116  d1.loss_cls: 0.1080  d1.loss_bbox: 0.0357  d1.loss_iou: 0.3017  d2.loss_cls: 0.0965  d2.loss_bbox: 0.0349  d2.loss_iou: 0.2978  d3.loss_cls: 0.0951  d3.loss_bbox: 0.0342  d3.loss_iou: 0.2927  d4.loss_cls: 0.0858  d4.loss_bbox: 0.0345  d4.loss_iou: 0.2956  enc_loss_cls: 0.1565  enc_loss_bbox: 0.0412  enc_loss_iou: 0.3382  dn_loss_cls: 0.0053  dn_loss_bbox: 0.0270  dn_loss_iou: 0.2219  d0.dn_loss_cls: 0.0256  d0.dn_loss_bbox: 0.0367  d0.dn_loss_iou: 0.2920  d1.dn_loss_cls: 0.0090  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2344  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2246  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0270  d3.dn_loss_iou: 0.2218  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0270  d4.dn_loss_iou: 0.2218  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:30:15 - mmengine - INFO - Epoch(train) [4][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:03:45  time: 1.7982  data_time: 0.0170  memory: 16249  grad_norm: 37.7491  loss: 4.4971  loss_cls: 0.0718  loss_bbox: 0.0327  loss_iou: 0.2759  d0.loss_cls: 0.1146  d0.loss_bbox: 0.0347  d0.loss_iou: 0.2910  d1.loss_cls: 0.0902  d1.loss_bbox: 0.0337  d1.loss_iou: 0.2869  d2.loss_cls: 0.0782  d2.loss_bbox: 0.0331  d2.loss_iou: 0.2818  d3.loss_cls: 0.0728  d3.loss_bbox: 0.0328  d3.loss_iou: 0.2789  d4.loss_cls: 0.0726  d4.loss_bbox: 0.0326  d4.loss_iou: 0.2756  enc_loss_cls: 0.1417  enc_loss_bbox: 0.0380  enc_loss_iou: 0.3131  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0254  dn_loss_iou: 0.2190  d0.dn_loss_cls: 0.0245  d0.dn_loss_bbox: 0.0351  d0.dn_loss_iou: 0.2895  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0270  d1.dn_loss_iou: 0.2320  d2.dn_loss_cls: 0.0047  d2.dn_loss_bbox: 0.0258  d2.dn_loss_iou: 0.2219  d3.dn_loss_cls: 0.0041  d3.dn_loss_bbox: 0.0254  d3.dn_loss_iou: 0.2186  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.0254  d4.dn_loss_iou: 0.2189  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:31:44 - mmengine - INFO - Epoch(train) [4][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:02:14  time: 1.7989  data_time: 0.0182  memory: 16268  grad_norm: 40.9094  loss: 4.4155  loss_cls: 0.0728  loss_bbox: 0.0299  loss_iou: 0.2657  d0.loss_cls: 0.1100  d0.loss_bbox: 0.0313  d0.loss_iou: 0.2753  d1.loss_cls: 0.0869  d1.loss_bbox: 0.0306  d1.loss_iou: 0.2724  d2.loss_cls: 0.0794  d2.loss_bbox: 0.0300  d2.loss_iou: 0.2680  d3.loss_cls: 0.0759  d3.loss_bbox: 0.0297  d3.loss_iou: 0.2642  d4.loss_cls: 0.0730  d4.loss_bbox: 0.0298  d4.loss_iou: 0.2655  enc_loss_cls: 0.1441  enc_loss_bbox: 0.0345  enc_loss_iou: 0.2963  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0242  dn_loss_iou: 0.2251  d0.dn_loss_cls: 0.0267  d0.dn_loss_bbox: 0.0339  d0.dn_loss_iou: 0.3005  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0256  d1.dn_loss_iou: 0.2369  d2.dn_loss_cls: 0.0050  d2.dn_loss_bbox: 0.0245  d2.dn_loss_iou: 0.2273  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0242  d3.dn_loss_iou: 0.2251  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0242  d4.dn_loss_iou: 0.2249  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0002
2025/10/29 12:32:47 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 12:32:47 - mmengine - INFO - Saving checkpoint at 4 epochs
2025/10/29 12:32:59 - mmengine - INFO - Epoch(val) [4][ 50/429]    eta: 0:00:34  time: 0.0913  data_time: 0.0030  memory: 16256  
2025/10/29 12:33:03 - mmengine - INFO - Epoch(val) [4][100/429]    eta: 0:00:29  time: 0.0878  data_time: 0.0023  memory: 3164  
2025/10/29 12:33:07 - mmengine - INFO - Epoch(val) [4][150/429]    eta: 0:00:24  time: 0.0875  data_time: 0.0023  memory: 3169  
2025/10/29 12:33:12 - mmengine - INFO - Epoch(val) [4][200/429]    eta: 0:00:20  time: 0.0876  data_time: 0.0023  memory: 3165  
2025/10/29 12:33:16 - mmengine - INFO - Epoch(val) [4][250/429]    eta: 0:00:15  time: 0.0879  data_time: 0.0023  memory: 3167  
2025/10/29 12:33:21 - mmengine - INFO - Epoch(val) [4][300/429]    eta: 0:00:11  time: 0.0873  data_time: 0.0023  memory: 3175  
2025/10/29 12:33:25 - mmengine - INFO - Epoch(val) [4][350/429]    eta: 0:00:06  time: 0.0874  data_time: 0.0023  memory: 3169  
2025/10/29 12:33:29 - mmengine - INFO - Epoch(val) [4][400/429]    eta: 0:00:02  time: 0.0871  data_time: 0.0023  memory: 3165  
2025/10/29 12:33:34 - mmengine - INFO - {'instance_F1_score': 0.6626199740596628, 'instance_acc': 0.499884642005691, 'image_F1_score': 0.543459174714662, 'image_acc': 0.3939393939393939}
2025/10/29 12:33:34 - mmengine - INFO - Epoch(val) [4][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.6626  grefcoco_val/refdrone/instance_acc: 0.4999  grefcoco_val/refdrone/image_F1_score: 0.5435  grefcoco_val/refdrone/image_acc: 0.3939  data_time: 0.0024  time: 0.0879
2025/10/29 12:35:04 - mmengine - INFO - Epoch(train) [5][  50/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:59:41  time: 1.8128  data_time: 0.0178  memory: 16256  grad_norm: 33.2521  loss: 4.1702  loss_cls: 0.0661  loss_bbox: 0.0277  loss_iou: 0.2533  d0.loss_cls: 0.1007  d0.loss_bbox: 0.0299  d0.loss_iou: 0.2691  d1.loss_cls: 0.0829  d1.loss_bbox: 0.0283  d1.loss_iou: 0.2583  d2.loss_cls: 0.0767  d2.loss_bbox: 0.0277  d2.loss_iou: 0.2540  d3.loss_cls: 0.0701  d3.loss_bbox: 0.0277  d3.loss_iou: 0.2523  d4.loss_cls: 0.0672  d4.loss_bbox: 0.0276  d4.loss_iou: 0.2522  enc_loss_cls: 0.1216  enc_loss_bbox: 0.0327  enc_loss_iou: 0.2898  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0240  dn_loss_iou: 0.2119  d0.dn_loss_cls: 0.0245  d0.dn_loss_bbox: 0.0326  d0.dn_loss_iou: 0.2755  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0256  d1.dn_loss_iou: 0.2234  d2.dn_loss_cls: 0.0050  d2.dn_loss_bbox: 0.0244  d2.dn_loss_iou: 0.2143  d3.dn_loss_cls: 0.0041  d3.dn_loss_bbox: 0.0241  d3.dn_loss_iou: 0.2124  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0240  d4.dn_loss_iou: 0.2118  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:36:34 - mmengine - INFO - Epoch(train) [5][ 100/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:58:11  time: 1.7994  data_time: 0.0170  memory: 16269  grad_norm: 28.4434  loss: 4.0588  loss_cls: 0.0483  loss_bbox: 0.0302  loss_iou: 0.2429  d0.loss_cls: 0.0887  d0.loss_bbox: 0.0321  d0.loss_iou: 0.2575  d1.loss_cls: 0.0664  d1.loss_bbox: 0.0315  d1.loss_iou: 0.2522  d2.loss_cls: 0.0570  d2.loss_bbox: 0.0308  d2.loss_iou: 0.2470  d3.loss_cls: 0.0521  d3.loss_bbox: 0.0302  d3.loss_iou: 0.2430  d4.loss_cls: 0.0497  d4.loss_bbox: 0.0302  d4.loss_iou: 0.2429  enc_loss_cls: 0.1102  enc_loss_bbox: 0.0362  enc_loss_iou: 0.2779  dn_loss_cls: 0.0043  dn_loss_bbox: 0.0268  dn_loss_iou: 0.2149  d0.dn_loss_cls: 0.0253  d0.dn_loss_bbox: 0.0372  d0.dn_loss_iou: 0.2821  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0290  d1.dn_loss_iou: 0.2296  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0275  d2.dn_loss_iou: 0.2182  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0269  d3.dn_loss_iou: 0.2153  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.0268  d4.dn_loss_iou: 0.2148  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:38:04 - mmengine - INFO - Epoch(train) [5][ 150/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:56:40  time: 1.8032  data_time: 0.0169  memory: 16269  grad_norm: 25.7569  loss: 4.8532  loss_cls: 0.0730  loss_bbox: 0.0355  loss_iou: 0.3120  d0.loss_cls: 0.1158  d0.loss_bbox: 0.0357  d0.loss_iou: 0.3258  d1.loss_cls: 0.0892  d1.loss_bbox: 0.0360  d1.loss_iou: 0.3168  d2.loss_cls: 0.0822  d2.loss_bbox: 0.0358  d2.loss_iou: 0.3161  d3.loss_cls: 0.0770  d3.loss_bbox: 0.0354  d3.loss_iou: 0.3112  d4.loss_cls: 0.0740  d4.loss_bbox: 0.0355  d4.loss_iou: 0.3120  enc_loss_cls: 0.1479  enc_loss_bbox: 0.0408  enc_loss_iou: 0.3456  dn_loss_cls: 0.0071  dn_loss_bbox: 0.0257  dn_loss_iou: 0.2302  d0.dn_loss_cls: 0.0286  d0.dn_loss_bbox: 0.0354  d0.dn_loss_iou: 0.2988  d1.dn_loss_cls: 0.0103  d1.dn_loss_bbox: 0.0272  d1.dn_loss_iou: 0.2424  d2.dn_loss_cls: 0.0077  d2.dn_loss_bbox: 0.0260  d2.dn_loss_iou: 0.2327  d3.dn_loss_cls: 0.0071  d3.dn_loss_bbox: 0.0257  d3.dn_loss_iou: 0.2305  d4.dn_loss_cls: 0.0071  d4.dn_loss_bbox: 0.0257  d4.dn_loss_iou: 0.2302  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:39:35 - mmengine - INFO - Epoch(train) [5][ 200/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:55:10  time: 1.8109  data_time: 0.0177  memory: 16275  grad_norm: 27.4942  loss: 4.2392  loss_cls: 0.0522  loss_bbox: 0.0307  loss_iou: 0.2755  d0.loss_cls: 0.0839  d0.loss_bbox: 0.0338  d0.loss_iou: 0.2948  d1.loss_cls: 0.0653  d1.loss_bbox: 0.0318  d1.loss_iou: 0.2841  d2.loss_cls: 0.0595  d2.loss_bbox: 0.0310  d2.loss_iou: 0.2772  d3.loss_cls: 0.0564  d3.loss_bbox: 0.0307  d3.loss_iou: 0.2757  d4.loss_cls: 0.0522  d4.loss_bbox: 0.0307  d4.loss_iou: 0.2758  enc_loss_cls: 0.1095  enc_loss_bbox: 0.0352  enc_loss_iou: 0.3093  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0258  dn_loss_iou: 0.2088  d0.dn_loss_cls: 0.0224  d0.dn_loss_bbox: 0.0350  d0.dn_loss_iou: 0.2718  d1.dn_loss_cls: 0.0063  d1.dn_loss_bbox: 0.0273  d1.dn_loss_iou: 0.2207  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0261  d2.dn_loss_iou: 0.2125  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2097  d4.dn_loss_cls: 0.0039  d4.dn_loss_bbox: 0.0258  d4.dn_loss_iou: 0.2086  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:41:05 - mmengine - INFO - Epoch(train) [5][ 250/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:53:40  time: 1.7982  data_time: 0.0172  memory: 16265  grad_norm: 26.2464  loss: 4.4373  loss_cls: 0.0666  loss_bbox: 0.0308  loss_iou: 0.2848  d0.loss_cls: 0.1101  d0.loss_bbox: 0.0330  d0.loss_iou: 0.3021  d1.loss_cls: 0.0850  d1.loss_bbox: 0.0315  d1.loss_iou: 0.2928  d2.loss_cls: 0.0750  d2.loss_bbox: 0.0312  d2.loss_iou: 0.2895  d3.loss_cls: 0.0695  d3.loss_bbox: 0.0308  d3.loss_iou: 0.2856  d4.loss_cls: 0.0672  d4.loss_bbox: 0.0308  d4.loss_iou: 0.2848  enc_loss_cls: 0.1325  enc_loss_bbox: 0.0380  enc_loss_iou: 0.3208  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2110  d0.dn_loss_cls: 0.0240  d0.dn_loss_bbox: 0.0321  d0.dn_loss_iou: 0.2710  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.0254  d1.dn_loss_iou: 0.2217  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0244  d2.dn_loss_iou: 0.2138  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0242  d3.dn_loss_iou: 0.2114  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0241  d4.dn_loss_iou: 0.2109  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:42:35 - mmengine - INFO - Epoch(train) [5][ 300/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:52:10  time: 1.8027  data_time: 0.0179  memory: 16268  grad_norm: 25.0079  loss: 4.5590  loss_cls: 0.0599  loss_bbox: 0.0318  loss_iou: 0.3051  d0.loss_cls: 0.0982  d0.loss_bbox: 0.0337  d0.loss_iou: 0.3175  d1.loss_cls: 0.0739  d1.loss_bbox: 0.0325  d1.loss_iou: 0.3110  d2.loss_cls: 0.0679  d2.loss_bbox: 0.0324  d2.loss_iou: 0.3080  d3.loss_cls: 0.0627  d3.loss_bbox: 0.0318  d3.loss_iou: 0.3048  d4.loss_cls: 0.0608  d4.loss_bbox: 0.0317  d4.loss_iou: 0.3043  enc_loss_cls: 0.1271  enc_loss_bbox: 0.0357  enc_loss_iou: 0.3359  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0236  dn_loss_iou: 0.2201  d0.dn_loss_cls: 0.0229  d0.dn_loss_bbox: 0.0317  d0.dn_loss_iou: 0.2800  d1.dn_loss_cls: 0.0071  d1.dn_loss_bbox: 0.0248  d1.dn_loss_iou: 0.2304  d2.dn_loss_cls: 0.0046  d2.dn_loss_bbox: 0.0238  d2.dn_loss_iou: 0.2223  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0236  d3.dn_loss_iou: 0.2207  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0236  d4.dn_loss_iou: 0.2201  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:44:05 - mmengine - INFO - Epoch(train) [5][ 350/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:50:40  time: 1.8056  data_time: 0.0171  memory: 16251  grad_norm: 27.3385  loss: 4.1424  loss_cls: 0.0522  loss_bbox: 0.0284  loss_iou: 0.2497  d0.loss_cls: 0.0835  d0.loss_bbox: 0.0300  d0.loss_iou: 0.2605  d1.loss_cls: 0.0685  d1.loss_bbox: 0.0290  d1.loss_iou: 0.2541  d2.loss_cls: 0.0588  d2.loss_bbox: 0.0288  d2.loss_iou: 0.2520  d3.loss_cls: 0.0561  d3.loss_bbox: 0.0285  d3.loss_iou: 0.2498  d4.loss_cls: 0.0524  d4.loss_bbox: 0.0284  d4.loss_iou: 0.2496  enc_loss_cls: 0.1153  enc_loss_bbox: 0.0323  enc_loss_iou: 0.2769  dn_loss_cls: 0.0036  dn_loss_bbox: 0.0260  dn_loss_iou: 0.2271  d0.dn_loss_cls: 0.0251  d0.dn_loss_bbox: 0.0350  d0.dn_loss_iou: 0.2946  d1.dn_loss_cls: 0.0060  d1.dn_loss_bbox: 0.0273  d1.dn_loss_iou: 0.2381  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0263  d2.dn_loss_iou: 0.2295  d3.dn_loss_cls: 0.0037  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2272  d4.dn_loss_cls: 0.0037  d4.dn_loss_bbox: 0.0260  d4.dn_loss_iou: 0.2270  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0002
2025/10/29 12:45:36 - mmengine - INFO - Epoch(train) [5][ 400/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:49:09  time: 1.8108  data_time: 0.0169  memory: 16271  grad_norm: 28.7953  loss: 4.3976  loss_cls: 0.0564  loss_bbox: 0.0323  loss_iou: 0.2823  d0.loss_cls: 0.0978  d0.loss_bbox: 0.0345  d0.loss_iou: 0.3000  d1.loss_cls: 0.0729  d1.loss_bbox: 0.0329  d1.loss_iou: 0.2887  d2.loss_cls: 0.0647  d2.loss_bbox: 0.0324  d2.loss_iou: 0.2833  d3.loss_cls: 0.0614  d3.loss_bbox: 0.0323  d3.loss_iou: 0.2820  d4.loss_cls: 0.0575  d4.loss_bbox: 0.0323  d4.loss_iou: 0.2818  enc_loss_cls: 0.1284  enc_loss_bbox: 0.0369  enc_loss_iou: 0.3166  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0246  dn_loss_iou: 0.2177  d0.dn_loss_cls: 0.0231  d0.dn_loss_bbox: 0.0332  d0.dn_loss_iou: 0.2833  d1.dn_loss_cls: 0.0059  d1.dn_loss_bbox: 0.0258  d1.dn_loss_iou: 0.2293  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0249  d2.dn_loss_iou: 0.2204  d3.dn_loss_cls: 0.0039  d3.dn_loss_bbox: 0.0246  d3.dn_loss_iou: 0.2181  d4.dn_loss_cls: 0.0037  d4.dn_loss_bbox: 0.0246  d4.dn_loss_iou: 0.2177  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:47:06 - mmengine - INFO - Epoch(train) [5][ 450/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:47:39  time: 1.8074  data_time: 0.0175  memory: 16267  grad_norm: 25.4858  loss: 4.1823  loss_cls: 0.0441  loss_bbox: 0.0318  loss_iou: 0.2691  d0.loss_cls: 0.0836  d0.loss_bbox: 0.0333  d0.loss_iou: 0.2818  d1.loss_cls: 0.0603  d1.loss_bbox: 0.0327  d1.loss_iou: 0.2748  d2.loss_cls: 0.0513  d2.loss_bbox: 0.0322  d2.loss_iou: 0.2712  d3.loss_cls: 0.0489  d3.loss_bbox: 0.0318  d3.loss_iou: 0.2689  d4.loss_cls: 0.0449  d4.loss_bbox: 0.0318  d4.loss_iou: 0.2690  enc_loss_cls: 0.1064  enc_loss_bbox: 0.0354  enc_loss_iou: 0.2973  dn_loss_cls: 0.0035  dn_loss_bbox: 0.0247  dn_loss_iou: 0.2181  d0.dn_loss_cls: 0.0202  d0.dn_loss_bbox: 0.0326  d0.dn_loss_iou: 0.2789  d1.dn_loss_cls: 0.0055  d1.dn_loss_bbox: 0.0258  d1.dn_loss_iou: 0.2286  d2.dn_loss_cls: 0.0041  d2.dn_loss_bbox: 0.0250  d2.dn_loss_iou: 0.2206  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0247  d3.dn_loss_iou: 0.2183  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0247  d4.dn_loss_iou: 0.2180  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:48:36 - mmengine - INFO - Epoch(train) [5][ 500/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:46:09  time: 1.8004  data_time: 0.0166  memory: 16258  grad_norm: 28.1786  loss: 4.1013  loss_cls: 0.0439  loss_bbox: 0.0300  loss_iou: 0.2803  d0.loss_cls: 0.0845  d0.loss_bbox: 0.0318  d0.loss_iou: 0.2943  d1.loss_cls: 0.0593  d1.loss_bbox: 0.0307  d1.loss_iou: 0.2872  d2.loss_cls: 0.0501  d2.loss_bbox: 0.0303  d2.loss_iou: 0.2837  d3.loss_cls: 0.0478  d3.loss_bbox: 0.0300  d3.loss_iou: 0.2812  d4.loss_cls: 0.0437  d4.loss_bbox: 0.0300  d4.loss_iou: 0.2803  enc_loss_cls: 0.1089  enc_loss_bbox: 0.0341  enc_loss_iou: 0.3097  dn_loss_cls: 0.0026  dn_loss_bbox: 0.0218  dn_loss_iou: 0.1965  d0.dn_loss_cls: 0.0204  d0.dn_loss_bbox: 0.0292  d0.dn_loss_iou: 0.2545  d1.dn_loss_cls: 0.0051  d1.dn_loss_bbox: 0.0231  d1.dn_loss_iou: 0.2074  d2.dn_loss_cls: 0.0033  d2.dn_loss_bbox: 0.0222  d2.dn_loss_iou: 0.1994  d3.dn_loss_cls: 0.0028  d3.dn_loss_bbox: 0.0219  d3.dn_loss_iou: 0.1969  d4.dn_loss_cls: 0.0026  d4.dn_loss_bbox: 0.0218  d4.dn_loss_iou: 0.1965  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:50:06 - mmengine - INFO - Epoch(train) [5][ 550/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:44:39  time: 1.8020  data_time: 0.0169  memory: 16264  grad_norm: 33.3758  loss: 4.1086  loss_cls: 0.0492  loss_bbox: 0.0312  loss_iou: 0.2696  d0.loss_cls: 0.0726  d0.loss_bbox: 0.0328  d0.loss_iou: 0.2811  d1.loss_cls: 0.0590  d1.loss_bbox: 0.0319  d1.loss_iou: 0.2741  d2.loss_cls: 0.0538  d2.loss_bbox: 0.0310  d2.loss_iou: 0.2688  d3.loss_cls: 0.0542  d3.loss_bbox: 0.0306  d3.loss_iou: 0.2671  d4.loss_cls: 0.0510  d4.loss_bbox: 0.0307  d4.loss_iou: 0.2673  enc_loss_cls: 0.1012  enc_loss_bbox: 0.0349  enc_loss_iou: 0.2958  dn_loss_cls: 0.0029  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2092  d0.dn_loss_cls: 0.0207  d0.dn_loss_bbox: 0.0321  d0.dn_loss_iou: 0.2698  d1.dn_loss_cls: 0.0052  d1.dn_loss_bbox: 0.0254  d1.dn_loss_iou: 0.2181  d2.dn_loss_cls: 0.0032  d2.dn_loss_bbox: 0.0245  d2.dn_loss_iou: 0.2115  d3.dn_loss_cls: 0.0029  d3.dn_loss_bbox: 0.0242  d3.dn_loss_iou: 0.2096  d4.dn_loss_cls: 0.0029  d4.dn_loss_bbox: 0.0242  d4.dn_loss_iou: 0.2091  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:51:37 - mmengine - INFO - Epoch(train) [5][ 600/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:43:09  time: 1.8159  data_time: 0.0177  memory: 16267  grad_norm: 28.2938  loss: 4.6079  loss_cls: 0.0602  loss_bbox: 0.0317  loss_iou: 0.3156  d0.loss_cls: 0.0995  d0.loss_bbox: 0.0329  d0.loss_iou: 0.3276  d1.loss_cls: 0.0727  d1.loss_bbox: 0.0325  d1.loss_iou: 0.3204  d2.loss_cls: 0.0672  d2.loss_bbox: 0.0322  d2.loss_iou: 0.3193  d3.loss_cls: 0.0646  d3.loss_bbox: 0.0318  d3.loss_iou: 0.3168  d4.loss_cls: 0.0629  d4.loss_bbox: 0.0316  d4.loss_iou: 0.3145  enc_loss_cls: 0.1236  enc_loss_bbox: 0.0353  enc_loss_iou: 0.3451  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0233  dn_loss_iou: 0.2163  d0.dn_loss_cls: 0.0245  d0.dn_loss_bbox: 0.0303  d0.dn_loss_iou: 0.2750  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0244  d1.dn_loss_iou: 0.2267  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0237  d2.dn_loss_iou: 0.2189  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0234  d3.dn_loss_iou: 0.2167  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0233  d4.dn_loss_iou: 0.2162  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:53:08 - mmengine - INFO - Epoch(train) [5][ 650/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:41:38  time: 1.8075  data_time: 0.0183  memory: 16258  grad_norm: 27.3610  loss: 4.1826  loss_cls: 0.0475  loss_bbox: 0.0315  loss_iou: 0.2655  d0.loss_cls: 0.0801  d0.loss_bbox: 0.0332  d0.loss_iou: 0.2808  d1.loss_cls: 0.0632  d1.loss_bbox: 0.0314  d1.loss_iou: 0.2675  d2.loss_cls: 0.0550  d2.loss_bbox: 0.0317  d2.loss_iou: 0.2682  d3.loss_cls: 0.0504  d3.loss_bbox: 0.0316  d3.loss_iou: 0.2675  d4.loss_cls: 0.0484  d4.loss_bbox: 0.0314  d4.loss_iou: 0.2649  enc_loss_cls: 0.1035  enc_loss_bbox: 0.0358  enc_loss_iou: 0.2969  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0243  dn_loss_iou: 0.2192  d0.dn_loss_cls: 0.0229  d0.dn_loss_bbox: 0.0323  d0.dn_loss_iou: 0.2839  d1.dn_loss_cls: 0.0065  d1.dn_loss_bbox: 0.0255  d1.dn_loss_iou: 0.2310  d2.dn_loss_cls: 0.0046  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2218  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0244  d3.dn_loss_iou: 0.2198  d4.dn_loss_cls: 0.0037  d4.dn_loss_bbox: 0.0243  d4.dn_loss_iou: 0.2191  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:54:37 - mmengine - INFO - Epoch(train) [5][ 700/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:40:08  time: 1.7972  data_time: 0.0174  memory: 16262  grad_norm: 27.5691  loss: 4.1758  loss_cls: 0.0536  loss_bbox: 0.0305  loss_iou: 0.2608  d0.loss_cls: 0.0891  d0.loss_bbox: 0.0324  d0.loss_iou: 0.2742  d1.loss_cls: 0.0671  d1.loss_bbox: 0.0308  d1.loss_iou: 0.2677  d2.loss_cls: 0.0593  d2.loss_bbox: 0.0301  d2.loss_iou: 0.2611  d3.loss_cls: 0.0571  d3.loss_bbox: 0.0300  d3.loss_iou: 0.2610  d4.loss_cls: 0.0584  d4.loss_bbox: 0.0300  d4.loss_iou: 0.2595  enc_loss_cls: 0.1159  enc_loss_bbox: 0.0362  enc_loss_iou: 0.2981  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0254  dn_loss_iou: 0.2118  d0.dn_loss_cls: 0.0264  d0.dn_loss_bbox: 0.0344  d0.dn_loss_iou: 0.2751  d1.dn_loss_cls: 0.0090  d1.dn_loss_bbox: 0.0271  d1.dn_loss_iou: 0.2241  d2.dn_loss_cls: 0.0063  d2.dn_loss_bbox: 0.0258  d2.dn_loss_iou: 0.2149  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0254  d3.dn_loss_iou: 0.2123  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0253  d4.dn_loss_iou: 0.2118  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:56:09 - mmengine - INFO - Epoch(train) [5][ 750/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:38:38  time: 1.8225  data_time: 0.0171  memory: 16281  grad_norm: 25.3692  loss: 4.4121  loss_cls: 0.0547  loss_bbox: 0.0335  loss_iou: 0.2791  d0.loss_cls: 0.0887  d0.loss_bbox: 0.0338  d0.loss_iou: 0.2912  d1.loss_cls: 0.0655  d1.loss_bbox: 0.0345  d1.loss_iou: 0.2870  d2.loss_cls: 0.0624  d2.loss_bbox: 0.0338  d2.loss_iou: 0.2815  d3.loss_cls: 0.0585  d3.loss_bbox: 0.0336  d3.loss_iou: 0.2795  d4.loss_cls: 0.0543  d4.loss_bbox: 0.0335  d4.loss_iou: 0.2790  enc_loss_cls: 0.1187  enc_loss_bbox: 0.0389  enc_loss_iou: 0.3111  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2256  d0.dn_loss_cls: 0.0254  d0.dn_loss_bbox: 0.0363  d0.dn_loss_iou: 0.2895  d1.dn_loss_cls: 0.0080  d1.dn_loss_bbox: 0.0289  d1.dn_loss_iou: 0.2367  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2285  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0273  d3.dn_loss_iou: 0.2260  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2256  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:57:39 - mmengine - INFO - Epoch(train) [5][ 800/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:37:08  time: 1.8030  data_time: 0.0177  memory: 16249  grad_norm: 28.3613  loss: 3.9404  loss_cls: 0.0474  loss_bbox: 0.0267  loss_iou: 0.2599  d0.loss_cls: 0.0826  d0.loss_bbox: 0.0288  d0.loss_iou: 0.2772  d1.loss_cls: 0.0617  d1.loss_bbox: 0.0272  d1.loss_iou: 0.2632  d2.loss_cls: 0.0538  d2.loss_bbox: 0.0268  d2.loss_iou: 0.2608  d3.loss_cls: 0.0513  d3.loss_bbox: 0.0267  d3.loss_iou: 0.2593  d4.loss_cls: 0.0486  d4.loss_bbox: 0.0266  d4.loss_iou: 0.2590  enc_loss_cls: 0.1140  enc_loss_bbox: 0.0306  enc_loss_iou: 0.2886  dn_loss_cls: 0.0027  dn_loss_bbox: 0.0222  dn_loss_iou: 0.1958  d0.dn_loss_cls: 0.0182  d0.dn_loss_bbox: 0.0294  d0.dn_loss_iou: 0.2502  d1.dn_loss_cls: 0.0049  d1.dn_loss_bbox: 0.0234  d1.dn_loss_iou: 0.2058  d2.dn_loss_cls: 0.0033  d2.dn_loss_bbox: 0.0224  d2.dn_loss_iou: 0.1979  d3.dn_loss_cls: 0.0029  d3.dn_loss_bbox: 0.0222  d3.dn_loss_iou: 0.1960  d4.dn_loss_cls: 0.0027  d4.dn_loss_bbox: 0.0222  d4.dn_loss_iou: 0.1958  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:59:09 - mmengine - INFO - Epoch(train) [5][ 850/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:35:38  time: 1.7984  data_time: 0.0179  memory: 16255  grad_norm: 24.5794  loss: 3.9193  loss_cls: 0.0405  loss_bbox: 0.0312  loss_iou: 0.2463  d0.loss_cls: 0.0695  d0.loss_bbox: 0.0318  d0.loss_iou: 0.2562  d1.loss_cls: 0.0528  d1.loss_bbox: 0.0308  d1.loss_iou: 0.2492  d2.loss_cls: 0.0454  d2.loss_bbox: 0.0313  d2.loss_iou: 0.2485  d3.loss_cls: 0.0421  d3.loss_bbox: 0.0317  d3.loss_iou: 0.2475  d4.loss_cls: 0.0408  d4.loss_bbox: 0.0312  d4.loss_iou: 0.2462  enc_loss_cls: 0.1026  enc_loss_bbox: 0.0336  enc_loss_iou: 0.2683  dn_loss_cls: 0.0028  dn_loss_bbox: 0.0253  dn_loss_iou: 0.2097  d0.dn_loss_cls: 0.0235  d0.dn_loss_bbox: 0.0343  d0.dn_loss_iou: 0.2731  d1.dn_loss_cls: 0.0062  d1.dn_loss_bbox: 0.0267  d1.dn_loss_iou: 0.2208  d2.dn_loss_cls: 0.0035  d2.dn_loss_bbox: 0.0258  d2.dn_loss_iou: 0.2122  d3.dn_loss_cls: 0.0030  d3.dn_loss_bbox: 0.0254  d3.dn_loss_iou: 0.2101  d4.dn_loss_cls: 0.0028  d4.dn_loss_bbox: 0.0253  d4.dn_loss_iou: 0.2096  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 12:59:27 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 13:00:40 - mmengine - INFO - Epoch(train) [5][ 900/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:34:08  time: 1.8181  data_time: 0.0176  memory: 16269  grad_norm: 25.8392  loss: 4.6306  loss_cls: 0.0549  loss_bbox: 0.0341  loss_iou: 0.3158  d0.loss_cls: 0.0971  d0.loss_bbox: 0.0363  d0.loss_iou: 0.3337  d1.loss_cls: 0.0680  d1.loss_bbox: 0.0347  d1.loss_iou: 0.3220  d2.loss_cls: 0.0616  d2.loss_bbox: 0.0340  d2.loss_iou: 0.3168  d3.loss_cls: 0.0567  d3.loss_bbox: 0.0337  d3.loss_iou: 0.3135  d4.loss_cls: 0.0548  d4.loss_bbox: 0.0339  d4.loss_iou: 0.3144  enc_loss_cls: 0.1296  enc_loss_bbox: 0.0390  enc_loss_iou: 0.3575  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0238  dn_loss_iou: 0.2169  d0.dn_loss_cls: 0.0255  d0.dn_loss_bbox: 0.0323  d0.dn_loss_iou: 0.2842  d1.dn_loss_cls: 0.0078  d1.dn_loss_bbox: 0.0252  d1.dn_loss_iou: 0.2289  d2.dn_loss_cls: 0.0049  d2.dn_loss_bbox: 0.0241  d2.dn_loss_iou: 0.2200  d3.dn_loss_cls: 0.0041  d3.dn_loss_bbox: 0.0238  d3.dn_loss_iou: 0.2175  d4.dn_loss_cls: 0.0037  d4.dn_loss_bbox: 0.0238  d4.dn_loss_iou: 0.2169  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 13:02:10 - mmengine - INFO - Epoch(train) [5][ 950/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:32:37  time: 1.8056  data_time: 0.0194  memory: 16285  grad_norm: 25.7558  loss: 3.8935  loss_cls: 0.0414  loss_bbox: 0.0276  loss_iou: 0.2472  d0.loss_cls: 0.0713  d0.loss_bbox: 0.0292  d0.loss_iou: 0.2588  d1.loss_cls: 0.0549  d1.loss_bbox: 0.0282  d1.loss_iou: 0.2536  d2.loss_cls: 0.0509  d2.loss_bbox: 0.0277  d2.loss_iou: 0.2481  d3.loss_cls: 0.0457  d3.loss_bbox: 0.0276  d3.loss_iou: 0.2472  d4.loss_cls: 0.0416  d4.loss_bbox: 0.0276  d4.loss_iou: 0.2471  enc_loss_cls: 0.0984  enc_loss_bbox: 0.0311  enc_loss_iou: 0.2712  dn_loss_cls: 0.0035  dn_loss_bbox: 0.0243  dn_loss_iou: 0.2085  d0.dn_loss_cls: 0.0216  d0.dn_loss_bbox: 0.0316  d0.dn_loss_iou: 0.2661  d1.dn_loss_cls: 0.0056  d1.dn_loss_bbox: 0.0253  d1.dn_loss_iou: 0.2171  d2.dn_loss_cls: 0.0041  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2106  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0244  d3.dn_loss_iou: 0.2087  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0243  d4.dn_loss_iou: 0.2085  loss_num: 0.0002  d0.loss_num: 0.0002  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:03:40 - mmengine - INFO - Epoch(train) [5][1000/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:31:07  time: 1.8032  data_time: 0.0177  memory: 16271  grad_norm: 24.6723  loss: 4.2129  loss_cls: 0.0362  loss_bbox: 0.0332  loss_iou: 0.2821  d0.loss_cls: 0.0741  d0.loss_bbox: 0.0354  d0.loss_iou: 0.3018  d1.loss_cls: 0.0532  d1.loss_bbox: 0.0339  d1.loss_iou: 0.2897  d2.loss_cls: 0.0453  d2.loss_bbox: 0.0333  d2.loss_iou: 0.2840  d3.loss_cls: 0.0403  d3.loss_bbox: 0.0333  d3.loss_iou: 0.2828  d4.loss_cls: 0.0368  d4.loss_bbox: 0.0332  d4.loss_iou: 0.2820  enc_loss_cls: 0.1014  enc_loss_bbox: 0.0401  enc_loss_iou: 0.3224  dn_loss_cls: 0.0030  dn_loss_bbox: 0.0256  dn_loss_iou: 0.2106  d0.dn_loss_cls: 0.0215  d0.dn_loss_bbox: 0.0341  d0.dn_loss_iou: 0.2684  d1.dn_loss_cls: 0.0055  d1.dn_loss_bbox: 0.0270  d1.dn_loss_iou: 0.2197  d2.dn_loss_cls: 0.0034  d2.dn_loss_bbox: 0.0261  d2.dn_loss_iou: 0.2134  d3.dn_loss_cls: 0.0029  d3.dn_loss_bbox: 0.0257  d3.dn_loss_iou: 0.2112  d4.dn_loss_cls: 0.0030  d4.dn_loss_bbox: 0.0256  d4.dn_loss_iou: 0.2105  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 13:05:10 - mmengine - INFO - Epoch(train) [5][1050/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:29:37  time: 1.8066  data_time: 0.0189  memory: 16275  grad_norm: 25.6763  loss: 3.8612  loss_cls: 0.0384  loss_bbox: 0.0259  loss_iou: 0.2340  d0.loss_cls: 0.0746  d0.loss_bbox: 0.0275  d0.loss_iou: 0.2475  d1.loss_cls: 0.0534  d1.loss_bbox: 0.0264  d1.loss_iou: 0.2382  d2.loss_cls: 0.0451  d2.loss_bbox: 0.0262  d2.loss_iou: 0.2376  d3.loss_cls: 0.0407  d3.loss_bbox: 0.0260  d3.loss_iou: 0.2358  d4.loss_cls: 0.0388  d4.loss_bbox: 0.0260  d4.loss_iou: 0.2352  enc_loss_cls: 0.1008  enc_loss_bbox: 0.0291  enc_loss_iou: 0.2606  dn_loss_cls: 0.0035  dn_loss_bbox: 0.0242  dn_loss_iou: 0.2181  d0.dn_loss_cls: 0.0248  d0.dn_loss_bbox: 0.0325  d0.dn_loss_iou: 0.2842  d1.dn_loss_cls: 0.0067  d1.dn_loss_bbox: 0.0256  d1.dn_loss_iou: 0.2302  d2.dn_loss_cls: 0.0046  d2.dn_loss_bbox: 0.0245  d2.dn_loss_iou: 0.2207  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0243  d3.dn_loss_iou: 0.2183  d4.dn_loss_cls: 0.0037  d4.dn_loss_bbox: 0.0242  d4.dn_loss_iou: 0.2181  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:06:41 - mmengine - INFO - Epoch(train) [5][1100/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:28:07  time: 1.8061  data_time: 0.0170  memory: 16282  grad_norm: 28.4723  loss: 4.7599  loss_cls: 0.0552  loss_bbox: 0.0366  loss_iou: 0.3310  d0.loss_cls: 0.0964  d0.loss_bbox: 0.0388  d0.loss_iou: 0.3508  d1.loss_cls: 0.0774  d1.loss_bbox: 0.0370  d1.loss_iou: 0.3371  d2.loss_cls: 0.0681  d2.loss_bbox: 0.0361  d2.loss_iou: 0.3289  d3.loss_cls: 0.0613  d3.loss_bbox: 0.0364  d3.loss_iou: 0.3293  d4.loss_cls: 0.0553  d4.loss_bbox: 0.0368  d4.loss_iou: 0.3313  enc_loss_cls: 0.1278  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3714  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0245  dn_loss_iou: 0.2150  d0.dn_loss_cls: 0.0249  d0.dn_loss_bbox: 0.0321  d0.dn_loss_iou: 0.2748  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0258  d1.dn_loss_iou: 0.2263  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0249  d2.dn_loss_iou: 0.2177  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0246  d3.dn_loss_iou: 0.2153  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0245  d4.dn_loss_iou: 0.2150  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:08:11 - mmengine - INFO - Epoch(train) [5][1150/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:26:36  time: 1.7979  data_time: 0.0186  memory: 16275  grad_norm: 25.8027  loss: 4.0418  loss_cls: 0.0402  loss_bbox: 0.0298  loss_iou: 0.2609  d0.loss_cls: 0.0738  d0.loss_bbox: 0.0325  d0.loss_iou: 0.2825  d1.loss_cls: 0.0500  d1.loss_bbox: 0.0311  d1.loss_iou: 0.2714  d2.loss_cls: 0.0468  d2.loss_bbox: 0.0298  d2.loss_iou: 0.2618  d3.loss_cls: 0.0443  d3.loss_bbox: 0.0295  d3.loss_iou: 0.2588  d4.loss_cls: 0.0409  d4.loss_bbox: 0.0296  d4.loss_iou: 0.2601  enc_loss_cls: 0.0901  enc_loss_bbox: 0.0346  enc_loss_iou: 0.2948  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0253  dn_loss_iou: 0.2094  d0.dn_loss_cls: 0.0264  d0.dn_loss_bbox: 0.0339  d0.dn_loss_iou: 0.2697  d1.dn_loss_cls: 0.0084  d1.dn_loss_bbox: 0.0268  d1.dn_loss_iou: 0.2203  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0257  d2.dn_loss_iou: 0.2126  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0254  d3.dn_loss_iou: 0.2099  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.0253  d4.dn_loss_iou: 0.2094  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 13:09:41 - mmengine - INFO - Epoch(train) [5][1200/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:25:06  time: 1.8165  data_time: 0.0179  memory: 16279  grad_norm: 26.1686  loss: 4.1875  loss_cls: 0.0385  loss_bbox: 0.0324  loss_iou: 0.2783  d0.loss_cls: 0.0705  d0.loss_bbox: 0.0345  d0.loss_iou: 0.2909  d1.loss_cls: 0.0506  d1.loss_bbox: 0.0331  d1.loss_iou: 0.2849  d2.loss_cls: 0.0452  d2.loss_bbox: 0.0329  d2.loss_iou: 0.2834  d3.loss_cls: 0.0437  d3.loss_bbox: 0.0324  d3.loss_iou: 0.2785  d4.loss_cls: 0.0396  d4.loss_bbox: 0.0325  d4.loss_iou: 0.2787  enc_loss_cls: 0.0949  enc_loss_bbox: 0.0365  enc_loss_iou: 0.3047  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0260  dn_loss_iou: 0.2138  d0.dn_loss_cls: 0.0206  d0.dn_loss_bbox: 0.0345  d0.dn_loss_iou: 0.2760  d1.dn_loss_cls: 0.0067  d1.dn_loss_bbox: 0.0274  d1.dn_loss_iou: 0.2258  d2.dn_loss_cls: 0.0045  d2.dn_loss_bbox: 0.0263  d2.dn_loss_iou: 0.2167  d3.dn_loss_cls: 0.0039  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2144  d4.dn_loss_cls: 0.0039  d4.dn_loss_bbox: 0.0260  d4.dn_loss_iou: 0.2137  loss_num: 0.0001  d0.loss_num: 0.0002  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 13:11:12 - mmengine - INFO - Epoch(train) [5][1250/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:23:36  time: 1.8087  data_time: 0.0168  memory: 16265  grad_norm: 26.8075  loss: 3.6546  loss_cls: 0.0300  loss_bbox: 0.0293  loss_iou: 0.2385  d0.loss_cls: 0.0618  d0.loss_bbox: 0.0314  d0.loss_iou: 0.2511  d1.loss_cls: 0.0448  d1.loss_bbox: 0.0303  d1.loss_iou: 0.2440  d2.loss_cls: 0.0354  d2.loss_bbox: 0.0297  d2.loss_iou: 0.2401  d3.loss_cls: 0.0336  d3.loss_bbox: 0.0293  d3.loss_iou: 0.2387  d4.loss_cls: 0.0313  d4.loss_bbox: 0.0292  d4.loss_iou: 0.2384  enc_loss_cls: 0.0881  enc_loss_bbox: 0.0333  enc_loss_iou: 0.2653  dn_loss_cls: 0.0020  dn_loss_bbox: 0.0234  dn_loss_iou: 0.1912  d0.dn_loss_cls: 0.0201  d0.dn_loss_bbox: 0.0316  d0.dn_loss_iou: 0.2479  d1.dn_loss_cls: 0.0050  d1.dn_loss_bbox: 0.0247  d1.dn_loss_iou: 0.2009  d2.dn_loss_cls: 0.0028  d2.dn_loss_bbox: 0.0237  d2.dn_loss_iou: 0.1931  d3.dn_loss_cls: 0.0023  d3.dn_loss_bbox: 0.0235  d3.dn_loss_iou: 0.1913  d4.dn_loss_cls: 0.0020  d4.dn_loss_bbox: 0.0234  d4.dn_loss_iou: 0.1911  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:12:41 - mmengine - INFO - Epoch(train) [5][1300/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:22:06  time: 1.7941  data_time: 0.0180  memory: 16256  grad_norm: 25.0001  loss: 4.0732  loss_cls: 0.0420  loss_bbox: 0.0293  loss_iou: 0.2578  d0.loss_cls: 0.0853  d0.loss_bbox: 0.0298  d0.loss_iou: 0.2734  d1.loss_cls: 0.0571  d1.loss_bbox: 0.0303  d1.loss_iou: 0.2641  d2.loss_cls: 0.0527  d2.loss_bbox: 0.0292  d2.loss_iou: 0.2567  d3.loss_cls: 0.0443  d3.loss_bbox: 0.0293  d3.loss_iou: 0.2566  d4.loss_cls: 0.0419  d4.loss_bbox: 0.0295  d4.loss_iou: 0.2574  enc_loss_cls: 0.0994  enc_loss_bbox: 0.0336  enc_loss_iou: 0.2886  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0244  dn_loss_iou: 0.2165  d0.dn_loss_cls: 0.0241  d0.dn_loss_bbox: 0.0324  d0.dn_loss_iou: 0.2795  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.0257  d1.dn_loss_iou: 0.2269  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0247  d2.dn_loss_iou: 0.2185  d3.dn_loss_cls: 0.0053  d3.dn_loss_bbox: 0.0245  d3.dn_loss_iou: 0.2167  d4.dn_loss_cls: 0.0051  d4.dn_loss_bbox: 0.0244  d4.dn_loss_iou: 0.2165  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:14:12 - mmengine - INFO - Epoch(train) [5][1350/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:20:36  time: 1.8100  data_time: 0.0176  memory: 16269  grad_norm: 27.7472  loss: 3.9509  loss_cls: 0.0441  loss_bbox: 0.0254  loss_iou: 0.2354  d0.loss_cls: 0.0756  d0.loss_bbox: 0.0275  d0.loss_iou: 0.2512  d1.loss_cls: 0.0530  d1.loss_bbox: 0.0261  d1.loss_iou: 0.2414  d2.loss_cls: 0.0475  d2.loss_bbox: 0.0256  d2.loss_iou: 0.2372  d3.loss_cls: 0.0471  d3.loss_bbox: 0.0255  d3.loss_iou: 0.2360  d4.loss_cls: 0.0450  d4.loss_bbox: 0.0254  d4.loss_iou: 0.2351  enc_loss_cls: 0.1107  enc_loss_bbox: 0.0295  enc_loss_iou: 0.2620  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0260  dn_loss_iou: 0.2259  d0.dn_loss_cls: 0.0222  d0.dn_loss_bbox: 0.0348  d0.dn_loss_iou: 0.2901  d1.dn_loss_cls: 0.0062  d1.dn_loss_bbox: 0.0272  d1.dn_loss_iou: 0.2361  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0263  d2.dn_loss_iou: 0.2284  d3.dn_loss_cls: 0.0041  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2262  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0260  d4.dn_loss_iou: 0.2258  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:15:42 - mmengine - INFO - Epoch(train) [5][1400/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:19:05  time: 1.8068  data_time: 0.0178  memory: 16264  grad_norm: 25.7483  loss: 3.9175  loss_cls: 0.0401  loss_bbox: 0.0290  loss_iou: 0.2584  d0.loss_cls: 0.0683  d0.loss_bbox: 0.0309  d0.loss_iou: 0.2731  d1.loss_cls: 0.0514  d1.loss_bbox: 0.0297  d1.loss_iou: 0.2614  d2.loss_cls: 0.0444  d2.loss_bbox: 0.0295  d2.loss_iou: 0.2582  d3.loss_cls: 0.0433  d3.loss_bbox: 0.0290  d3.loss_iou: 0.2575  d4.loss_cls: 0.0412  d4.loss_bbox: 0.0290  d4.loss_iou: 0.2571  enc_loss_cls: 0.1038  enc_loss_bbox: 0.0334  enc_loss_iou: 0.2866  dn_loss_cls: 0.0036  dn_loss_bbox: 0.0236  dn_loss_iou: 0.1995  d0.dn_loss_cls: 0.0224  d0.dn_loss_bbox: 0.0320  d0.dn_loss_iou: 0.2570  d1.dn_loss_cls: 0.0060  d1.dn_loss_bbox: 0.0247  d1.dn_loss_iou: 0.2081  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0239  d2.dn_loss_iou: 0.2017  d3.dn_loss_cls: 0.0038  d3.dn_loss_bbox: 0.0236  d3.dn_loss_iou: 0.1999  d4.dn_loss_cls: 0.0036  d4.dn_loss_bbox: 0.0236  d4.dn_loss_iou: 0.1995  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:17:12 - mmengine - INFO - Epoch(train) [5][1450/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:17:35  time: 1.7954  data_time: 0.0173  memory: 16258  grad_norm: 25.0396  loss: 4.2135  loss_cls: 0.0393  loss_bbox: 0.0308  loss_iou: 0.2862  d0.loss_cls: 0.0692  d0.loss_bbox: 0.0330  d0.loss_iou: 0.3061  d1.loss_cls: 0.0517  d1.loss_bbox: 0.0316  d1.loss_iou: 0.2945  d2.loss_cls: 0.0454  d2.loss_bbox: 0.0312  d2.loss_iou: 0.2908  d3.loss_cls: 0.0425  d3.loss_bbox: 0.0308  d3.loss_iou: 0.2867  d4.loss_cls: 0.0392  d4.loss_bbox: 0.0307  d4.loss_iou: 0.2861  enc_loss_cls: 0.0943  enc_loss_bbox: 0.0353  enc_loss_iou: 0.3201  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0228  dn_loss_iou: 0.2115  d0.dn_loss_cls: 0.0236  d0.dn_loss_bbox: 0.0307  d0.dn_loss_iou: 0.2726  d1.dn_loss_cls: 0.0071  d1.dn_loss_bbox: 0.0239  d1.dn_loss_iou: 0.2214  d2.dn_loss_cls: 0.0048  d2.dn_loss_bbox: 0.0230  d2.dn_loss_iou: 0.2136  d3.dn_loss_cls: 0.0043  d3.dn_loss_bbox: 0.0229  d3.dn_loss_iou: 0.2119  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0228  d4.dn_loss_iou: 0.2115  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 13:18:43 - mmengine - INFO - Epoch(train) [5][1500/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:16:05  time: 1.8106  data_time: 0.0177  memory: 16292  grad_norm: 26.8634  loss: 4.4809  loss_cls: 0.0568  loss_bbox: 0.0325  loss_iou: 0.3033  d0.loss_cls: 0.0939  d0.loss_bbox: 0.0339  d0.loss_iou: 0.3173  d1.loss_cls: 0.0692  d1.loss_bbox: 0.0329  d1.loss_iou: 0.3078  d2.loss_cls: 0.0627  d2.loss_bbox: 0.0322  d2.loss_iou: 0.3015  d3.loss_cls: 0.0611  d3.loss_bbox: 0.0321  d3.loss_iou: 0.3015  d4.loss_cls: 0.0558  d4.loss_bbox: 0.0325  d4.loss_iou: 0.3030  enc_loss_cls: 0.1169  enc_loss_bbox: 0.0360  enc_loss_iou: 0.3316  dn_loss_cls: 0.0035  dn_loss_bbox: 0.0240  dn_loss_iou: 0.2142  d0.dn_loss_cls: 0.0236  d0.dn_loss_bbox: 0.0323  d0.dn_loss_iou: 0.2790  d1.dn_loss_cls: 0.0069  d1.dn_loss_bbox: 0.0255  d1.dn_loss_iou: 0.2267  d2.dn_loss_cls: 0.0042  d2.dn_loss_bbox: 0.0243  d2.dn_loss_iou: 0.2169  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0241  d3.dn_loss_iou: 0.2145  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0240  d4.dn_loss_iou: 0.2141  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:20:13 - mmengine - INFO - Epoch(train) [5][1550/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:14:35  time: 1.8102  data_time: 0.0176  memory: 16268  grad_norm: 27.5532  loss: 4.5363  loss_cls: 0.0560  loss_bbox: 0.0308  loss_iou: 0.3085  d0.loss_cls: 0.1057  d0.loss_bbox: 0.0326  d0.loss_iou: 0.3232  d1.loss_cls: 0.0809  d1.loss_bbox: 0.0313  d1.loss_iou: 0.3120  d2.loss_cls: 0.0615  d2.loss_bbox: 0.0310  d2.loss_iou: 0.3099  d3.loss_cls: 0.0587  d3.loss_bbox: 0.0309  d3.loss_iou: 0.3096  d4.loss_cls: 0.0569  d4.loss_bbox: 0.0308  d4.loss_iou: 0.3083  enc_loss_cls: 0.1299  enc_loss_bbox: 0.0357  enc_loss_iou: 0.3432  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0221  dn_loss_iou: 0.2139  d0.dn_loss_cls: 0.0230  d0.dn_loss_bbox: 0.0292  d0.dn_loss_iou: 0.2758  d1.dn_loss_cls: 0.0068  d1.dn_loss_bbox: 0.0232  d1.dn_loss_iou: 0.2255  d2.dn_loss_cls: 0.0047  d2.dn_loss_bbox: 0.0225  d2.dn_loss_iou: 0.2170  d3.dn_loss_cls: 0.0039  d3.dn_loss_bbox: 0.0222  d3.dn_loss_iou: 0.2144  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0221  d4.dn_loss_iou: 0.2139  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:21:43 - mmengine - INFO - Epoch(train) [5][1600/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:13:04  time: 1.7946  data_time: 0.0172  memory: 16269  grad_norm: 26.7070  loss: 4.1403  loss_cls: 0.0457  loss_bbox: 0.0298  loss_iou: 0.2557  d0.loss_cls: 0.0728  d0.loss_bbox: 0.0326  d0.loss_iou: 0.2712  d1.loss_cls: 0.0526  d1.loss_bbox: 0.0303  d1.loss_iou: 0.2579  d2.loss_cls: 0.0493  d2.loss_bbox: 0.0305  d2.loss_iou: 0.2572  d3.loss_cls: 0.0494  d3.loss_bbox: 0.0299  d3.loss_iou: 0.2560  d4.loss_cls: 0.0439  d4.loss_bbox: 0.0299  d4.loss_iou: 0.2564  enc_loss_cls: 0.0937  enc_loss_bbox: 0.0343  enc_loss_iou: 0.2821  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0258  dn_loss_iou: 0.2296  d0.dn_loss_cls: 0.0244  d0.dn_loss_bbox: 0.0351  d0.dn_loss_iou: 0.2972  d1.dn_loss_cls: 0.0073  d1.dn_loss_bbox: 0.0274  d1.dn_loss_iou: 0.2420  d2.dn_loss_cls: 0.0049  d2.dn_loss_bbox: 0.0262  d2.dn_loss_iou: 0.2327  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0259  d3.dn_loss_iou: 0.2300  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0258  d4.dn_loss_iou: 0.2297  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:23:14 - mmengine - INFO - Epoch(train) [5][1650/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:11:34  time: 1.8167  data_time: 0.0174  memory: 16279  grad_norm: 24.5751  loss: 4.0011  loss_cls: 0.0498  loss_bbox: 0.0297  loss_iou: 0.2452  d0.loss_cls: 0.0809  d0.loss_bbox: 0.0322  d0.loss_iou: 0.2625  d1.loss_cls: 0.0629  d1.loss_bbox: 0.0303  d1.loss_iou: 0.2498  d2.loss_cls: 0.0561  d2.loss_bbox: 0.0301  d2.loss_iou: 0.2494  d3.loss_cls: 0.0512  d3.loss_bbox: 0.0298  d3.loss_iou: 0.2473  d4.loss_cls: 0.0480  d4.loss_bbox: 0.0299  d4.loss_iou: 0.2488  enc_loss_cls: 0.1026  enc_loss_bbox: 0.0348  enc_loss_iou: 0.2799  dn_loss_cls: 0.0034  dn_loss_bbox: 0.0252  dn_loss_iou: 0.2111  d0.dn_loss_cls: 0.0228  d0.dn_loss_bbox: 0.0347  d0.dn_loss_iou: 0.2721  d1.dn_loss_cls: 0.0067  d1.dn_loss_bbox: 0.0267  d1.dn_loss_iou: 0.2221  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0256  d2.dn_loss_iou: 0.2138  d3.dn_loss_cls: 0.0037  d3.dn_loss_bbox: 0.0252  d3.dn_loss_iou: 0.2116  d4.dn_loss_cls: 0.0034  d4.dn_loss_bbox: 0.0252  d4.dn_loss_iou: 0.2111  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:24:44 - mmengine - INFO - Epoch(train) [5][1700/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:10:04  time: 1.8013  data_time: 0.0171  memory: 16269  grad_norm: 29.1758  loss: 4.0573  loss_cls: 0.0458  loss_bbox: 0.0292  loss_iou: 0.2618  d0.loss_cls: 0.0781  d0.loss_bbox: 0.0321  d0.loss_iou: 0.2802  d1.loss_cls: 0.0625  d1.loss_bbox: 0.0299  d1.loss_iou: 0.2666  d2.loss_cls: 0.0521  d2.loss_bbox: 0.0296  d2.loss_iou: 0.2659  d3.loss_cls: 0.0488  d3.loss_bbox: 0.0293  d3.loss_iou: 0.2632  d4.loss_cls: 0.0462  d4.loss_bbox: 0.0292  d4.loss_iou: 0.2617  enc_loss_cls: 0.1060  enc_loss_bbox: 0.0366  enc_loss_iou: 0.3007  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0236  dn_loss_iou: 0.2030  d0.dn_loss_cls: 0.0238  d0.dn_loss_bbox: 0.0322  d0.dn_loss_iou: 0.2664  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0248  d1.dn_loss_iou: 0.2136  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0239  d2.dn_loss_iou: 0.2057  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0236  d3.dn_loss_iou: 0.2032  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0236  d4.dn_loss_iou: 0.2030  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:26:14 - mmengine - INFO - Epoch(train) [5][1750/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:08:34  time: 1.8042  data_time: 0.0192  memory: 16274  grad_norm: 26.3039  loss: 4.1334  loss_cls: 0.0435  loss_bbox: 0.0305  loss_iou: 0.2663  d0.loss_cls: 0.0836  d0.loss_bbox: 0.0322  d0.loss_iou: 0.2791  d1.loss_cls: 0.0611  d1.loss_bbox: 0.0310  d1.loss_iou: 0.2705  d2.loss_cls: 0.0530  d2.loss_bbox: 0.0306  d2.loss_iou: 0.2673  d3.loss_cls: 0.0469  d3.loss_bbox: 0.0304  d3.loss_iou: 0.2654  d4.loss_cls: 0.0452  d4.loss_bbox: 0.0304  d4.loss_iou: 0.2650  enc_loss_cls: 0.1184  enc_loss_bbox: 0.0343  enc_loss_iou: 0.2934  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0235  dn_loss_iou: 0.2129  d0.dn_loss_cls: 0.0254  d0.dn_loss_bbox: 0.0311  d0.dn_loss_iou: 0.2734  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.0247  d1.dn_loss_iou: 0.2237  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0238  d2.dn_loss_iou: 0.2160  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0235  d3.dn_loss_iou: 0.2133  d4.dn_loss_cls: 0.0045  d4.dn_loss_bbox: 0.0235  d4.dn_loss_iou: 0.2129  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 13:27:44 - mmengine - INFO - Epoch(train) [5][1800/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:07:04  time: 1.8046  data_time: 0.0174  memory: 16275  grad_norm: 28.6995  loss: 4.3165  loss_cls: 0.0542  loss_bbox: 0.0309  loss_iou: 0.2883  d0.loss_cls: 0.0851  d0.loss_bbox: 0.0332  d0.loss_iou: 0.3035  d1.loss_cls: 0.0701  d1.loss_bbox: 0.0318  d1.loss_iou: 0.2922  d2.loss_cls: 0.0608  d2.loss_bbox: 0.0315  d2.loss_iou: 0.2915  d3.loss_cls: 0.0566  d3.loss_bbox: 0.0311  d3.loss_iou: 0.2900  d4.loss_cls: 0.0550  d4.loss_bbox: 0.0309  d4.loss_iou: 0.2869  enc_loss_cls: 0.1095  enc_loss_bbox: 0.0352  enc_loss_iou: 0.3191  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0246  dn_loss_iou: 0.2080  d0.dn_loss_cls: 0.0246  d0.dn_loss_bbox: 0.0331  d0.dn_loss_iou: 0.2673  d1.dn_loss_cls: 0.0079  d1.dn_loss_bbox: 0.0259  d1.dn_loss_iou: 0.2186  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0250  d2.dn_loss_iou: 0.2109  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0247  d3.dn_loss_iou: 0.2082  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0246  d4.dn_loss_iou: 0.2079  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 13:29:14 - mmengine - INFO - Epoch(train) [5][1850/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:05:33  time: 1.8054  data_time: 0.0168  memory: 16274  grad_norm: 30.6335  loss: 4.2714  loss_cls: 0.0518  loss_bbox: 0.0347  loss_iou: 0.2854  d0.loss_cls: 0.0940  d0.loss_bbox: 0.0365  d0.loss_iou: 0.3007  d1.loss_cls: 0.0682  d1.loss_bbox: 0.0354  d1.loss_iou: 0.2925  d2.loss_cls: 0.0567  d2.loss_bbox: 0.0349  d2.loss_iou: 0.2872  d3.loss_cls: 0.0577  d3.loss_bbox: 0.0347  d3.loss_iou: 0.2853  d4.loss_cls: 0.0518  d4.loss_bbox: 0.0346  d4.loss_iou: 0.2839  enc_loss_cls: 0.1156  enc_loss_bbox: 0.0389  enc_loss_iou: 0.3199  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0228  dn_loss_iou: 0.1996  d0.dn_loss_cls: 0.0241  d0.dn_loss_bbox: 0.0310  d0.dn_loss_iou: 0.2605  d1.dn_loss_cls: 0.0071  d1.dn_loss_bbox: 0.0241  d1.dn_loss_iou: 0.2116  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0231  d2.dn_loss_iou: 0.2026  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0228  d3.dn_loss_iou: 0.1999  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.0228  d4.dn_loss_iou: 0.1996  loss_num: 0.0001  d0.loss_num: 0.0002  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 13:29:33 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 13:30:45 - mmengine - INFO - Epoch(train) [5][1900/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:04:03  time: 1.8044  data_time: 0.0178  memory: 16251  grad_norm: 28.8572  loss: 4.3776  loss_cls: 0.0523  loss_bbox: 0.0303  loss_iou: 0.2948  d0.loss_cls: 0.0837  d0.loss_bbox: 0.0334  d0.loss_iou: 0.3148  d1.loss_cls: 0.0674  d1.loss_bbox: 0.0309  d1.loss_iou: 0.3012  d2.loss_cls: 0.0583  d2.loss_bbox: 0.0307  d2.loss_iou: 0.2988  d3.loss_cls: 0.0568  d3.loss_bbox: 0.0303  d3.loss_iou: 0.2952  d4.loss_cls: 0.0529  d4.loss_bbox: 0.0303  d4.loss_iou: 0.2951  enc_loss_cls: 0.1076  enc_loss_bbox: 0.0347  enc_loss_iou: 0.3257  dn_loss_cls: 0.0036  dn_loss_bbox: 0.0238  dn_loss_iou: 0.2136  d0.dn_loss_cls: 0.0222  d0.dn_loss_bbox: 0.0315  d0.dn_loss_iou: 0.2730  d1.dn_loss_cls: 0.0064  d1.dn_loss_bbox: 0.0252  d1.dn_loss_iou: 0.2245  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0242  d2.dn_loss_iou: 0.2163  d3.dn_loss_cls: 0.0038  d3.dn_loss_bbox: 0.0238  d3.dn_loss_iou: 0.2140  d4.dn_loss_cls: 0.0036  d4.dn_loss_bbox: 0.0238  d4.dn_loss_iou: 0.2136  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 13:32:15 - mmengine - INFO - Epoch(train) [5][1950/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:02:33  time: 1.8123  data_time: 0.0179  memory: 16256  grad_norm: 29.7004  loss: 3.5522  loss_cls: 0.0289  loss_bbox: 0.0258  loss_iou: 0.2205  d0.loss_cls: 0.0631  d0.loss_bbox: 0.0265  d0.loss_iou: 0.2284  d1.loss_cls: 0.0441  d1.loss_bbox: 0.0262  d1.loss_iou: 0.2259  d2.loss_cls: 0.0387  d2.loss_bbox: 0.0257  d2.loss_iou: 0.2205  d3.loss_cls: 0.0334  d3.loss_bbox: 0.0258  d3.loss_iou: 0.2209  d4.loss_cls: 0.0295  d4.loss_bbox: 0.0258  d4.loss_iou: 0.2204  enc_loss_cls: 0.0870  enc_loss_bbox: 0.0295  enc_loss_iou: 0.2455  dn_loss_cls: 0.0032  dn_loss_bbox: 0.0240  dn_loss_iou: 0.1990  d0.dn_loss_cls: 0.0230  d0.dn_loss_bbox: 0.0318  d0.dn_loss_iou: 0.2548  d1.dn_loss_cls: 0.0064  d1.dn_loss_bbox: 0.0251  d1.dn_loss_iou: 0.2084  d2.dn_loss_cls: 0.0041  d2.dn_loss_bbox: 0.0243  d2.dn_loss_iou: 0.2016  d3.dn_loss_cls: 0.0033  d3.dn_loss_bbox: 0.0240  d3.dn_loss_iou: 0.1994  d4.dn_loss_cls: 0.0033  d4.dn_loss_bbox: 0.0240  d4.dn_loss_iou: 0.1990  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 13:33:45 - mmengine - INFO - Epoch(train) [5][2000/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:01:03  time: 1.7991  data_time: 0.0168  memory: 16257  grad_norm: 29.3633  loss: 4.0989  loss_cls: 0.0431  loss_bbox: 0.0299  loss_iou: 0.2623  d0.loss_cls: 0.0794  d0.loss_bbox: 0.0319  d0.loss_iou: 0.2780  d1.loss_cls: 0.0606  d1.loss_bbox: 0.0303  d1.loss_iou: 0.2667  d2.loss_cls: 0.0512  d2.loss_bbox: 0.0301  d2.loss_iou: 0.2645  d3.loss_cls: 0.0484  d3.loss_bbox: 0.0300  d3.loss_iou: 0.2631  d4.loss_cls: 0.0429  d4.loss_bbox: 0.0299  d4.loss_iou: 0.2624  enc_loss_cls: 0.1031  enc_loss_bbox: 0.0349  enc_loss_iou: 0.2989  dn_loss_cls: 0.0035  dn_loss_bbox: 0.0243  dn_loss_iou: 0.2143  d0.dn_loss_cls: 0.0217  d0.dn_loss_bbox: 0.0320  d0.dn_loss_iou: 0.2734  d1.dn_loss_cls: 0.0064  d1.dn_loss_bbox: 0.0255  d1.dn_loss_iou: 0.2244  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2173  d3.dn_loss_cls: 0.0037  d3.dn_loss_bbox: 0.0244  d3.dn_loss_iou: 0.2146  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0243  d4.dn_loss_iou: 0.2143  loss_num: 0.0001  d0.loss_num: 0.0002  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 13:34:48 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v2_20251029_082517
2025/10/29 13:34:48 - mmengine - INFO - Saving checkpoint at 5 epochs
2025/10/29 13:35:00 - mmengine - INFO - Epoch(val) [5][ 50/429]    eta: 0:00:34  time: 0.0910  data_time: 0.0034  memory: 16275  
2025/10/29 13:35:04 - mmengine - INFO - Epoch(val) [5][100/429]    eta: 0:00:29  time: 0.0875  data_time: 0.0024  memory: 3164  
2025/10/29 13:35:08 - mmengine - INFO - Epoch(val) [5][150/429]    eta: 0:00:24  time: 0.0880  data_time: 0.0023  memory: 3169  
2025/10/29 13:35:13 - mmengine - INFO - Epoch(val) [5][200/429]    eta: 0:00:20  time: 0.0878  data_time: 0.0023  memory: 3165  
2025/10/29 13:35:17 - mmengine - INFO - Epoch(val) [5][250/429]    eta: 0:00:15  time: 0.0876  data_time: 0.0024  memory: 3167  
2025/10/29 13:35:21 - mmengine - INFO - Epoch(val) [5][300/429]    eta: 0:00:11  time: 0.0876  data_time: 0.0024  memory: 3175  
2025/10/29 13:35:26 - mmengine - INFO - Epoch(val) [5][350/429]    eta: 0:00:06  time: 0.0874  data_time: 0.0023  memory: 3169  
2025/10/29 13:35:30 - mmengine - INFO - Epoch(val) [5][400/429]    eta: 0:00:02  time: 0.0873  data_time: 0.0023  memory: 3165  
2025/10/29 13:35:35 - mmengine - INFO - {'instance_F1_score': 0.7111153901699321, 'instance_acc': 0.5551519644180875, 'image_F1_score': 0.5650862068965518, 'image_acc': 0.412004662004662}
2025/10/29 13:35:35 - mmengine - INFO - Epoch(val) [5][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.7111  grefcoco_val/refdrone/instance_acc: 0.5552  grefcoco_val/refdrone/image_F1_score: 0.5651  grefcoco_val/refdrone/image_acc: 0.4120  data_time: 0.0025  time: 0.0879
