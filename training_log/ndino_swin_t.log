2024/10/31 06:26:50 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1766494737
    GPU 0,1,2,3: NVIDIA A100-SXM4-40GB
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0a0
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1766494737
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 4
------------------------------------------------------------

2024/10/31 06:26:52 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=32, enable=False)
backend_args = None
coco_od_dataset = dict(
    ann_file='o365v1_train_odvg.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='data/objects365v1/',
    filter_cfg=dict(filter_empty_gt=False),
    label_map_file='o365v1_label_map.json',
    pipeline=[
        dict(backend_args=None, type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            transforms=[
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                400,
                                4200,
                            ),
                            (
                                500,
                                4200,
                            ),
                            (
                                600,
                                4200,
                            ),
                        ],
                        type='RandomChoiceResize'),
                    dict(
                        allow_negative_crop=True,
                        crop_size=(
                            384,
                            600,
                        ),
                        crop_type='absolute_range',
                        type='RandomCrop'),
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
            ],
            type='RandomChoice'),
        dict(min_gt_bbox_wh=(
            0.01,
            0.01,
        ), type='FilterAnnotations'),
        dict(
            max_tokens=256,
            num_sample_negative=85,
            tokenizer_name=
            '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
            type='RandomSamplingNegPos'),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'flip',
                'flip_direction',
                'text',
                'custom_entities',
                'tokens_positive',
                'dataset_mode',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    type='ODVGDataset')
data_root = 'data/objects365v1/'
dataset_prefixes = [
    'grefcoco_val',
]
dataset_type = 'ODVGDataset'
datasets = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        backend_args=None,
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        test_mode=True,
        type='MDETRStyleRefCocoDataset'),
]
default_hooks = dict(
    checkpoint=dict(interval=1, type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='GroundingVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
lang_model_name = '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased'
launcher = 'pytorch'
load_from = '/mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-t_numbranch_pretrain_31_refcoco.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
max_epochs = 5
metrics = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        iou_thrs=0.5,
        metric='bbox',
        thresh_f1=1.0,
        thresh_score=0.7,
        type='RefDroneMetric'),
]
model = dict(
    as_two_stage=True,
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            6,
            2,
        ],
        drop_path_rate=0.2,
        drop_rate=0.0,
        embed_dims=96,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            '/mnt/public/usr/sunzhichao/mmdetection/swin_tiny_patch4_window7_224.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            3,
            6,
            12,
            24,
        ],
        out_indices=(
            1,
            2,
            3,
        ),
        patch_norm=True,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=7,
        with_cp=True),
    bbox_head=dict(
        contrastive_cfg=dict(bias=True, log_scale='auto', max_text_len=256),
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            type='FocalLoss',
            use_sigmoid=True),
        num_classes=256,
        sync_cls_avg_factor=True,
        type='GroundingDINOHeadNumv11'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    decoder=dict(
        layer_cfg=dict(
            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            cross_attn_text_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),
        num_layers=6,
        post_norm_cfg=None,
        return_intermediate=True),
    dn_cfg=dict(
        box_noise_scale=1.0,
        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
        label_noise_scale=0.5),
    encoder=dict(
        fusion_layer_cfg=dict(
            embed_dim=1024,
            init_values=0.0001,
            l_dim=256,
            num_heads=4,
            v_dim=256),
        layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=4)),
        num_cp=6,
        num_layers=6,
        text_layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=1024, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=4))),
    language_model=dict(
        add_pooling_layer=False,
        max_tokens=256,
        name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        pad_to_max=False,
        special_tokens_list=[
            '[CLS]',
            '[SEP]',
            '.',
            '?',
        ],
        type='BertModel',
        use_sub_sentence_represent=True),
    neck=dict(
        act_cfg=None,
        bias=True,
        in_channels=[
            192,
            384,
            768,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=4,
        out_channels=256,
        type='ChannelMapper'),
    num_queries=900,
    positional_encoding=dict(
        normalize=True, num_feats=128, offset=0.0, temperature=20),
    test_cfg=dict(max_per_img=300),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='BinaryFocalLossCost', weight=2.0),
                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                dict(iou_mode='giou', type='IoUCost', weight=2.0),
            ],
            type='HungarianAssigner')),
    type='NumGroundingDINO',
    with_box_refine=True)
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0002, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.1))),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=5,
        gamma=0.1,
        milestones=[
            3,
        ],
        type='MultiStepLR'),
]
pretrained = '/mnt/public/usr/sunzhichao/mmdetection/swin_tiny_patch4_window7_224.pth'
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
                backend_args=None,
                data_prefix=dict(
                    img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='pillow',
                        type='LoadImageFromFile'),
                    dict(
                        backend='pillow',
                        keep_ratio=True,
                        scale=(
                            800,
                            1333,
                        ),
                        type='FixScaleResize'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(
                        meta_keys=(
                            'img_id',
                            'img_path',
                            'ori_shape',
                            'img_shape',
                            'scale_factor',
                            'text',
                            'custom_entities',
                            'tokens_positive',
                        ),
                        type='PackDetInputs'),
                ],
                return_classes=True,
                test_mode=True,
                type='MDETRStyleRefCocoDataset'),
        ],
        type='ConcatDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'tokens_positive',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=5, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=4,
    dataset=dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_train7_vg.json',
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        filter_cfg=dict(filter_empty_gt=False),
        pipeline=[
            dict(backend_args=None, type='LoadImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(prob=0.0, type='RandomFlip'),
            dict(
                keep_ratio=True,
                scales=[
                    (
                        480,
                        1333,
                    ),
                    (
                        512,
                        1333,
                    ),
                    (
                        544,
                        1333,
                    ),
                    (
                        576,
                        1333,
                    ),
                    (
                        608,
                        1333,
                    ),
                    (
                        640,
                        1333,
                    ),
                    (
                        672,
                        1333,
                    ),
                    (
                        704,
                        1333,
                    ),
                    (
                        736,
                        1333,
                    ),
                    (
                        768,
                        1333,
                    ),
                    (
                        800,
                        1333,
                    ),
                ],
                type='RandomChoiceResize'),
            dict(
                max_tokens=256,
                num_sample_negative=85,
                tokenizer_name=
                '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
                type='RandomSamplingNegPos'),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'flip',
                    'flip_direction',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                    'dataset_mode',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        type='ODVGDataset'),
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.0, type='RandomFlip'),
    dict(
        keep_ratio=True,
        scales=[
            (
                480,
                1333,
            ),
            (
                512,
                1333,
            ),
            (
                544,
                1333,
            ),
            (
                576,
                1333,
            ),
            (
                608,
                1333,
            ),
            (
                640,
                1333,
            ),
            (
                672,
                1333,
            ),
            (
                704,
                1333,
            ),
            (
                736,
                1333,
            ),
            (
                768,
                1333,
            ),
            (
                800,
                1333,
            ),
        ],
        type='RandomChoiceResize'),
    dict(
        max_tokens=256,
        num_sample_negative=85,
        tokenizer_name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        type='RandomSamplingNegPos'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'text',
            'custom_entities',
            'tokens_positive',
            'dataset_mode',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
                backend_args=None,
                data_prefix=dict(
                    img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='pillow',
                        type='LoadImageFromFile'),
                    dict(
                        backend='pillow',
                        keep_ratio=True,
                        scale=(
                            800,
                            1333,
                        ),
                        type='FixScaleResize'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(
                        meta_keys=(
                            'img_id',
                            'img_path',
                            'ori_shape',
                            'img_shape',
                            'scale_factor',
                            'text',
                            'custom_entities',
                            'tokens_positive',
                        ),
                        type='PackDetInputs'),
                ],
                return_classes=True,
                test_mode=True,
                type='MDETRStyleRefCocoDataset'),
        ],
        type='ConcatDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_dataset_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    backend_args=None,
    data_prefix=dict(img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='pillow',
            type='LoadImageFromFile'),
        dict(
            backend='pillow',
            keep_ratio=True,
            scale=(
                800,
                1333,
            ),
            type='FixScaleResize'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'text',
                'custom_entities',
                'tokens_positive',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    test_mode=True,
    type='MDETRStyleRefCocoDataset')
val_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
val_evaluator_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    iou_thrs=0.5,
    metric='bbox',
    thresh_f1=1.0,
    thresh_score=0.7,
    type='RefDroneMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3'

2024/10/31 06:26:56 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr_mult=0.1
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr=2e-05
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:weight_decay=0.0001
2024/10/31 06:26:59 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr_mult=0.1
2024/10/31 06:27:01 - mmengine - INFO - Loads checkpoint by local backend from path: /mnt/public/usr/sunzhichao/mmdetection/swin_tiny_patch4_window7_224.pth
Name of parameter - Initialization information

level_embed - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.patch_embed.projection.weight - torch.Size([96, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([288]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([96, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([384, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([96, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 3]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([288, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([288]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([96, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([384, 96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([96, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([96]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([192, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 6]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([576, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([576]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([192, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([768, 192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([192, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([384, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([169, 12]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1152, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1152]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([384, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([1536, 384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([384, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([768, 1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([169, 24]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([2304, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([2304]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([768, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([3072, 768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([768, 3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm1.weight - torch.Size([192]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm1.bias - torch.Size([192]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.weight - torch.Size([384]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.bias - torch.Size([384]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.conv.weight - torch.Size([256, 192, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.conv.weight - torch.Size([256, 384, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.conv.weight - torch.Size([256, 768, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.conv.weight - torch.Size([256, 768, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.extra_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.3.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.5.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.6.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.single_value_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

query_embedding.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.word_embeddings.weight - torch.Size([30522, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.position_embeddings.weight - torch.Size([512, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight - torch.Size([2, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.weight - torch.Size([256, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

dn_query_generator.label_embedding.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  
2024/10/31 06:27:03 - mmengine - INFO - Load checkpoint from /mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-t_numbranch_pretrain_31_refcoco.pth
2024/10/31 06:27:03 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2024/10/31 06:27:03 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2024/10/31 06:27:03 - mmengine - INFO - Checkpoints will be saved to /mnt/public/usr/sunzhichao/mmdetection/work_dirs/pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3.
2024/10/31 06:27:59 - mmengine - INFO - Epoch(train) [1][ 50/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:14:11  time: 1.1073  data_time: 0.0127  memory: 9505  grad_norm: 76.6996  loss: 11.0371  loss_cls: 0.6384  loss_bbox: 0.0933  loss_iou: 0.4592  d0.loss_cls: 0.6572  d0.loss_bbox: 0.0787  d0.loss_iou: 0.4615  d1.loss_cls: 0.6824  d1.loss_bbox: 0.0662  d1.loss_iou: 0.4381  d2.loss_cls: 0.6577  d2.loss_bbox: 0.0803  d2.loss_iou: 0.4521  d3.loss_cls: 0.6546  d3.loss_bbox: 0.0859  d3.loss_iou: 0.4561  d4.loss_cls: 0.6370  d4.loss_bbox: 0.0942  d4.loss_iou: 0.4600  enc_loss_cls: 0.6775  enc_loss_bbox: 0.0771  enc_loss_iou: 0.4579  dn_loss_cls: 0.0383  dn_loss_bbox: 0.0431  dn_loss_iou: 0.3302  d0.dn_loss_cls: 0.0747  d0.dn_loss_bbox: 0.0597  d0.dn_loss_iou: 0.4248  d1.dn_loss_cls: 0.0428  d1.dn_loss_bbox: 0.0472  d1.dn_loss_iou: 0.3550  d2.dn_loss_cls: 0.0400  d2.dn_loss_bbox: 0.0438  d2.dn_loss_iou: 0.3345  d3.dn_loss_cls: 0.0386  d3.dn_loss_bbox: 0.0431  d3.dn_loss_iou: 0.3297  d4.dn_loss_cls: 0.0389  d4.dn_loss_bbox: 0.0430  d4.dn_loss_iou: 0.3296  loss_num: 0.0024  d0.loss_num: 0.0024  d1.loss_num: 0.0026  d2.loss_num: 0.0025  d3.loss_num: 0.0024  d4.loss_num: 0.0025
2024/10/31 06:28:50 - mmengine - INFO - Epoch(train) [1][100/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:10:35  time: 1.0266  data_time: 0.0110  memory: 9494  grad_norm: 71.1080  loss: 10.6178  loss_cls: 0.6257  loss_bbox: 0.0690  loss_iou: 0.4322  d0.loss_cls: 0.6548  d0.loss_bbox: 0.0667  d0.loss_iou: 0.4364  d1.loss_cls: 0.6403  d1.loss_bbox: 0.0706  d1.loss_iou: 0.4453  d2.loss_cls: 0.6285  d2.loss_bbox: 0.0709  d2.loss_iou: 0.4444  d3.loss_cls: 0.6202  d3.loss_bbox: 0.0718  d3.loss_iou: 0.4428  d4.loss_cls: 0.6248  d4.loss_bbox: 0.0691  d4.loss_iou: 0.4327  enc_loss_cls: 0.6447  enc_loss_bbox: 0.0702  enc_loss_iou: 0.4499  dn_loss_cls: 0.0324  dn_loss_bbox: 0.0459  dn_loss_iou: 0.3183  d0.dn_loss_cls: 0.0791  d0.dn_loss_bbox: 0.0665  d0.dn_loss_iou: 0.4273  d1.dn_loss_cls: 0.0417  d1.dn_loss_bbox: 0.0495  d1.dn_loss_iou: 0.3384  d2.dn_loss_cls: 0.0346  d2.dn_loss_bbox: 0.0465  d2.dn_loss_iou: 0.3210  d3.dn_loss_cls: 0.0329  d3.dn_loss_bbox: 0.0459  d3.dn_loss_iou: 0.3177  d4.dn_loss_cls: 0.0329  d4.dn_loss_bbox: 0.0460  d4.dn_loss_iou: 0.3178  loss_num: 0.0021  d0.loss_num: 0.0020  d1.loss_num: 0.0020  d2.loss_num: 0.0021  d3.loss_num: 0.0020  d4.loss_num: 0.0021
2024/10/31 06:29:41 - mmengine - INFO - Epoch(train) [1][150/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:08:46  time: 1.0243  data_time: 0.0107  memory: 9499  grad_norm: 75.3906  loss: 10.7369  loss_cls: 0.6487  loss_bbox: 0.0693  loss_iou: 0.4482  d0.loss_cls: 0.6826  d0.loss_bbox: 0.0695  d0.loss_iou: 0.4575  d1.loss_cls: 0.6713  d1.loss_bbox: 0.0705  d1.loss_iou: 0.4555  d2.loss_cls: 0.6636  d2.loss_bbox: 0.0700  d2.loss_iou: 0.4488  d3.loss_cls: 0.6577  d3.loss_bbox: 0.0690  d3.loss_iou: 0.4457  d4.loss_cls: 0.6495  d4.loss_bbox: 0.0688  d4.loss_iou: 0.4443  enc_loss_cls: 0.6799  enc_loss_bbox: 0.0729  enc_loss_iou: 0.4829  dn_loss_cls: 0.0267  dn_loss_bbox: 0.0384  dn_loss_iou: 0.3011  d0.dn_loss_cls: 0.0690  d0.dn_loss_bbox: 0.0573  d0.dn_loss_iou: 0.4105  d1.dn_loss_cls: 0.0331  d1.dn_loss_bbox: 0.0416  d1.dn_loss_iou: 0.3208  d2.dn_loss_cls: 0.0282  d2.dn_loss_bbox: 0.0389  d2.dn_loss_iou: 0.3041  d3.dn_loss_cls: 0.0268  d3.dn_loss_bbox: 0.0384  d3.dn_loss_iou: 0.3001  d4.dn_loss_cls: 0.0264  d4.dn_loss_bbox: 0.0384  d4.dn_loss_iou: 0.3005  loss_num: 0.0018  d0.loss_num: 0.0017  d1.loss_num: 0.0018  d2.loss_num: 0.0018  d3.loss_num: 0.0018  d4.loss_num: 0.0018
2024/10/31 06:30:32 - mmengine - INFO - Epoch(train) [1][200/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:07:22  time: 1.0196  data_time: 0.0109  memory: 9496  grad_norm: 83.5854  loss: 9.4646  loss_cls: 0.5781  loss_bbox: 0.0539  loss_iou: 0.3711  d0.loss_cls: 0.6080  d0.loss_bbox: 0.0560  d0.loss_iou: 0.3910  d1.loss_cls: 0.5906  d1.loss_bbox: 0.0550  d1.loss_iou: 0.3799  d2.loss_cls: 0.5879  d2.loss_bbox: 0.0531  d2.loss_iou: 0.3701  d3.loss_cls: 0.5853  d3.loss_bbox: 0.0523  d3.loss_iou: 0.3687  d4.loss_cls: 0.5802  d4.loss_bbox: 0.0541  d4.loss_iou: 0.3704  enc_loss_cls: 0.6104  enc_loss_bbox: 0.0579  enc_loss_iou: 0.3985  dn_loss_cls: 0.0349  dn_loss_bbox: 0.0365  dn_loss_iou: 0.2775  d0.dn_loss_cls: 0.0723  d0.dn_loss_bbox: 0.0525  d0.dn_loss_iou: 0.3723  d1.dn_loss_cls: 0.0446  d1.dn_loss_bbox: 0.0391  d1.dn_loss_iou: 0.2944  d2.dn_loss_cls: 0.0396  d2.dn_loss_bbox: 0.0372  d2.dn_loss_iou: 0.2803  d3.dn_loss_cls: 0.0369  d3.dn_loss_bbox: 0.0365  d3.dn_loss_iou: 0.2766  d4.dn_loss_cls: 0.0363  d4.dn_loss_bbox: 0.0365  d4.dn_loss_iou: 0.2766  loss_num: 0.0020  d0.loss_num: 0.0020  d1.loss_num: 0.0018  d2.loss_num: 0.0019  d3.loss_num: 0.0019  d4.loss_num: 0.0019
2024/10/31 06:31:24 - mmengine - INFO - Epoch(train) [1][250/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:06:20  time: 1.0321  data_time: 0.0107  memory: 9511  grad_norm: 56.8243  loss: 9.0421  loss_cls: 0.5171  loss_bbox: 0.0557  loss_iou: 0.3653  d0.loss_cls: 0.5397  d0.loss_bbox: 0.0575  d0.loss_iou: 0.3758  d1.loss_cls: 0.5311  d1.loss_bbox: 0.0575  d1.loss_iou: 0.3702  d2.loss_cls: 0.5223  d2.loss_bbox: 0.0568  d2.loss_iou: 0.3675  d3.loss_cls: 0.5207  d3.loss_bbox: 0.0560  d3.loss_iou: 0.3664  d4.loss_cls: 0.5186  d4.loss_bbox: 0.0557  d4.loss_iou: 0.3654  enc_loss_cls: 0.5387  enc_loss_bbox: 0.0619  enc_loss_iou: 0.4026  dn_loss_cls: 0.0084  dn_loss_bbox: 0.0407  dn_loss_iou: 0.3057  d0.dn_loss_cls: 0.0487  d0.dn_loss_bbox: 0.0599  d0.dn_loss_iou: 0.4123  d1.dn_loss_cls: 0.0146  d1.dn_loss_bbox: 0.0442  d1.dn_loss_iou: 0.3255  d2.dn_loss_cls: 0.0099  d2.dn_loss_bbox: 0.0412  d2.dn_loss_iou: 0.3085  d3.dn_loss_cls: 0.0091  d3.dn_loss_bbox: 0.0407  d3.dn_loss_iou: 0.3052  d4.dn_loss_cls: 0.0085  d4.dn_loss_bbox: 0.0407  d4.dn_loss_iou: 0.3056  loss_num: 0.0017  d0.loss_num: 0.0015  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0017
2024/10/31 06:32:15 - mmengine - INFO - Epoch(train) [1][300/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:05:14  time: 1.0205  data_time: 0.0100  memory: 9511  grad_norm: 60.5941  loss: 11.0910  loss_cls: 0.6387  loss_bbox: 0.0555  loss_iou: 0.4242  d0.loss_cls: 0.6750  d0.loss_bbox: 0.0599  d0.loss_iou: 0.4484  d1.loss_cls: 0.6630  d1.loss_bbox: 0.0576  d1.loss_iou: 0.4379  d2.loss_cls: 0.6471  d2.loss_bbox: 0.0579  d2.loss_iou: 0.4385  d3.loss_cls: 0.6451  d3.loss_bbox: 0.0562  d3.loss_iou: 0.4297  d4.loss_cls: 0.6365  d4.loss_bbox: 0.0555  d4.loss_iou: 0.4268  enc_loss_cls: 0.6720  enc_loss_bbox: 0.0647  enc_loss_iou: 0.4767  dn_loss_cls: 0.1452  dn_loss_bbox: 0.0375  dn_loss_iou: 0.2987  d0.dn_loss_cls: 0.1574  d0.dn_loss_bbox: 0.0548  d0.dn_loss_iou: 0.4012  d1.dn_loss_cls: 0.1367  d1.dn_loss_bbox: 0.0405  d1.dn_loss_iou: 0.3179  d2.dn_loss_cls: 0.1348  d2.dn_loss_bbox: 0.0378  d2.dn_loss_iou: 0.3001  d3.dn_loss_cls: 0.1377  d3.dn_loss_bbox: 0.0375  d3.dn_loss_iou: 0.2983  d4.dn_loss_cls: 0.1420  d4.dn_loss_bbox: 0.0375  d4.dn_loss_iou: 0.2984  loss_num: 0.0018  d0.loss_num: 0.0016  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0016
2024/10/31 06:33:06 - mmengine - INFO - Epoch(train) [1][350/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:04:14  time: 1.0235  data_time: 0.0105  memory: 9499  grad_norm: 68.6622  loss: 10.1119  loss_cls: 0.5951  loss_bbox: 0.0548  loss_iou: 0.4498  d0.loss_cls: 0.6113  d0.loss_bbox: 0.0613  d0.loss_iou: 0.4774  d1.loss_cls: 0.6095  d1.loss_bbox: 0.0594  d1.loss_iou: 0.4608  d2.loss_cls: 0.6025  d2.loss_bbox: 0.0554  d2.loss_iou: 0.4497  d3.loss_cls: 0.5964  d3.loss_bbox: 0.0552  d3.loss_iou: 0.4494  d4.loss_cls: 0.5920  d4.loss_bbox: 0.0555  d4.loss_iou: 0.4510  enc_loss_cls: 0.6204  enc_loss_bbox: 0.0616  enc_loss_iou: 0.4895  dn_loss_cls: 0.0217  dn_loss_bbox: 0.0356  dn_loss_iou: 0.2886  d0.dn_loss_cls: 0.0520  d0.dn_loss_bbox: 0.0515  d0.dn_loss_iou: 0.3859  d1.dn_loss_cls: 0.0264  d1.dn_loss_bbox: 0.0385  d1.dn_loss_iou: 0.3068  d2.dn_loss_cls: 0.0212  d2.dn_loss_bbox: 0.0360  d2.dn_loss_iou: 0.2912  d3.dn_loss_cls: 0.0211  d3.dn_loss_bbox: 0.0357  d3.dn_loss_iou: 0.2880  d4.dn_loss_cls: 0.0212  d4.dn_loss_bbox: 0.0356  d4.dn_loss_iou: 0.2877  loss_num: 0.0017  d0.loss_num: 0.0015  d1.loss_num: 0.0015  d2.loss_num: 0.0015  d3.loss_num: 0.0016  d4.loss_num: 0.0016
2024/10/31 06:33:58 - mmengine - INFO - Epoch(train) [1][400/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:03:19  time: 1.0291  data_time: 0.0109  memory: 9511  grad_norm: 65.2827  loss: 10.6170  loss_cls: 0.5903  loss_bbox: 0.0691  loss_iou: 0.4936  d0.loss_cls: 0.6263  d0.loss_bbox: 0.0699  d0.loss_iou: 0.4984  d1.loss_cls: 0.6131  d1.loss_bbox: 0.0689  d1.loss_iou: 0.4986  d2.loss_cls: 0.6029  d2.loss_bbox: 0.0675  d2.loss_iou: 0.4934  d3.loss_cls: 0.6032  d3.loss_bbox: 0.0656  d3.loss_iou: 0.4840  d4.loss_cls: 0.5952  d4.loss_bbox: 0.0663  d4.loss_iou: 0.4861  enc_loss_cls: 0.6275  enc_loss_bbox: 0.0705  enc_loss_iou: 0.5196  dn_loss_cls: 0.0202  dn_loss_bbox: 0.0355  dn_loss_iou: 0.3132  d0.dn_loss_cls: 0.0607  d0.dn_loss_bbox: 0.0517  d0.dn_loss_iou: 0.4112  d1.dn_loss_cls: 0.0263  d1.dn_loss_bbox: 0.0380  d1.dn_loss_iou: 0.3307  d2.dn_loss_cls: 0.0216  d2.dn_loss_bbox: 0.0357  d2.dn_loss_iou: 0.3145  d3.dn_loss_cls: 0.0211  d3.dn_loss_bbox: 0.0354  d3.dn_loss_iou: 0.3124  d4.dn_loss_cls: 0.0202  d4.dn_loss_bbox: 0.0355  d4.dn_loss_iou: 0.3128  loss_num: 0.0018  d0.loss_num: 0.0018  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0017
2024/10/31 06:34:49 - mmengine - INFO - Epoch(train) [1][450/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:02:24  time: 1.0266  data_time: 0.0104  memory: 9505  grad_norm: 62.3461  loss: 8.1782  loss_cls: 0.4545  loss_bbox: 0.0460  loss_iou: 0.3206  d0.loss_cls: 0.4879  d0.loss_bbox: 0.0482  d0.loss_iou: 0.3354  d1.loss_cls: 0.4623  d1.loss_bbox: 0.0499  d1.loss_iou: 0.3411  d2.loss_cls: 0.4607  d2.loss_bbox: 0.0467  d2.loss_iou: 0.3263  d3.loss_cls: 0.4571  d3.loss_bbox: 0.0468  d3.loss_iou: 0.3266  d4.loss_cls: 0.4555  d4.loss_bbox: 0.0458  d4.loss_iou: 0.3202  enc_loss_cls: 0.4854  enc_loss_bbox: 0.0543  enc_loss_iou: 0.3767  dn_loss_cls: 0.0273  dn_loss_bbox: 0.0357  dn_loss_iou: 0.2769  d0.dn_loss_cls: 0.0558  d0.dn_loss_bbox: 0.0531  d0.dn_loss_iou: 0.3834  d1.dn_loss_cls: 0.0303  d1.dn_loss_bbox: 0.0385  d1.dn_loss_iou: 0.2968  d2.dn_loss_cls: 0.0272  d2.dn_loss_bbox: 0.0362  d2.dn_loss_iou: 0.2801  d3.dn_loss_cls: 0.0269  d3.dn_loss_bbox: 0.0356  d3.dn_loss_iou: 0.2767  d4.dn_loss_cls: 0.0276  d4.dn_loss_bbox: 0.0356  d4.dn_loss_iou: 0.2768  loss_num: 0.0016  d0.loss_num: 0.0016  d1.loss_num: 0.0016  d2.loss_num: 0.0016  d3.loss_num: 0.0016  d4.loss_num: 0.0016
2024/10/31 06:35:41 - mmengine - INFO - Epoch(train) [1][500/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:01:33  time: 1.0358  data_time: 0.0110  memory: 9505  grad_norm: 63.7554  loss: 9.5681  loss_cls: 0.5467  loss_bbox: 0.0522  loss_iou: 0.3936  d0.loss_cls: 0.5580  d0.loss_bbox: 0.0600  d0.loss_iou: 0.4296  d1.loss_cls: 0.5527  d1.loss_bbox: 0.0574  d1.loss_iou: 0.4116  d2.loss_cls: 0.5494  d2.loss_bbox: 0.0554  d2.loss_iou: 0.3986  d3.loss_cls: 0.5382  d3.loss_bbox: 0.0555  d3.loss_iou: 0.4009  d4.loss_cls: 0.5454  d4.loss_bbox: 0.0524  d4.loss_iou: 0.3944  enc_loss_cls: 0.5580  enc_loss_bbox: 0.0644  enc_loss_iou: 0.4449  dn_loss_cls: 0.0207  dn_loss_bbox: 0.0386  dn_loss_iou: 0.3090  d0.dn_loss_cls: 0.0674  d0.dn_loss_bbox: 0.0576  d0.dn_loss_iou: 0.4285  d1.dn_loss_cls: 0.0309  d1.dn_loss_bbox: 0.0417  d1.dn_loss_iou: 0.3305  d2.dn_loss_cls: 0.0247  d2.dn_loss_bbox: 0.0392  d2.dn_loss_iou: 0.3132  d3.dn_loss_cls: 0.0223  d3.dn_loss_bbox: 0.0386  d3.dn_loss_iou: 0.3087  d4.dn_loss_cls: 0.0208  d4.dn_loss_bbox: 0.0385  d4.dn_loss_iou: 0.3085  loss_num: 0.0016  d0.loss_num: 0.0015  d1.loss_num: 0.0016  d2.loss_num: 0.0016  d3.loss_num: 0.0016  d4.loss_num: 0.0016
2024/10/31 06:36:32 - mmengine - INFO - Epoch(train) [1][550/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 1:00:39  time: 1.0281  data_time: 0.0106  memory: 9499  grad_norm: 73.4636  loss: 10.4466  loss_cls: 0.5609  loss_bbox: 0.0773  loss_iou: 0.5041  d0.loss_cls: 0.5913  d0.loss_bbox: 0.0775  d0.loss_iou: 0.5153  d1.loss_cls: 0.5730  d1.loss_bbox: 0.0811  d1.loss_iou: 0.5208  d2.loss_cls: 0.5626  d2.loss_bbox: 0.0795  d2.loss_iou: 0.5110  d3.loss_cls: 0.5617  d3.loss_bbox: 0.0801  d3.loss_iou: 0.5069  d4.loss_cls: 0.5558  d4.loss_bbox: 0.0813  d4.loss_iou: 0.5095  enc_loss_cls: 0.5904  enc_loss_bbox: 0.0875  enc_loss_iou: 0.5551  dn_loss_cls: 0.0208  dn_loss_bbox: 0.0330  dn_loss_iou: 0.2904  d0.dn_loss_cls: 0.0562  d0.dn_loss_bbox: 0.0503  d0.dn_loss_iou: 0.3950  d1.dn_loss_cls: 0.0268  d1.dn_loss_bbox: 0.0363  d1.dn_loss_iou: 0.3116  d2.dn_loss_cls: 0.0216  d2.dn_loss_bbox: 0.0337  d2.dn_loss_iou: 0.2926  d3.dn_loss_cls: 0.0210  d3.dn_loss_bbox: 0.0330  d3.dn_loss_iou: 0.2892  d4.dn_loss_cls: 0.0207  d4.dn_loss_bbox: 0.0330  d4.dn_loss_iou: 0.2895  loss_num: 0.0016  d0.loss_num: 0.0015  d1.loss_num: 0.0015  d2.loss_num: 0.0016  d3.loss_num: 0.0015  d4.loss_num: 0.0015
2024/10/31 06:37:24 - mmengine - INFO - Epoch(train) [1][600/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:59:46  time: 1.0297  data_time: 0.0106  memory: 9505  grad_norm: 62.7671  loss: 8.8485  loss_cls: 0.5185  loss_bbox: 0.0530  loss_iou: 0.3542  d0.loss_cls: 0.5417  d0.loss_bbox: 0.0566  d0.loss_iou: 0.3638  d1.loss_cls: 0.5358  d1.loss_bbox: 0.0537  d1.loss_iou: 0.3613  d2.loss_cls: 0.5197  d2.loss_bbox: 0.0532  d2.loss_iou: 0.3553  d3.loss_cls: 0.5132  d3.loss_bbox: 0.0524  d3.loss_iou: 0.3528  d4.loss_cls: 0.5137  d4.loss_bbox: 0.0524  d4.loss_iou: 0.3520  enc_loss_cls: 0.5477  enc_loss_bbox: 0.0559  enc_loss_iou: 0.3744  dn_loss_cls: 0.0124  dn_loss_bbox: 0.0397  dn_loss_iou: 0.2922  d0.dn_loss_cls: 0.0523  d0.dn_loss_bbox: 0.0582  d0.dn_loss_iou: 0.3915  d1.dn_loss_cls: 0.0197  d1.dn_loss_bbox: 0.0431  d1.dn_loss_iou: 0.3096  d2.dn_loss_cls: 0.0151  d2.dn_loss_bbox: 0.0402  d2.dn_loss_iou: 0.2943  d3.dn_loss_cls: 0.0135  d3.dn_loss_bbox: 0.0396  d3.dn_loss_iou: 0.2918  d4.dn_loss_cls: 0.0122  d4.dn_loss_bbox: 0.0396  d4.dn_loss_iou: 0.2915  loss_num: 0.0018  d0.loss_num: 0.0018  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0018  d4.loss_num: 0.0018
2024/10/31 06:38:15 - mmengine - INFO - Epoch(train) [1][650/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:58:56  time: 1.0381  data_time: 0.0114  memory: 9508  grad_norm: 64.3054  loss: 8.9355  loss_cls: 0.4873  loss_bbox: 0.0490  loss_iou: 0.3948  d0.loss_cls: 0.5136  d0.loss_bbox: 0.0529  d0.loss_iou: 0.4142  d1.loss_cls: 0.4943  d1.loss_bbox: 0.0524  d1.loss_iou: 0.4145  d2.loss_cls: 0.4940  d2.loss_bbox: 0.0493  d2.loss_iou: 0.3981  d3.loss_cls: 0.4910  d3.loss_bbox: 0.0489  d3.loss_iou: 0.3943  d4.loss_cls: 0.4835  d4.loss_bbox: 0.0497  d4.loss_iou: 0.3997  enc_loss_cls: 0.5223  enc_loss_bbox: 0.0548  enc_loss_iou: 0.4326  dn_loss_cls: 0.0118  dn_loss_bbox: 0.0349  dn_loss_iou: 0.2959  d0.dn_loss_cls: 0.0409  d0.dn_loss_bbox: 0.0524  d0.dn_loss_iou: 0.3978  d1.dn_loss_cls: 0.0165  d1.dn_loss_bbox: 0.0376  d1.dn_loss_iou: 0.3151  d2.dn_loss_cls: 0.0135  d2.dn_loss_bbox: 0.0354  d2.dn_loss_iou: 0.2981  d3.dn_loss_cls: 0.0126  d3.dn_loss_bbox: 0.0350  d3.dn_loss_iou: 0.2952  d4.dn_loss_cls: 0.0118  d4.dn_loss_bbox: 0.0349  d4.dn_loss_iou: 0.2951  loss_num: 0.0017  d0.loss_num: 0.0017  d1.loss_num: 0.0016  d2.loss_num: 0.0017  d3.loss_num: 0.0016  d4.loss_num: 0.0016
2024/10/31 06:39:06 - mmengine - INFO - Epoch(train) [1][700/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:58:00  time: 1.0193  data_time: 0.0113  memory: 9508  grad_norm: 60.7202  loss: 10.5078  loss_cls: 0.5881  loss_bbox: 0.0574  loss_iou: 0.4266  d0.loss_cls: 0.5978  d0.loss_bbox: 0.0701  d0.loss_iou: 0.4637  d1.loss_cls: 0.6012  d1.loss_bbox: 0.0614  d1.loss_iou: 0.4500  d2.loss_cls: 0.5985  d2.loss_bbox: 0.0590  d2.loss_iou: 0.4334  d3.loss_cls: 0.5935  d3.loss_bbox: 0.0578  d3.loss_iou: 0.4241  d4.loss_cls: 0.5835  d4.loss_bbox: 0.0586  d4.loss_iou: 0.4298  enc_loss_cls: 0.6265  enc_loss_bbox: 0.0657  enc_loss_iou: 0.4579  dn_loss_cls: 0.0855  dn_loss_bbox: 0.0384  dn_loss_iou: 0.3101  d0.dn_loss_cls: 0.1197  d0.dn_loss_bbox: 0.0552  d0.dn_loss_iou: 0.4198  d1.dn_loss_cls: 0.0910  d1.dn_loss_bbox: 0.0410  d1.dn_loss_iou: 0.3290  d2.dn_loss_cls: 0.0883  d2.dn_loss_bbox: 0.0387  d2.dn_loss_iou: 0.3118  d3.dn_loss_cls: 0.0850  d3.dn_loss_bbox: 0.0383  d3.dn_loss_iou: 0.3095  d4.dn_loss_cls: 0.0840  d4.dn_loss_bbox: 0.0383  d4.dn_loss_iou: 0.3094  loss_num: 0.0017  d0.loss_num: 0.0017  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0017
2024/10/31 06:39:58 - mmengine - INFO - Epoch(train) [1][750/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:57:09  time: 1.0360  data_time: 0.0109  memory: 9508  grad_norm: 61.4411  loss: 9.0951  loss_cls: 0.5199  loss_bbox: 0.0507  loss_iou: 0.3883  d0.loss_cls: 0.5444  d0.loss_bbox: 0.0523  d0.loss_iou: 0.3992  d1.loss_cls: 0.5280  d1.loss_bbox: 0.0531  d1.loss_iou: 0.4016  d2.loss_cls: 0.5153  d2.loss_bbox: 0.0515  d2.loss_iou: 0.3934  d3.loss_cls: 0.5155  d3.loss_bbox: 0.0509  d3.loss_iou: 0.3906  d4.loss_cls: 0.5176  d4.loss_bbox: 0.0508  d4.loss_iou: 0.3904  enc_loss_cls: 0.5427  enc_loss_bbox: 0.0588  enc_loss_iou: 0.4321  dn_loss_cls: 0.0279  dn_loss_bbox: 0.0361  dn_loss_iou: 0.2771  d0.dn_loss_cls: 0.0666  d0.dn_loss_bbox: 0.0543  d0.dn_loss_iou: 0.3787  d1.dn_loss_cls: 0.0343  d1.dn_loss_bbox: 0.0388  d1.dn_loss_iou: 0.2947  d2.dn_loss_cls: 0.0291  d2.dn_loss_bbox: 0.0366  d2.dn_loss_iou: 0.2802  d3.dn_loss_cls: 0.0285  d3.dn_loss_bbox: 0.0361  d3.dn_loss_iou: 0.2764  d4.dn_loss_cls: 0.0282  d4.dn_loss_bbox: 0.0361  d4.dn_loss_iou: 0.2764  loss_num: 0.0020  d0.loss_num: 0.0021  d1.loss_num: 0.0020  d2.loss_num: 0.0020  d3.loss_num: 0.0019  d4.loss_num: 0.0019
2024/10/31 06:40:49 - mmengine - INFO - Epoch(train) [1][800/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:56:15  time: 1.0207  data_time: 0.0108  memory: 9508  grad_norm: 63.2735  loss: 9.2393  loss_cls: 0.4949  loss_bbox: 0.0626  loss_iou: 0.3661  d0.loss_cls: 0.5327  d0.loss_bbox: 0.0606  d0.loss_iou: 0.3750  d1.loss_cls: 0.5146  d1.loss_bbox: 0.0631  d1.loss_iou: 0.3796  d2.loss_cls: 0.5007  d2.loss_bbox: 0.0641  d2.loss_iou: 0.3764  d3.loss_cls: 0.4939  d3.loss_bbox: 0.0632  d3.loss_iou: 0.3737  d4.loss_cls: 0.4931  d4.loss_bbox: 0.0613  d4.loss_iou: 0.3660  enc_loss_cls: 0.5389  enc_loss_bbox: 0.0683  enc_loss_iou: 0.3973  dn_loss_cls: 0.0477  dn_loss_bbox: 0.0396  dn_loss_iou: 0.3099  d0.dn_loss_cls: 0.0818  d0.dn_loss_bbox: 0.0610  d0.dn_loss_iou: 0.4290  d1.dn_loss_cls: 0.0495  d1.dn_loss_bbox: 0.0436  d1.dn_loss_iou: 0.3332  d2.dn_loss_cls: 0.0475  d2.dn_loss_bbox: 0.0401  d2.dn_loss_iou: 0.3126  d3.dn_loss_cls: 0.0465  d3.dn_loss_bbox: 0.0395  d3.dn_loss_iou: 0.3084  d4.dn_loss_cls: 0.0463  d4.dn_loss_bbox: 0.0395  d4.dn_loss_iou: 0.3088  loss_num: 0.0015  d0.loss_num: 0.0014  d1.loss_num: 0.0015  d2.loss_num: 0.0015  d3.loss_num: 0.0015  d4.loss_num: 0.0014
2024/10/31 06:41:04 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 06:41:04 - mmengine - INFO - Saving checkpoint at 1 epochs
2024/10/31 06:41:15 - mmengine - INFO - Epoch(val) [1][ 50/858]    eta: 0:01:15  time: 0.0931  data_time: 0.0058  memory: 9505  
2024/10/31 06:41:19 - mmengine - INFO - Epoch(val) [1][100/858]    eta: 0:01:07  time: 0.0862  data_time: 0.0019  memory: 3167  
2024/10/31 06:41:24 - mmengine - INFO - Epoch(val) [1][150/858]    eta: 0:01:02  time: 0.0866  data_time: 0.0019  memory: 3164  
2024/10/31 06:41:28 - mmengine - INFO - Epoch(val) [1][200/858]    eta: 0:00:58  time: 0.0872  data_time: 0.0019  memory: 3170  
2024/10/31 06:41:32 - mmengine - INFO - Epoch(val) [1][250/858]    eta: 0:00:53  time: 0.0866  data_time: 0.0019  memory: 3170  
2024/10/31 06:41:37 - mmengine - INFO - Epoch(val) [1][300/858]    eta: 0:00:48  time: 0.0864  data_time: 0.0019  memory: 3170  
2024/10/31 06:41:41 - mmengine - INFO - Epoch(val) [1][350/858]    eta: 0:00:44  time: 0.0861  data_time: 0.0019  memory: 3167  
2024/10/31 06:41:45 - mmengine - INFO - Epoch(val) [1][400/858]    eta: 0:00:40  time: 0.0874  data_time: 0.0020  memory: 3173  
2024/10/31 06:41:50 - mmengine - INFO - Epoch(val) [1][450/858]    eta: 0:00:35  time: 0.0877  data_time: 0.0019  memory: 3170  
2024/10/31 06:41:54 - mmengine - INFO - Epoch(val) [1][500/858]    eta: 0:00:31  time: 0.0871  data_time: 0.0019  memory: 3169  
2024/10/31 06:41:59 - mmengine - INFO - Epoch(val) [1][550/858]    eta: 0:00:26  time: 0.0866  data_time: 0.0019  memory: 3170  
2024/10/31 06:42:03 - mmengine - INFO - Epoch(val) [1][600/858]    eta: 0:00:22  time: 0.0868  data_time: 0.0021  memory: 3176  
2024/10/31 06:42:07 - mmengine - INFO - Epoch(val) [1][650/858]    eta: 0:00:18  time: 0.0862  data_time: 0.0019  memory: 3170  
2024/10/31 06:42:11 - mmengine - INFO - Epoch(val) [1][700/858]    eta: 0:00:13  time: 0.0863  data_time: 0.0019  memory: 3170  
2024/10/31 06:42:16 - mmengine - INFO - Epoch(val) [1][750/858]    eta: 0:00:09  time: 0.0863  data_time: 0.0019  memory: 3167  
2024/10/31 06:42:20 - mmengine - INFO - Epoch(val) [1][800/858]    eta: 0:00:05  time: 0.0863  data_time: 0.0019  memory: 3167  
2024/10/31 06:42:24 - mmengine - INFO - Epoch(val) [1][850/858]    eta: 0:00:00  time: 0.0858  data_time: 0.0020  memory: 3170  
2024/10/31 06:42:27 - mmengine - INFO - {'instance_F1_score': 0.04145502212026872, 'instance_acc': 0.033616915833815146, 'image_F1_score': 0.10410641989589357, 'image_acc': 0.09731934731934731}
2024/10/31 06:42:27 - mmengine - INFO - Epoch(val) [1][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.0415  grefcoco_val/refdrone/instance_acc: 0.0336  grefcoco_val/refdrone/image_F1_score: 0.1041  grefcoco_val/refdrone/image_acc: 0.0973  data_time: 0.0022  time: 0.0870
2024/10/31 06:43:19 - mmengine - INFO - Epoch(train) [2][ 50/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:55:10  time: 1.0364  data_time: 0.0094  memory: 9502  grad_norm: 58.8656  loss: 9.8129  loss_cls: 0.5185  loss_bbox: 0.0594  loss_iou: 0.4562  d0.loss_cls: 0.5547  d0.loss_bbox: 0.0609  d0.loss_iou: 0.4638  d1.loss_cls: 0.5359  d1.loss_bbox: 0.0617  d1.loss_iou: 0.4600  d2.loss_cls: 0.5307  d2.loss_bbox: 0.0607  d2.loss_iou: 0.4584  d3.loss_cls: 0.5223  d3.loss_bbox: 0.0594  d3.loss_iou: 0.4550  d4.loss_cls: 0.5192  d4.loss_bbox: 0.0594  d4.loss_iou: 0.4558  enc_loss_cls: 0.5651  enc_loss_bbox: 0.0637  enc_loss_iou: 0.4887  dn_loss_cls: 0.0442  dn_loss_bbox: 0.0348  dn_loss_iou: 0.2929  d0.dn_loss_cls: 0.0683  d0.dn_loss_bbox: 0.0508  d0.dn_loss_iou: 0.3911  d1.dn_loss_cls: 0.0489  d1.dn_loss_bbox: 0.0375  d1.dn_loss_iou: 0.3098  d2.dn_loss_cls: 0.0434  d2.dn_loss_bbox: 0.0354  d2.dn_loss_iou: 0.2967  d3.dn_loss_cls: 0.0439  d3.dn_loss_bbox: 0.0348  d3.dn_loss_iou: 0.2927  d4.dn_loss_cls: 0.0442  d4.dn_loss_bbox: 0.0348  d4.dn_loss_iou: 0.2923  loss_num: 0.0012  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0012
2024/10/31 06:44:10 - mmengine - INFO - Epoch(train) [2][100/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:54:19  time: 1.0329  data_time: 0.0104  memory: 9496  grad_norm: 60.3503  loss: 9.3096  loss_cls: 0.4794  loss_bbox: 0.0684  loss_iou: 0.4370  d0.loss_cls: 0.5074  d0.loss_bbox: 0.0727  d0.loss_iou: 0.4650  d1.loss_cls: 0.4989  d1.loss_bbox: 0.0703  d1.loss_iou: 0.4506  d2.loss_cls: 0.4863  d2.loss_bbox: 0.0710  d2.loss_iou: 0.4491  d3.loss_cls: 0.4849  d3.loss_bbox: 0.0692  d3.loss_iou: 0.4406  d4.loss_cls: 0.4785  d4.loss_bbox: 0.0692  d4.loss_iou: 0.4423  enc_loss_cls: 0.5203  enc_loss_bbox: 0.0763  enc_loss_iou: 0.4909  dn_loss_cls: 0.0181  dn_loss_bbox: 0.0350  dn_loss_iou: 0.2821  d0.dn_loss_cls: 0.0493  d0.dn_loss_bbox: 0.0502  d0.dn_loss_iou: 0.3706  d1.dn_loss_cls: 0.0234  d1.dn_loss_bbox: 0.0376  d1.dn_loss_iou: 0.2974  d2.dn_loss_cls: 0.0196  d2.dn_loss_bbox: 0.0357  d2.dn_loss_iou: 0.2845  d3.dn_loss_cls: 0.0186  d3.dn_loss_bbox: 0.0349  d3.dn_loss_iou: 0.2809  d4.dn_loss_cls: 0.0181  d4.dn_loss_bbox: 0.0350  d4.dn_loss_iou: 0.2813  loss_num: 0.0015  d0.loss_num: 0.0015  d1.loss_num: 0.0015  d2.loss_num: 0.0015  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2024/10/31 06:45:02 - mmengine - INFO - Epoch(train) [2][150/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:53:28  time: 1.0379  data_time: 0.0110  memory: 9502  grad_norm: 66.2402  loss: 9.0228  loss_cls: 0.4960  loss_bbox: 0.0663  loss_iou: 0.4229  d0.loss_cls: 0.5247  d0.loss_bbox: 0.0685  d0.loss_iou: 0.4397  d1.loss_cls: 0.5117  d1.loss_bbox: 0.0671  d1.loss_iou: 0.4281  d2.loss_cls: 0.5027  d2.loss_bbox: 0.0670  d2.loss_iou: 0.4187  d3.loss_cls: 0.4966  d3.loss_bbox: 0.0646  d3.loss_iou: 0.4176  d4.loss_cls: 0.4958  d4.loss_bbox: 0.0648  d4.loss_iou: 0.4201  enc_loss_cls: 0.5183  enc_loss_bbox: 0.0755  enc_loss_iou: 0.4578  dn_loss_cls: 0.0117  dn_loss_bbox: 0.0338  dn_loss_iou: 0.2612  d0.dn_loss_cls: 0.0391  d0.dn_loss_bbox: 0.0475  d0.dn_loss_iou: 0.3479  d1.dn_loss_cls: 0.0162  d1.dn_loss_bbox: 0.0360  d1.dn_loss_iou: 0.2746  d2.dn_loss_cls: 0.0129  d2.dn_loss_bbox: 0.0342  d2.dn_loss_iou: 0.2631  d3.dn_loss_cls: 0.0122  d3.dn_loss_bbox: 0.0338  d3.dn_loss_iou: 0.2607  d4.dn_loss_cls: 0.0120  d4.dn_loss_bbox: 0.0338  d4.dn_loss_iou: 0.2606  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/31 06:45:39 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 06:45:53 - mmengine - INFO - Epoch(train) [2][200/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:52:35  time: 1.0236  data_time: 0.0104  memory: 9499  grad_norm: 59.2246  loss: 8.4802  loss_cls: 0.4697  loss_bbox: 0.0513  loss_iou: 0.3844  d0.loss_cls: 0.4927  d0.loss_bbox: 0.0516  d0.loss_iou: 0.3876  d1.loss_cls: 0.4798  d1.loss_bbox: 0.0519  d1.loss_iou: 0.3893  d2.loss_cls: 0.4794  d2.loss_bbox: 0.0524  d2.loss_iou: 0.3887  d3.loss_cls: 0.4816  d3.loss_bbox: 0.0498  d3.loss_iou: 0.3785  d4.loss_cls: 0.4709  d4.loss_bbox: 0.0501  d4.loss_iou: 0.3796  enc_loss_cls: 0.5146  enc_loss_bbox: 0.0554  enc_loss_iou: 0.4063  dn_loss_cls: 0.0134  dn_loss_bbox: 0.0329  dn_loss_iou: 0.2607  d0.dn_loss_cls: 0.0443  d0.dn_loss_bbox: 0.0468  d0.dn_loss_iou: 0.3541  d1.dn_loss_cls: 0.0183  d1.dn_loss_bbox: 0.0348  d1.dn_loss_iou: 0.2765  d2.dn_loss_cls: 0.0150  d2.dn_loss_bbox: 0.0332  d2.dn_loss_iou: 0.2638  d3.dn_loss_cls: 0.0131  d3.dn_loss_bbox: 0.0328  d3.dn_loss_iou: 0.2604  d4.dn_loss_cls: 0.0127  d4.dn_loss_bbox: 0.0328  d4.dn_loss_iou: 0.2603  loss_num: 0.0014  d0.loss_num: 0.0014  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2024/10/31 06:46:45 - mmengine - INFO - Epoch(train) [2][250/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:51:43  time: 1.0312  data_time: 0.0118  memory: 9514  grad_norm: 63.7682  loss: 9.8045  loss_cls: 0.5373  loss_bbox: 0.0585  loss_iou: 0.4773  d0.loss_cls: 0.5517  d0.loss_bbox: 0.0655  d0.loss_iou: 0.5067  d1.loss_cls: 0.5407  d1.loss_bbox: 0.0632  d1.loss_iou: 0.4956  d2.loss_cls: 0.5422  d2.loss_bbox: 0.0600  d2.loss_iou: 0.4872  d3.loss_cls: 0.5394  d3.loss_bbox: 0.0596  d3.loss_iou: 0.4794  d4.loss_cls: 0.5377  d4.loss_bbox: 0.0573  d4.loss_iou: 0.4723  enc_loss_cls: 0.5624  enc_loss_bbox: 0.0670  enc_loss_iou: 0.5217  dn_loss_cls: 0.0134  dn_loss_bbox: 0.0329  dn_loss_iou: 0.2748  d0.dn_loss_cls: 0.0511  d0.dn_loss_bbox: 0.0489  d0.dn_loss_iou: 0.3757  d1.dn_loss_cls: 0.0198  d1.dn_loss_bbox: 0.0356  d1.dn_loss_iou: 0.2937  d2.dn_loss_cls: 0.0149  d2.dn_loss_bbox: 0.0332  d2.dn_loss_iou: 0.2771  d3.dn_loss_cls: 0.0142  d3.dn_loss_bbox: 0.0329  d3.dn_loss_iou: 0.2747  d4.dn_loss_cls: 0.0135  d4.dn_loss_bbox: 0.0328  d4.dn_loss_iou: 0.2743  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/31 06:47:36 - mmengine - INFO - Epoch(train) [2][300/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:50:51  time: 1.0290  data_time: 0.0103  memory: 9502  grad_norm: 62.3697  loss: 8.7004  loss_cls: 0.4651  loss_bbox: 0.0535  loss_iou: 0.3858  d0.loss_cls: 0.4933  d0.loss_bbox: 0.0565  d0.loss_iou: 0.4026  d1.loss_cls: 0.4747  d1.loss_bbox: 0.0569  d1.loss_iou: 0.3998  d2.loss_cls: 0.4674  d2.loss_bbox: 0.0582  d2.loss_iou: 0.3981  d3.loss_cls: 0.4668  d3.loss_bbox: 0.0556  d3.loss_iou: 0.3914  d4.loss_cls: 0.4642  d4.loss_bbox: 0.0547  d4.loss_iou: 0.3876  enc_loss_cls: 0.5017  enc_loss_bbox: 0.0643  enc_loss_iou: 0.4287  dn_loss_cls: 0.0165  dn_loss_bbox: 0.0367  dn_loss_iou: 0.2795  d0.dn_loss_cls: 0.0452  d0.dn_loss_bbox: 0.0527  d0.dn_loss_iou: 0.3772  d1.dn_loss_cls: 0.0218  d1.dn_loss_bbox: 0.0388  d1.dn_loss_iou: 0.2951  d2.dn_loss_cls: 0.0176  d2.dn_loss_bbox: 0.0369  d2.dn_loss_iou: 0.2819  d3.dn_loss_cls: 0.0168  d3.dn_loss_bbox: 0.0367  d3.dn_loss_iou: 0.2797  d4.dn_loss_cls: 0.0167  d4.dn_loss_bbox: 0.0367  d4.dn_loss_iou: 0.2792  loss_num: 0.0013  d0.loss_num: 0.0012  d1.loss_num: 0.0014  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/31 06:48:28 - mmengine - INFO - Epoch(train) [2][350/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:49:58  time: 1.0263  data_time: 0.0102  memory: 9520  grad_norm: 76.1429  loss: 9.8517  loss_cls: 0.5159  loss_bbox: 0.0598  loss_iou: 0.4991  d0.loss_cls: 0.5591  d0.loss_bbox: 0.0649  d0.loss_iou: 0.5259  d1.loss_cls: 0.5354  d1.loss_bbox: 0.0645  d1.loss_iou: 0.5007  d2.loss_cls: 0.5221  d2.loss_bbox: 0.0598  d2.loss_iou: 0.4989  d3.loss_cls: 0.5191  d3.loss_bbox: 0.0598  d3.loss_iou: 0.4976  d4.loss_cls: 0.5183  d4.loss_bbox: 0.0578  d4.loss_iou: 0.4939  enc_loss_cls: 0.5556  enc_loss_bbox: 0.0745  enc_loss_iou: 0.5578  dn_loss_cls: 0.0160  dn_loss_bbox: 0.0304  dn_loss_iou: 0.2753  d0.dn_loss_cls: 0.0485  d0.dn_loss_bbox: 0.0456  d0.dn_loss_iou: 0.3707  d1.dn_loss_cls: 0.0225  d1.dn_loss_bbox: 0.0326  d1.dn_loss_iou: 0.2924  d2.dn_loss_cls: 0.0174  d2.dn_loss_bbox: 0.0309  d2.dn_loss_iou: 0.2786  d3.dn_loss_cls: 0.0166  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2745  d4.dn_loss_cls: 0.0159  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2748  loss_num: 0.0013  d0.loss_num: 0.0012  d1.loss_num: 0.0014  d2.loss_num: 0.0013  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/31 06:49:19 - mmengine - INFO - Epoch(train) [2][400/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:49:07  time: 1.0346  data_time: 0.0109  memory: 9502  grad_norm: 64.2512  loss: 9.1528  loss_cls: 0.5125  loss_bbox: 0.0522  loss_iou: 0.4004  d0.loss_cls: 0.5321  d0.loss_bbox: 0.0577  d0.loss_iou: 0.4257  d1.loss_cls: 0.5234  d1.loss_bbox: 0.0536  d1.loss_iou: 0.4177  d2.loss_cls: 0.5163  d2.loss_bbox: 0.0522  d2.loss_iou: 0.4040  d3.loss_cls: 0.5083  d3.loss_bbox: 0.0523  d3.loss_iou: 0.4028  d4.loss_cls: 0.5136  d4.loss_bbox: 0.0517  d4.loss_iou: 0.4028  enc_loss_cls: 0.5299  enc_loss_bbox: 0.0622  enc_loss_iou: 0.4458  dn_loss_cls: 0.0100  dn_loss_bbox: 0.0363  dn_loss_iou: 0.2912  d0.dn_loss_cls: 0.0506  d0.dn_loss_bbox: 0.0533  d0.dn_loss_iou: 0.3964  d1.dn_loss_cls: 0.0181  d1.dn_loss_bbox: 0.0393  d1.dn_loss_iou: 0.3119  d2.dn_loss_cls: 0.0122  d2.dn_loss_bbox: 0.0370  d2.dn_loss_iou: 0.2963  d3.dn_loss_cls: 0.0111  d3.dn_loss_bbox: 0.0363  d3.dn_loss_iou: 0.2910  d4.dn_loss_cls: 0.0101  d4.dn_loss_bbox: 0.0363  d4.dn_loss_iou: 0.2907  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0014  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0011
2024/10/31 06:50:11 - mmengine - INFO - Epoch(train) [2][450/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:48:15  time: 1.0295  data_time: 0.0108  memory: 9511  grad_norm: 63.2576  loss: 8.8853  loss_cls: 0.4804  loss_bbox: 0.0513  loss_iou: 0.4001  d0.loss_cls: 0.5077  d0.loss_bbox: 0.0550  d0.loss_iou: 0.4250  d1.loss_cls: 0.4948  d1.loss_bbox: 0.0527  d1.loss_iou: 0.4156  d2.loss_cls: 0.4866  d2.loss_bbox: 0.0526  d2.loss_iou: 0.4057  d3.loss_cls: 0.4811  d3.loss_bbox: 0.0521  d3.loss_iou: 0.4014  d4.loss_cls: 0.4771  d4.loss_bbox: 0.0513  d4.loss_iou: 0.4000  enc_loss_cls: 0.5143  enc_loss_bbox: 0.0577  enc_loss_iou: 0.4431  dn_loss_cls: 0.0206  dn_loss_bbox: 0.0314  dn_loss_iou: 0.2804  d0.dn_loss_cls: 0.0532  d0.dn_loss_bbox: 0.0466  d0.dn_loss_iou: 0.3787  d1.dn_loss_cls: 0.0273  d1.dn_loss_bbox: 0.0340  d1.dn_loss_iou: 0.2983  d2.dn_loss_cls: 0.0223  d2.dn_loss_bbox: 0.0319  d2.dn_loss_iou: 0.2836  d3.dn_loss_cls: 0.0213  d3.dn_loss_bbox: 0.0314  d3.dn_loss_iou: 0.2799  d4.dn_loss_cls: 0.0204  d4.dn_loss_bbox: 0.0314  d4.dn_loss_iou: 0.2800  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0014  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0010
2024/10/31 06:51:03 - mmengine - INFO - Epoch(train) [2][500/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:47:24  time: 1.0344  data_time: 0.0112  memory: 9502  grad_norm: 72.7408  loss: 8.5977  loss_cls: 0.4543  loss_bbox: 0.0550  loss_iou: 0.3736  d0.loss_cls: 0.4798  d0.loss_bbox: 0.0592  d0.loss_iou: 0.3904  d1.loss_cls: 0.4510  d1.loss_bbox: 0.0647  d1.loss_iou: 0.4017  d2.loss_cls: 0.4568  d2.loss_bbox: 0.0587  d2.loss_iou: 0.3815  d3.loss_cls: 0.4589  d3.loss_bbox: 0.0546  d3.loss_iou: 0.3748  d4.loss_cls: 0.4602  d4.loss_bbox: 0.0517  d4.loss_iou: 0.3683  enc_loss_cls: 0.4783  enc_loss_bbox: 0.0662  enc_loss_iou: 0.4232  dn_loss_cls: 0.0143  dn_loss_bbox: 0.0349  dn_loss_iou: 0.2918  d0.dn_loss_cls: 0.0510  d0.dn_loss_bbox: 0.0526  d0.dn_loss_iou: 0.3907  d1.dn_loss_cls: 0.0202  d1.dn_loss_bbox: 0.0375  d1.dn_loss_iou: 0.3083  d2.dn_loss_cls: 0.0161  d2.dn_loss_bbox: 0.0354  d2.dn_loss_iou: 0.2934  d3.dn_loss_cls: 0.0151  d3.dn_loss_bbox: 0.0348  d3.dn_loss_iou: 0.2907  d4.dn_loss_cls: 0.0143  d4.dn_loss_bbox: 0.0349  d4.dn_loss_iou: 0.2908  loss_num: 0.0013  d0.loss_num: 0.0013  d1.loss_num: 0.0014  d2.loss_num: 0.0013  d3.loss_num: 0.0012  d4.loss_num: 0.0013
2024/10/31 06:51:54 - mmengine - INFO - Epoch(train) [2][550/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:46:32  time: 1.0299  data_time: 0.0106  memory: 9499  grad_norm: 56.5255  loss: 8.5933  loss_cls: 0.4092  loss_bbox: 0.0615  loss_iou: 0.3797  d0.loss_cls: 0.4383  d0.loss_bbox: 0.0650  d0.loss_iou: 0.3944  d1.loss_cls: 0.4219  d1.loss_bbox: 0.0646  d1.loss_iou: 0.3876  d2.loss_cls: 0.4147  d2.loss_bbox: 0.0617  d2.loss_iou: 0.3814  d3.loss_cls: 0.4079  d3.loss_bbox: 0.0635  d3.loss_iou: 0.3802  d4.loss_cls: 0.4077  d4.loss_bbox: 0.0637  d4.loss_iou: 0.3805  enc_loss_cls: 0.4552  enc_loss_bbox: 0.0660  enc_loss_iou: 0.4083  dn_loss_cls: 0.0561  dn_loss_bbox: 0.0386  dn_loss_iou: 0.2876  d0.dn_loss_cls: 0.0879  d0.dn_loss_bbox: 0.0567  d0.dn_loss_iou: 0.3892  d1.dn_loss_cls: 0.0639  d1.dn_loss_bbox: 0.0414  d1.dn_loss_iou: 0.3062  d2.dn_loss_cls: 0.0531  d2.dn_loss_bbox: 0.0391  d2.dn_loss_iou: 0.2919  d3.dn_loss_cls: 0.0541  d3.dn_loss_bbox: 0.0385  d3.dn_loss_iou: 0.2875  d4.dn_loss_cls: 0.0549  d4.dn_loss_bbox: 0.0386  d4.dn_loss_iou: 0.2874  loss_num: 0.0013  d0.loss_num: 0.0013  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/31 06:52:45 - mmengine - INFO - Epoch(train) [2][600/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:45:40  time: 1.0230  data_time: 0.0109  memory: 9505  grad_norm: 68.2380  loss: 9.0074  loss_cls: 0.4767  loss_bbox: 0.0586  loss_iou: 0.4194  d0.loss_cls: 0.5067  d0.loss_bbox: 0.0571  d0.loss_iou: 0.4223  d1.loss_cls: 0.4892  d1.loss_bbox: 0.0591  d1.loss_iou: 0.4213  d2.loss_cls: 0.4808  d2.loss_bbox: 0.0581  d2.loss_iou: 0.4189  d3.loss_cls: 0.4769  d3.loss_bbox: 0.0592  d3.loss_iou: 0.4253  d4.loss_cls: 0.4732  d4.loss_bbox: 0.0589  d4.loss_iou: 0.4220  enc_loss_cls: 0.5130  enc_loss_bbox: 0.0639  enc_loss_iou: 0.4488  dn_loss_cls: 0.0200  dn_loss_bbox: 0.0355  dn_loss_iou: 0.2800  d0.dn_loss_cls: 0.0506  d0.dn_loss_bbox: 0.0527  d0.dn_loss_iou: 0.3808  d1.dn_loss_cls: 0.0244  d1.dn_loss_bbox: 0.0386  d1.dn_loss_iou: 0.2978  d2.dn_loss_cls: 0.0209  d2.dn_loss_bbox: 0.0360  d2.dn_loss_iou: 0.2834  d3.dn_loss_cls: 0.0206  d3.dn_loss_bbox: 0.0355  d3.dn_loss_iou: 0.2793  d4.dn_loss_cls: 0.0200  d4.dn_loss_bbox: 0.0355  d4.dn_loss_iou: 0.2794  loss_num: 0.0012  d0.loss_num: 0.0011  d1.loss_num: 0.0012  d2.loss_num: 0.0011  d3.loss_num: 0.0012  d4.loss_num: 0.0011
2024/10/31 06:53:37 - mmengine - INFO - Epoch(train) [2][650/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:44:48  time: 1.0292  data_time: 0.0104  memory: 9499  grad_norm: 61.1055  loss: 8.4829  loss_cls: 0.4698  loss_bbox: 0.0524  loss_iou: 0.3482  d0.loss_cls: 0.4959  d0.loss_bbox: 0.0547  d0.loss_iou: 0.3602  d1.loss_cls: 0.4810  d1.loss_bbox: 0.0529  d1.loss_iou: 0.3580  d2.loss_cls: 0.4803  d2.loss_bbox: 0.0523  d2.loss_iou: 0.3514  d3.loss_cls: 0.4763  d3.loss_bbox: 0.0524  d3.loss_iou: 0.3481  d4.loss_cls: 0.4685  d4.loss_bbox: 0.0524  d4.loss_iou: 0.3489  enc_loss_cls: 0.5062  enc_loss_bbox: 0.0552  enc_loss_iou: 0.3681  dn_loss_cls: 0.0301  dn_loss_bbox: 0.0371  dn_loss_iou: 0.2746  d0.dn_loss_cls: 0.0675  d0.dn_loss_bbox: 0.0549  d0.dn_loss_iou: 0.3785  d1.dn_loss_cls: 0.0384  d1.dn_loss_bbox: 0.0399  d1.dn_loss_iou: 0.2898  d2.dn_loss_cls: 0.0323  d2.dn_loss_bbox: 0.0375  d2.dn_loss_iou: 0.2761  d3.dn_loss_cls: 0.0310  d3.dn_loss_bbox: 0.0371  d3.dn_loss_iou: 0.2741  d4.dn_loss_cls: 0.0299  d4.dn_loss_bbox: 0.0370  d4.dn_loss_iou: 0.2740  loss_num: 0.0016  d0.loss_num: 0.0016  d1.loss_num: 0.0017  d2.loss_num: 0.0016  d3.loss_num: 0.0016  d4.loss_num: 0.0016
2024/10/31 06:54:28 - mmengine - INFO - Epoch(train) [2][700/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:43:56  time: 1.0339  data_time: 0.0110  memory: 9511  grad_norm: 64.1033  loss: 8.3755  loss_cls: 0.4641  loss_bbox: 0.0510  loss_iou: 0.3459  d0.loss_cls: 0.4689  d0.loss_bbox: 0.0539  d0.loss_iou: 0.3681  d1.loss_cls: 0.4680  d1.loss_bbox: 0.0524  d1.loss_iou: 0.3597  d2.loss_cls: 0.4689  d2.loss_bbox: 0.0500  d2.loss_iou: 0.3474  d3.loss_cls: 0.4706  d3.loss_bbox: 0.0478  d3.loss_iou: 0.3416  d4.loss_cls: 0.4641  d4.loss_bbox: 0.0510  d4.loss_iou: 0.3456  enc_loss_cls: 0.4808  enc_loss_bbox: 0.0552  enc_loss_iou: 0.3797  dn_loss_cls: 0.0318  dn_loss_bbox: 0.0352  dn_loss_iou: 0.2746  d0.dn_loss_cls: 0.0681  d0.dn_loss_bbox: 0.0508  d0.dn_loss_iou: 0.3712  d1.dn_loss_cls: 0.0379  d1.dn_loss_bbox: 0.0383  d1.dn_loss_iou: 0.2941  d2.dn_loss_cls: 0.0328  d2.dn_loss_bbox: 0.0358  d2.dn_loss_iou: 0.2782  d3.dn_loss_cls: 0.0313  d3.dn_loss_bbox: 0.0353  d3.dn_loss_iou: 0.2744  d4.dn_loss_cls: 0.0317  d4.dn_loss_bbox: 0.0351  d4.dn_loss_iou: 0.2742  loss_num: 0.0017  d0.loss_num: 0.0018  d1.loss_num: 0.0018  d2.loss_num: 0.0016  d3.loss_num: 0.0017  d4.loss_num: 0.0016
2024/10/31 06:55:20 - mmengine - INFO - Epoch(train) [2][750/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:43:06  time: 1.0406  data_time: 0.0106  memory: 9491  grad_norm: 72.0006  loss: 8.0368  loss_cls: 0.4274  loss_bbox: 0.0520  loss_iou: 0.3629  d0.loss_cls: 0.4425  d0.loss_bbox: 0.0476  d0.loss_iou: 0.3716  d1.loss_cls: 0.4307  d1.loss_bbox: 0.0517  d1.loss_iou: 0.3701  d2.loss_cls: 0.4284  d2.loss_bbox: 0.0505  d2.loss_iou: 0.3610  d3.loss_cls: 0.4222  d3.loss_bbox: 0.0530  d3.loss_iou: 0.3688  d4.loss_cls: 0.4268  d4.loss_bbox: 0.0518  d4.loss_iou: 0.3626  enc_loss_cls: 0.4584  enc_loss_bbox: 0.0513  enc_loss_iou: 0.3935  dn_loss_cls: 0.0130  dn_loss_bbox: 0.0318  dn_loss_iou: 0.2678  d0.dn_loss_cls: 0.0415  d0.dn_loss_bbox: 0.0469  d0.dn_loss_iou: 0.3620  d1.dn_loss_cls: 0.0201  d1.dn_loss_bbox: 0.0339  d1.dn_loss_iou: 0.2843  d2.dn_loss_cls: 0.0155  d2.dn_loss_bbox: 0.0321  d2.dn_loss_iou: 0.2700  d3.dn_loss_cls: 0.0145  d3.dn_loss_bbox: 0.0317  d3.dn_loss_iou: 0.2674  d4.dn_loss_cls: 0.0129  d4.dn_loss_bbox: 0.0318  d4.dn_loss_iou: 0.2674  loss_num: 0.0012  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/31 06:56:12 - mmengine - INFO - Epoch(train) [2][800/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:42:14  time: 1.0299  data_time: 0.0108  memory: 9508  grad_norm: 71.3373  loss: 7.6791  loss_cls: 0.3873  loss_bbox: 0.0428  loss_iou: 0.3235  d0.loss_cls: 0.4032  d0.loss_bbox: 0.0498  d0.loss_iou: 0.3492  d1.loss_cls: 0.3990  d1.loss_bbox: 0.0452  d1.loss_iou: 0.3336  d2.loss_cls: 0.3934  d2.loss_bbox: 0.0437  d2.loss_iou: 0.3269  d3.loss_cls: 0.3874  d3.loss_bbox: 0.0442  d3.loss_iou: 0.3252  d4.loss_cls: 0.3833  d4.loss_bbox: 0.0441  d4.loss_iou: 0.3257  enc_loss_cls: 0.4166  enc_loss_bbox: 0.0514  enc_loss_iou: 0.3626  dn_loss_cls: 0.0102  dn_loss_bbox: 0.0371  dn_loss_iou: 0.2926  d0.dn_loss_cls: 0.0462  d0.dn_loss_bbox: 0.0557  d0.dn_loss_iou: 0.4020  d1.dn_loss_cls: 0.0159  d1.dn_loss_bbox: 0.0400  d1.dn_loss_iou: 0.3114  d2.dn_loss_cls: 0.0109  d2.dn_loss_bbox: 0.0378  d2.dn_loss_iou: 0.2957  d3.dn_loss_cls: 0.0102  d3.dn_loss_bbox: 0.0372  d3.dn_loss_iou: 0.2919  d4.dn_loss_cls: 0.0103  d4.dn_loss_bbox: 0.0371  d4.dn_loss_iou: 0.2918  loss_num: 0.0012  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/31 06:56:26 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 06:56:26 - mmengine - INFO - Saving checkpoint at 2 epochs
2024/10/31 06:56:37 - mmengine - INFO - Epoch(val) [2][ 50/858]    eta: 0:01:12  time: 0.0903  data_time: 0.0027  memory: 9499  
2024/10/31 06:56:42 - mmengine - INFO - Epoch(val) [2][100/858]    eta: 0:01:07  time: 0.0874  data_time: 0.0019  memory: 3167  
2024/10/31 06:56:46 - mmengine - INFO - Epoch(val) [2][150/858]    eta: 0:01:02  time: 0.0871  data_time: 0.0020  memory: 3164  
2024/10/31 06:56:50 - mmengine - INFO - Epoch(val) [2][200/858]    eta: 0:00:57  time: 0.0873  data_time: 0.0019  memory: 3170  
2024/10/31 06:56:55 - mmengine - INFO - Epoch(val) [2][250/858]    eta: 0:00:53  time: 0.0870  data_time: 0.0019  memory: 3170  
2024/10/31 06:56:59 - mmengine - INFO - Epoch(val) [2][300/858]    eta: 0:00:48  time: 0.0870  data_time: 0.0019  memory: 3170  
2024/10/31 06:57:04 - mmengine - INFO - Epoch(val) [2][350/858]    eta: 0:00:44  time: 0.0868  data_time: 0.0019  memory: 3167  
2024/10/31 06:57:08 - mmengine - INFO - Epoch(val) [2][400/858]    eta: 0:00:40  time: 0.0871  data_time: 0.0020  memory: 3173  
2024/10/31 06:57:12 - mmengine - INFO - Epoch(val) [2][450/858]    eta: 0:00:35  time: 0.0869  data_time: 0.0020  memory: 3170  
2024/10/31 06:57:17 - mmengine - INFO - Epoch(val) [2][500/858]    eta: 0:00:31  time: 0.0871  data_time: 0.0020  memory: 3169  
2024/10/31 06:57:21 - mmengine - INFO - Epoch(val) [2][550/858]    eta: 0:00:26  time: 0.0872  data_time: 0.0021  memory: 3170  
2024/10/31 06:57:25 - mmengine - INFO - Epoch(val) [2][600/858]    eta: 0:00:22  time: 0.0874  data_time: 0.0021  memory: 3176  
2024/10/31 06:57:30 - mmengine - INFO - Epoch(val) [2][650/858]    eta: 0:00:18  time: 0.0874  data_time: 0.0020  memory: 3170  
2024/10/31 06:57:34 - mmengine - INFO - Epoch(val) [2][700/858]    eta: 0:00:13  time: 0.0871  data_time: 0.0020  memory: 3170  
2024/10/31 06:57:38 - mmengine - INFO - Epoch(val) [2][750/858]    eta: 0:00:09  time: 0.0868  data_time: 0.0019  memory: 3167  
2024/10/31 06:57:43 - mmengine - INFO - Epoch(val) [2][800/858]    eta: 0:00:05  time: 0.0867  data_time: 0.0019  memory: 3167  
2024/10/31 06:57:47 - mmengine - INFO - Epoch(val) [2][850/858]    eta: 0:00:00  time: 0.0854  data_time: 0.0019  memory: 3170  
2024/10/31 06:57:49 - mmengine - INFO - {'instance_F1_score': 0.12016021361815754, 'instance_acc': 0.07558379404241274, 'image_F1_score': 0.18025513033832502, 'image_acc': 0.1386946386946387}
2024/10/31 06:57:49 - mmengine - INFO - Epoch(val) [2][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.1202  grefcoco_val/refdrone/instance_acc: 0.0756  grefcoco_val/refdrone/image_F1_score: 0.1803  grefcoco_val/refdrone/image_acc: 0.1387  data_time: 0.0020  time: 0.0872
2024/10/31 06:58:41 - mmengine - INFO - Epoch(train) [3][ 50/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:41:08  time: 1.0411  data_time: 0.0094  memory: 9505  grad_norm: 63.1577  loss: 9.4399  loss_cls: 0.4767  loss_bbox: 0.0536  loss_iou: 0.4570  d0.loss_cls: 0.5087  d0.loss_bbox: 0.0587  d0.loss_iou: 0.4753  d1.loss_cls: 0.4844  d1.loss_bbox: 0.0571  d1.loss_iou: 0.4716  d2.loss_cls: 0.4758  d2.loss_bbox: 0.0580  d2.loss_iou: 0.4660  d3.loss_cls: 0.4790  d3.loss_bbox: 0.0532  d3.loss_iou: 0.4549  d4.loss_cls: 0.4771  d4.loss_bbox: 0.0536  d4.loss_iou: 0.4580  enc_loss_cls: 0.5100  enc_loss_bbox: 0.0609  enc_loss_iou: 0.4926  dn_loss_cls: 0.0171  dn_loss_bbox: 0.0342  dn_loss_iou: 0.3086  d0.dn_loss_cls: 0.0566  d0.dn_loss_bbox: 0.0483  d0.dn_loss_iou: 0.4096  d1.dn_loss_cls: 0.0234  d1.dn_loss_bbox: 0.0368  d1.dn_loss_iou: 0.3296  d2.dn_loss_cls: 0.0184  d2.dn_loss_bbox: 0.0346  d2.dn_loss_iou: 0.3125  d3.dn_loss_cls: 0.0175  d3.dn_loss_bbox: 0.0342  d3.dn_loss_iou: 0.3084  d4.dn_loss_cls: 0.0172  d4.dn_loss_bbox: 0.0342  d4.dn_loss_iou: 0.3084  loss_num: 0.0013  d0.loss_num: 0.0013  d1.loss_num: 0.0014  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0012
2024/10/31 06:59:33 - mmengine - INFO - Epoch(train) [3][100/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:40:16  time: 1.0253  data_time: 0.0107  memory: 9502  grad_norm: 61.3172  loss: 7.8703  loss_cls: 0.3708  loss_bbox: 0.0527  loss_iou: 0.3769  d0.loss_cls: 0.4009  d0.loss_bbox: 0.0557  d0.loss_iou: 0.3863  d1.loss_cls: 0.3831  d1.loss_bbox: 0.0542  d1.loss_iou: 0.3852  d2.loss_cls: 0.3762  d2.loss_bbox: 0.0544  d2.loss_iou: 0.3855  d3.loss_cls: 0.3715  d3.loss_bbox: 0.0539  d3.loss_iou: 0.3859  d4.loss_cls: 0.3744  d4.loss_bbox: 0.0521  d4.loss_iou: 0.3749  enc_loss_cls: 0.4021  enc_loss_bbox: 0.0615  enc_loss_iou: 0.4159  dn_loss_cls: 0.0150  dn_loss_bbox: 0.0377  dn_loss_iou: 0.2700  d0.dn_loss_cls: 0.0439  d0.dn_loss_bbox: 0.0518  d0.dn_loss_iou: 0.3561  d1.dn_loss_cls: 0.0194  d1.dn_loss_bbox: 0.0408  d1.dn_loss_iou: 0.2868  d2.dn_loss_cls: 0.0160  d2.dn_loss_bbox: 0.0384  d2.dn_loss_iou: 0.2724  d3.dn_loss_cls: 0.0147  d3.dn_loss_bbox: 0.0376  d3.dn_loss_iou: 0.2682  d4.dn_loss_cls: 0.0145  d4.dn_loss_bbox: 0.0377  d4.dn_loss_iou: 0.2689  loss_num: 0.0011  d0.loss_num: 0.0011  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0010
2024/10/31 07:00:25 - mmengine - INFO - Epoch(train) [3][150/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:39:25  time: 1.0407  data_time: 0.0126  memory: 9508  grad_norm: 65.0239  loss: 7.9787  loss_cls: 0.4113  loss_bbox: 0.0504  loss_iou: 0.3598  d0.loss_cls: 0.4501  d0.loss_bbox: 0.0549  d0.loss_iou: 0.3706  d1.loss_cls: 0.4330  d1.loss_bbox: 0.0523  d1.loss_iou: 0.3662  d2.loss_cls: 0.4205  d2.loss_bbox: 0.0503  d2.loss_iou: 0.3595  d3.loss_cls: 0.4143  d3.loss_bbox: 0.0537  d3.loss_iou: 0.3624  d4.loss_cls: 0.4130  d4.loss_bbox: 0.0499  d4.loss_iou: 0.3580  enc_loss_cls: 0.4559  enc_loss_bbox: 0.0603  enc_loss_iou: 0.4011  dn_loss_cls: 0.0116  dn_loss_bbox: 0.0339  dn_loss_iou: 0.2656  d0.dn_loss_cls: 0.0408  d0.dn_loss_bbox: 0.0481  d0.dn_loss_iou: 0.3543  d1.dn_loss_cls: 0.0160  d1.dn_loss_bbox: 0.0360  d1.dn_loss_iou: 0.2806  d2.dn_loss_cls: 0.0124  d2.dn_loss_bbox: 0.0342  d2.dn_loss_iou: 0.2687  d3.dn_loss_cls: 0.0116  d3.dn_loss_bbox: 0.0339  d3.dn_loss_iou: 0.2652  d4.dn_loss_cls: 0.0113  d4.dn_loss_bbox: 0.0338  d4.dn_loss_iou: 0.2650  loss_num: 0.0013  d0.loss_num: 0.0013  d1.loss_num: 0.0014  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/31 07:01:15 - mmengine - INFO - Epoch(train) [3][200/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:38:32  time: 1.0117  data_time: 0.0110  memory: 9499  grad_norm: 61.1438  loss: 8.5736  loss_cls: 0.4439  loss_bbox: 0.0525  loss_iou: 0.3893  d0.loss_cls: 0.4701  d0.loss_bbox: 0.0546  d0.loss_iou: 0.4022  d1.loss_cls: 0.4452  d1.loss_bbox: 0.0550  d1.loss_iou: 0.3960  d2.loss_cls: 0.4428  d2.loss_bbox: 0.0538  d2.loss_iou: 0.3920  d3.loss_cls: 0.4435  d3.loss_bbox: 0.0529  d3.loss_iou: 0.3929  d4.loss_cls: 0.4459  d4.loss_bbox: 0.0528  d4.loss_iou: 0.3927  enc_loss_cls: 0.4711  enc_loss_bbox: 0.0637  enc_loss_iou: 0.4416  dn_loss_cls: 0.0106  dn_loss_bbox: 0.0341  dn_loss_iou: 0.2937  d0.dn_loss_cls: 0.0419  d0.dn_loss_bbox: 0.0501  d0.dn_loss_iou: 0.3938  d1.dn_loss_cls: 0.0165  d1.dn_loss_bbox: 0.0369  d1.dn_loss_iou: 0.3119  d2.dn_loss_cls: 0.0129  d2.dn_loss_bbox: 0.0347  d2.dn_loss_iou: 0.2978  d3.dn_loss_cls: 0.0120  d3.dn_loss_bbox: 0.0341  d3.dn_loss_iou: 0.2930  d4.dn_loss_cls: 0.0106  d4.dn_loss_bbox: 0.0341  d4.dn_loss_iou: 0.2934  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0013  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/31 07:02:07 - mmengine - INFO - Epoch(train) [3][250/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:37:41  time: 1.0298  data_time: 0.0106  memory: 9499  grad_norm: 61.4856  loss: 8.7119  loss_cls: 0.4441  loss_bbox: 0.0612  loss_iou: 0.3981  d0.loss_cls: 0.4805  d0.loss_bbox: 0.0691  d0.loss_iou: 0.4312  d1.loss_cls: 0.4786  d1.loss_bbox: 0.0618  d1.loss_iou: 0.4061  d2.loss_cls: 0.4621  d2.loss_bbox: 0.0611  d2.loss_iou: 0.4003  d3.loss_cls: 0.4510  d3.loss_bbox: 0.0618  d3.loss_iou: 0.3992  d4.loss_cls: 0.4429  d4.loss_bbox: 0.0615  d4.loss_iou: 0.3985  enc_loss_cls: 0.5064  enc_loss_bbox: 0.0703  enc_loss_iou: 0.4434  dn_loss_cls: 0.0090  dn_loss_bbox: 0.0377  dn_loss_iou: 0.2776  d0.dn_loss_cls: 0.0379  d0.dn_loss_bbox: 0.0542  d0.dn_loss_iou: 0.3738  d1.dn_loss_cls: 0.0134  d1.dn_loss_bbox: 0.0402  d1.dn_loss_iou: 0.2942  d2.dn_loss_cls: 0.0103  d2.dn_loss_bbox: 0.0382  d2.dn_loss_iou: 0.2809  d3.dn_loss_cls: 0.0098  d3.dn_loss_bbox: 0.0377  d3.dn_loss_iou: 0.2770  d4.dn_loss_cls: 0.0090  d4.dn_loss_bbox: 0.0377  d4.dn_loss_iou: 0.2771  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/31 07:02:58 - mmengine - INFO - Epoch(train) [3][300/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:36:49  time: 1.0323  data_time: 0.0119  memory: 9511  grad_norm: 59.0172  loss: 8.2549  loss_cls: 0.4044  loss_bbox: 0.0548  loss_iou: 0.3887  d0.loss_cls: 0.4391  d0.loss_bbox: 0.0547  d0.loss_iou: 0.3956  d1.loss_cls: 0.4201  d1.loss_bbox: 0.0526  d1.loss_iou: 0.3829  d2.loss_cls: 0.4183  d2.loss_bbox: 0.0523  d2.loss_iou: 0.3825  d3.loss_cls: 0.4156  d3.loss_bbox: 0.0540  d3.loss_iou: 0.3866  d4.loss_cls: 0.4054  d4.loss_bbox: 0.0546  d4.loss_iou: 0.3883  enc_loss_cls: 0.4520  enc_loss_bbox: 0.0609  enc_loss_iou: 0.4213  dn_loss_cls: 0.0179  dn_loss_bbox: 0.0337  dn_loss_iou: 0.2782  d0.dn_loss_cls: 0.0490  d0.dn_loss_bbox: 0.0500  d0.dn_loss_iou: 0.3804  d1.dn_loss_cls: 0.0247  d1.dn_loss_bbox: 0.0363  d1.dn_loss_iou: 0.2958  d2.dn_loss_cls: 0.0200  d2.dn_loss_bbox: 0.0344  d2.dn_loss_iou: 0.2813  d3.dn_loss_cls: 0.0191  d3.dn_loss_bbox: 0.0337  d3.dn_loss_iou: 0.2774  d4.dn_loss_cls: 0.0182  d4.dn_loss_bbox: 0.0337  d4.dn_loss_iou: 0.2778  loss_num: 0.0014  d0.loss_num: 0.0014  d1.loss_num: 0.0015  d2.loss_num: 0.0015  d3.loss_num: 0.0014  d4.loss_num: 0.0013
2024/10/31 07:03:50 - mmengine - INFO - Epoch(train) [3][350/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:35:57  time: 1.0286  data_time: 0.0105  memory: 9499  grad_norm: 65.0889  loss: 6.8146  loss_cls: 0.3351  loss_bbox: 0.0402  loss_iou: 0.3080  d0.loss_cls: 0.3538  d0.loss_bbox: 0.0427  d0.loss_iou: 0.3210  d1.loss_cls: 0.3311  d1.loss_bbox: 0.0446  d1.loss_iou: 0.3186  d2.loss_cls: 0.3373  d2.loss_bbox: 0.0386  d2.loss_iou: 0.3020  d3.loss_cls: 0.3340  d3.loss_bbox: 0.0394  d3.loss_iou: 0.3052  d4.loss_cls: 0.3355  d4.loss_bbox: 0.0401  d4.loss_iou: 0.3066  enc_loss_cls: 0.3788  enc_loss_bbox: 0.0496  enc_loss_iou: 0.3470  dn_loss_cls: 0.0082  dn_loss_bbox: 0.0324  dn_loss_iou: 0.2501  d0.dn_loss_cls: 0.0353  d0.dn_loss_bbox: 0.0473  d0.dn_loss_iou: 0.3430  d1.dn_loss_cls: 0.0118  d1.dn_loss_bbox: 0.0344  d1.dn_loss_iou: 0.2630  d2.dn_loss_cls: 0.0092  d2.dn_loss_bbox: 0.0326  d2.dn_loss_iou: 0.2514  d3.dn_loss_cls: 0.0086  d3.dn_loss_bbox: 0.0323  d3.dn_loss_iou: 0.2493  d4.dn_loss_cls: 0.0083  d4.dn_loss_bbox: 0.0323  d4.dn_loss_iou: 0.2496  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/31 07:04:12 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 07:04:41 - mmengine - INFO - Epoch(train) [3][400/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:35:05  time: 1.0249  data_time: 0.0101  memory: 9321  grad_norm: 64.1937  loss: 8.8412  loss_cls: 0.4182  loss_bbox: 0.0521  loss_iou: 0.4302  d0.loss_cls: 0.4587  d0.loss_bbox: 0.0558  d0.loss_iou: 0.4462  d1.loss_cls: 0.4395  d1.loss_bbox: 0.0534  d1.loss_iou: 0.4363  d2.loss_cls: 0.4267  d2.loss_bbox: 0.0532  d2.loss_iou: 0.4361  d3.loss_cls: 0.4194  d3.loss_bbox: 0.0521  d3.loss_iou: 0.4293  d4.loss_cls: 0.4156  d4.loss_bbox: 0.0522  d4.loss_iou: 0.4299  enc_loss_cls: 0.4824  enc_loss_bbox: 0.0600  enc_loss_iou: 0.4728  dn_loss_cls: 0.0181  dn_loss_bbox: 0.0351  dn_loss_iou: 0.3015  d0.dn_loss_cls: 0.0533  d0.dn_loss_bbox: 0.0513  d0.dn_loss_iou: 0.4104  d1.dn_loss_cls: 0.0234  d1.dn_loss_bbox: 0.0372  d1.dn_loss_iou: 0.3182  d2.dn_loss_cls: 0.0191  d2.dn_loss_bbox: 0.0353  d2.dn_loss_iou: 0.3040  d3.dn_loss_cls: 0.0183  d3.dn_loss_bbox: 0.0350  d3.dn_loss_iou: 0.2999  d4.dn_loss_cls: 0.0173  d4.dn_loss_bbox: 0.0350  d4.dn_loss_iou: 0.3004  loss_num: 0.0013  d0.loss_num: 0.0015  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2024/10/31 07:05:32 - mmengine - INFO - Epoch(train) [3][450/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:34:14  time: 1.0280  data_time: 0.0098  memory: 9321  grad_norm: 60.2205  loss: 8.1556  loss_cls: 0.4097  loss_bbox: 0.0525  loss_iou: 0.3868  d0.loss_cls: 0.4486  d0.loss_bbox: 0.0592  d0.loss_iou: 0.4053  d1.loss_cls: 0.4141  d1.loss_bbox: 0.0576  d1.loss_iou: 0.3965  d2.loss_cls: 0.4207  d2.loss_bbox: 0.0555  d2.loss_iou: 0.3802  d3.loss_cls: 0.4181  d3.loss_bbox: 0.0517  d3.loss_iou: 0.3808  d4.loss_cls: 0.4119  d4.loss_bbox: 0.0530  d4.loss_iou: 0.3831  enc_loss_cls: 0.4521  enc_loss_bbox: 0.0614  enc_loss_iou: 0.4162  dn_loss_cls: 0.0121  dn_loss_bbox: 0.0342  dn_loss_iou: 0.2645  d0.dn_loss_cls: 0.0426  d0.dn_loss_bbox: 0.0503  d0.dn_loss_iou: 0.3599  d1.dn_loss_cls: 0.0171  d1.dn_loss_bbox: 0.0370  d1.dn_loss_iou: 0.2803  d2.dn_loss_cls: 0.0135  d2.dn_loss_bbox: 0.0346  d2.dn_loss_iou: 0.2664  d3.dn_loss_cls: 0.0124  d3.dn_loss_bbox: 0.0341  d3.dn_loss_iou: 0.2640  d4.dn_loss_cls: 0.0122  d4.dn_loss_bbox: 0.0342  d4.dn_loss_iou: 0.2639  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0013  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/31 07:06:24 - mmengine - INFO - Epoch(train) [3][500/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:33:22  time: 1.0272  data_time: 0.0114  memory: 9225  grad_norm: 64.7827  loss: 9.2858  loss_cls: 0.4655  loss_bbox: 0.0590  loss_iou: 0.4556  d0.loss_cls: 0.5016  d0.loss_bbox: 0.0594  d0.loss_iou: 0.4522  d1.loss_cls: 0.4845  d1.loss_bbox: 0.0590  d1.loss_iou: 0.4609  d2.loss_cls: 0.4827  d2.loss_bbox: 0.0577  d2.loss_iou: 0.4508  d3.loss_cls: 0.4720  d3.loss_bbox: 0.0585  d3.loss_iou: 0.4510  d4.loss_cls: 0.4674  d4.loss_bbox: 0.0585  d4.loss_iou: 0.4527  enc_loss_cls: 0.5044  enc_loss_bbox: 0.0605  enc_loss_iou: 0.4788  dn_loss_cls: 0.0145  dn_loss_bbox: 0.0354  dn_loss_iou: 0.3013  d0.dn_loss_cls: 0.0461  d0.dn_loss_bbox: 0.0511  d0.dn_loss_iou: 0.4016  d1.dn_loss_cls: 0.0206  d1.dn_loss_bbox: 0.0380  d1.dn_loss_iou: 0.3200  d2.dn_loss_cls: 0.0165  d2.dn_loss_bbox: 0.0357  d2.dn_loss_iou: 0.3043  d3.dn_loss_cls: 0.0150  d3.dn_loss_bbox: 0.0354  d3.dn_loss_iou: 0.3004  d4.dn_loss_cls: 0.0145  d4.dn_loss_bbox: 0.0354  d4.dn_loss_iou: 0.3007  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0010
2024/10/31 07:07:15 - mmengine - INFO - Epoch(train) [3][550/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:32:31  time: 1.0333  data_time: 0.0104  memory: 9508  grad_norm: 56.0857  loss: 7.8091  loss_cls: 0.3742  loss_bbox: 0.0480  loss_iou: 0.3697  d0.loss_cls: 0.3924  d0.loss_bbox: 0.0509  d0.loss_iou: 0.3834  d1.loss_cls: 0.3723  d1.loss_bbox: 0.0504  d1.loss_iou: 0.3773  d2.loss_cls: 0.3768  d2.loss_bbox: 0.0486  d2.loss_iou: 0.3670  d3.loss_cls: 0.3751  d3.loss_bbox: 0.0473  d3.loss_iou: 0.3688  d4.loss_cls: 0.3773  d4.loss_bbox: 0.0474  d4.loss_iou: 0.3660  enc_loss_cls: 0.4187  enc_loss_bbox: 0.0502  enc_loss_iou: 0.3934  dn_loss_cls: 0.0108  dn_loss_bbox: 0.0334  dn_loss_iou: 0.2860  d0.dn_loss_cls: 0.0436  d0.dn_loss_bbox: 0.0468  d0.dn_loss_iou: 0.3782  d1.dn_loss_cls: 0.0146  d1.dn_loss_bbox: 0.0355  d1.dn_loss_iou: 0.3030  d2.dn_loss_cls: 0.0118  d2.dn_loss_bbox: 0.0337  d2.dn_loss_iou: 0.2889  d3.dn_loss_cls: 0.0111  d3.dn_loss_bbox: 0.0333  d3.dn_loss_iou: 0.2858  d4.dn_loss_cls: 0.0109  d4.dn_loss_bbox: 0.0334  d4.dn_loss_iou: 0.2858  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0012  d2.loss_num: 0.0013  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2024/10/31 07:08:07 - mmengine - INFO - Epoch(train) [3][600/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:31:39  time: 1.0359  data_time: 0.0105  memory: 9499  grad_norm: 66.9327  loss: 8.9222  loss_cls: 0.4554  loss_bbox: 0.0510  loss_iou: 0.4164  d0.loss_cls: 0.4669  d0.loss_bbox: 0.0533  d0.loss_iou: 0.4345  d1.loss_cls: 0.4589  d1.loss_bbox: 0.0516  d1.loss_iou: 0.4190  d2.loss_cls: 0.4538  d2.loss_bbox: 0.0506  d2.loss_iou: 0.4164  d3.loss_cls: 0.4579  d3.loss_bbox: 0.0505  d3.loss_iou: 0.4134  d4.loss_cls: 0.4540  d4.loss_bbox: 0.0509  d4.loss_iou: 0.4163  enc_loss_cls: 0.4804  enc_loss_bbox: 0.0579  enc_loss_iou: 0.4596  dn_loss_cls: 0.0132  dn_loss_bbox: 0.0359  dn_loss_iou: 0.3097  d0.dn_loss_cls: 0.0517  d0.dn_loss_bbox: 0.0519  d0.dn_loss_iou: 0.4175  d1.dn_loss_cls: 0.0190  d1.dn_loss_bbox: 0.0383  d1.dn_loss_iou: 0.3288  d2.dn_loss_cls: 0.0147  d2.dn_loss_bbox: 0.0362  d2.dn_loss_iou: 0.3122  d3.dn_loss_cls: 0.0136  d3.dn_loss_bbox: 0.0359  d3.dn_loss_iou: 0.3095  d4.dn_loss_cls: 0.0132  d4.dn_loss_bbox: 0.0359  d4.dn_loss_iou: 0.3093  loss_num: 0.0011  d0.loss_num: 0.0013  d1.loss_num: 0.0011  d2.loss_num: 0.0012  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2024/10/31 07:08:59 - mmengine - INFO - Epoch(train) [3][650/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:30:48  time: 1.0308  data_time: 0.0108  memory: 9502  grad_norm: 60.5638  loss: 8.9428  loss_cls: 0.4615  loss_bbox: 0.0559  loss_iou: 0.4364  d0.loss_cls: 0.4895  d0.loss_bbox: 0.0605  d0.loss_iou: 0.4551  d1.loss_cls: 0.4692  d1.loss_bbox: 0.0575  d1.loss_iou: 0.4455  d2.loss_cls: 0.4699  d2.loss_bbox: 0.0558  d2.loss_iou: 0.4332  d3.loss_cls: 0.4651  d3.loss_bbox: 0.0550  d3.loss_iou: 0.4320  d4.loss_cls: 0.4649  d4.loss_bbox: 0.0550  d4.loss_iou: 0.4320  enc_loss_cls: 0.4999  enc_loss_bbox: 0.0655  enc_loss_iou: 0.4942  dn_loss_cls: 0.0266  dn_loss_bbox: 0.0310  dn_loss_iou: 0.2643  d0.dn_loss_cls: 0.0488  d0.dn_loss_bbox: 0.0470  d0.dn_loss_iou: 0.3613  d1.dn_loss_cls: 0.0265  d1.dn_loss_bbox: 0.0338  d1.dn_loss_iou: 0.2811  d2.dn_loss_cls: 0.0252  d2.dn_loss_bbox: 0.0315  d2.dn_loss_iou: 0.2668  d3.dn_loss_cls: 0.0242  d3.dn_loss_bbox: 0.0311  d3.dn_loss_iou: 0.2634  d4.dn_loss_cls: 0.0262  d4.dn_loss_bbox: 0.0310  d4.dn_loss_iou: 0.2639  loss_num: 0.0008  d0.loss_num: 0.0010  d1.loss_num: 0.0010  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/31 07:09:50 - mmengine - INFO - Epoch(train) [3][700/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:29:56  time: 1.0299  data_time: 0.0122  memory: 9499  grad_norm: 67.5064  loss: 8.2010  loss_cls: 0.3681  loss_bbox: 0.0551  loss_iou: 0.4279  d0.loss_cls: 0.4038  d0.loss_bbox: 0.0553  d0.loss_iou: 0.4417  d1.loss_cls: 0.3762  d1.loss_bbox: 0.0589  d1.loss_iou: 0.4433  d2.loss_cls: 0.3690  d2.loss_bbox: 0.0549  d2.loss_iou: 0.4268  d3.loss_cls: 0.3666  d3.loss_bbox: 0.0557  d3.loss_iou: 0.4301  d4.loss_cls: 0.3705  d4.loss_bbox: 0.0552  d4.loss_iou: 0.4279  enc_loss_cls: 0.4036  enc_loss_bbox: 0.0606  enc_loss_iou: 0.4742  dn_loss_cls: 0.0125  dn_loss_bbox: 0.0339  dn_loss_iou: 0.2691  d0.dn_loss_cls: 0.0460  d0.dn_loss_bbox: 0.0497  d0.dn_loss_iou: 0.3679  d1.dn_loss_cls: 0.0175  d1.dn_loss_bbox: 0.0365  d1.dn_loss_iou: 0.2876  d2.dn_loss_cls: 0.0142  d2.dn_loss_bbox: 0.0341  d2.dn_loss_iou: 0.2712  d3.dn_loss_cls: 0.0128  d3.dn_loss_bbox: 0.0339  d3.dn_loss_iou: 0.2683  d4.dn_loss_cls: 0.0126  d4.dn_loss_bbox: 0.0338  d4.dn_loss_iou: 0.2681  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0011  d2.loss_num: 0.0010  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/31 07:10:41 - mmengine - INFO - Epoch(train) [3][750/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:29:04  time: 1.0199  data_time: 0.0109  memory: 9336  grad_norm: 66.0347  loss: 9.5927  loss_cls: 0.4467  loss_bbox: 0.0688  loss_iou: 0.5009  d0.loss_cls: 0.4682  d0.loss_bbox: 0.0797  d0.loss_iou: 0.5458  d1.loss_cls: 0.4565  d1.loss_bbox: 0.0639  d1.loss_iou: 0.5121  d2.loss_cls: 0.4534  d2.loss_bbox: 0.0621  d2.loss_iou: 0.4998  d3.loss_cls: 0.4545  d3.loss_bbox: 0.0610  d3.loss_iou: 0.4965  d4.loss_cls: 0.4442  d4.loss_bbox: 0.0688  d4.loss_iou: 0.5059  enc_loss_cls: 0.4988  enc_loss_bbox: 0.0737  enc_loss_iou: 0.5599  dn_loss_cls: 0.0153  dn_loss_bbox: 0.0338  dn_loss_iou: 0.2964  d0.dn_loss_cls: 0.0481  d0.dn_loss_bbox: 0.0490  d0.dn_loss_iou: 0.4041  d1.dn_loss_cls: 0.0221  d1.dn_loss_bbox: 0.0364  d1.dn_loss_iou: 0.3173  d2.dn_loss_cls: 0.0174  d2.dn_loss_bbox: 0.0342  d2.dn_loss_iou: 0.3003  d3.dn_loss_cls: 0.0156  d3.dn_loss_bbox: 0.0338  d3.dn_loss_iou: 0.2962  d4.dn_loss_cls: 0.0157  d4.dn_loss_bbox: 0.0338  d4.dn_loss_iou: 0.2960  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/31 07:11:33 - mmengine - INFO - Epoch(train) [3][800/814]  base_lr: 2.0000e-04 lr: 2.0000e-04  eta: 0:28:12  time: 1.0318  data_time: 0.0105  memory: 9502  grad_norm: 62.2122  loss: 8.0216  loss_cls: 0.4143  loss_bbox: 0.0477  loss_iou: 0.3643  d0.loss_cls: 0.4404  d0.loss_bbox: 0.0472  d0.loss_iou: 0.3685  d1.loss_cls: 0.4105  d1.loss_bbox: 0.0480  d1.loss_iou: 0.3692  d2.loss_cls: 0.4152  d2.loss_bbox: 0.0465  d2.loss_iou: 0.3644  d3.loss_cls: 0.4042  d3.loss_bbox: 0.0476  d3.loss_iou: 0.3698  d4.loss_cls: 0.4130  d4.loss_bbox: 0.0477  d4.loss_iou: 0.3643  enc_loss_cls: 0.4531  enc_loss_bbox: 0.0517  enc_loss_iou: 0.3945  dn_loss_cls: 0.0121  dn_loss_bbox: 0.0335  dn_loss_iou: 0.2799  d0.dn_loss_cls: 0.0446  d0.dn_loss_bbox: 0.0498  d0.dn_loss_iou: 0.3818  d1.dn_loss_cls: 0.0161  d1.dn_loss_bbox: 0.0360  d1.dn_loss_iou: 0.2978  d2.dn_loss_cls: 0.0127  d2.dn_loss_bbox: 0.0342  d2.dn_loss_iou: 0.2840  d3.dn_loss_cls: 0.0125  d3.dn_loss_bbox: 0.0335  d3.dn_loss_iou: 0.2799  d4.dn_loss_cls: 0.0120  d4.dn_loss_bbox: 0.0335  d4.dn_loss_iou: 0.2795  loss_num: 0.0010  d0.loss_num: 0.0012  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2024/10/31 07:11:47 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 07:11:47 - mmengine - INFO - Saving checkpoint at 3 epochs
2024/10/31 07:11:58 - mmengine - INFO - Epoch(val) [3][ 50/858]    eta: 0:01:11  time: 0.0889  data_time: 0.0026  memory: 9324  
2024/10/31 07:12:02 - mmengine - INFO - Epoch(val) [3][100/858]    eta: 0:01:06  time: 0.0864  data_time: 0.0019  memory: 3167  
2024/10/31 07:12:06 - mmengine - INFO - Epoch(val) [3][150/858]    eta: 0:01:01  time: 0.0863  data_time: 0.0019  memory: 3164  
2024/10/31 07:12:11 - mmengine - INFO - Epoch(val) [3][200/858]    eta: 0:00:57  time: 0.0866  data_time: 0.0020  memory: 3170  
2024/10/31 07:12:15 - mmengine - INFO - Epoch(val) [3][250/858]    eta: 0:00:52  time: 0.0864  data_time: 0.0019  memory: 3170  
2024/10/31 07:12:19 - mmengine - INFO - Epoch(val) [3][300/858]    eta: 0:00:48  time: 0.0863  data_time: 0.0020  memory: 3170  
2024/10/31 07:12:24 - mmengine - INFO - Epoch(val) [3][350/858]    eta: 0:00:44  time: 0.0863  data_time: 0.0019  memory: 3167  
2024/10/31 07:12:28 - mmengine - INFO - Epoch(val) [3][400/858]    eta: 0:00:39  time: 0.0868  data_time: 0.0020  memory: 3173  
2024/10/31 07:12:32 - mmengine - INFO - Epoch(val) [3][450/858]    eta: 0:00:35  time: 0.0867  data_time: 0.0020  memory: 3170  
2024/10/31 07:12:37 - mmengine - INFO - Epoch(val) [3][500/858]    eta: 0:00:31  time: 0.0872  data_time: 0.0020  memory: 3169  
2024/10/31 07:12:41 - mmengine - INFO - Epoch(val) [3][550/858]    eta: 0:00:26  time: 0.0872  data_time: 0.0020  memory: 3170  
2024/10/31 07:12:45 - mmengine - INFO - Epoch(val) [3][600/858]    eta: 0:00:22  time: 0.0873  data_time: 0.0020  memory: 3176  
2024/10/31 07:12:50 - mmengine - INFO - Epoch(val) [3][650/858]    eta: 0:00:18  time: 0.0876  data_time: 0.0020  memory: 3170  
2024/10/31 07:12:54 - mmengine - INFO - Epoch(val) [3][700/858]    eta: 0:00:13  time: 0.0877  data_time: 0.0020  memory: 3170  
2024/10/31 07:12:58 - mmengine - INFO - Epoch(val) [3][750/858]    eta: 0:00:09  time: 0.0875  data_time: 0.0021  memory: 3167  
2024/10/31 07:13:03 - mmengine - INFO - Epoch(val) [3][800/858]    eta: 0:00:05  time: 0.0876  data_time: 0.0021  memory: 3167  
2024/10/31 07:13:07 - mmengine - INFO - Epoch(val) [3][850/858]    eta: 0:00:00  time: 0.0861  data_time: 0.0020  memory: 3170  
2024/10/31 07:13:09 - mmengine - INFO - {'instance_F1_score': 0.2112644535621037, 'instance_acc': 0.12914916398978668, 'image_F1_score': 0.21496598639455783, 'image_acc': 0.15938228438228438}
2024/10/31 07:13:09 - mmengine - INFO - Epoch(val) [3][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.2113  grefcoco_val/refdrone/instance_acc: 0.1291  grefcoco_val/refdrone/image_F1_score: 0.2150  grefcoco_val/refdrone/image_acc: 0.1594  data_time: 0.0020  time: 0.0870
2024/10/31 07:14:01 - mmengine - INFO - Epoch(train) [4][ 50/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:27:06  time: 1.0233  data_time: 0.0093  memory: 9508  grad_norm: 51.2447  loss: 6.6978  loss_cls: 0.3177  loss_bbox: 0.0392  loss_iou: 0.3131  d0.loss_cls: 0.3457  d0.loss_bbox: 0.0464  d0.loss_iou: 0.3357  d1.loss_cls: 0.3272  d1.loss_bbox: 0.0418  d1.loss_iou: 0.3215  d2.loss_cls: 0.3251  d2.loss_bbox: 0.0400  d2.loss_iou: 0.3136  d3.loss_cls: 0.3209  d3.loss_bbox: 0.0399  d3.loss_iou: 0.3119  d4.loss_cls: 0.3185  d4.loss_bbox: 0.0392  d4.loss_iou: 0.3116  enc_loss_cls: 0.3572  enc_loss_bbox: 0.0544  enc_loss_iou: 0.3663  dn_loss_cls: 0.0113  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2379  d0.dn_loss_cls: 0.0308  d0.dn_loss_bbox: 0.0420  d0.dn_loss_iou: 0.3188  d1.dn_loss_cls: 0.0137  d1.dn_loss_bbox: 0.0316  d1.dn_loss_iou: 0.2524  d2.dn_loss_cls: 0.0110  d2.dn_loss_bbox: 0.0300  d2.dn_loss_iou: 0.2414  d3.dn_loss_cls: 0.0099  d3.dn_loss_bbox: 0.0296  d3.dn_loss_iou: 0.2386  d4.dn_loss_cls: 0.0105  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2377  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:14:52 - mmengine - INFO - Epoch(train) [4][100/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:26:15  time: 1.0328  data_time: 0.0116  memory: 9508  grad_norm: 54.0832  loss: 7.3198  loss_cls: 0.3515  loss_bbox: 0.0449  loss_iou: 0.3429  d0.loss_cls: 0.3819  d0.loss_bbox: 0.0497  d0.loss_iou: 0.3624  d1.loss_cls: 0.3626  d1.loss_bbox: 0.0471  d1.loss_iou: 0.3484  d2.loss_cls: 0.3539  d2.loss_bbox: 0.0448  d2.loss_iou: 0.3385  d3.loss_cls: 0.3510  d3.loss_bbox: 0.0451  d3.loss_iou: 0.3438  d4.loss_cls: 0.3503  d4.loss_bbox: 0.0449  d4.loss_iou: 0.3436  enc_loss_cls: 0.4098  enc_loss_bbox: 0.0494  enc_loss_iou: 0.3695  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0332  dn_loss_iou: 0.2637  d0.dn_loss_cls: 0.0359  d0.dn_loss_bbox: 0.0469  d0.dn_loss_iou: 0.3521  d1.dn_loss_cls: 0.0107  d1.dn_loss_bbox: 0.0358  d1.dn_loss_iou: 0.2828  d2.dn_loss_cls: 0.0064  d2.dn_loss_bbox: 0.0338  d2.dn_loss_iou: 0.2682  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0333  d3.dn_loss_iou: 0.2645  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0332  d4.dn_loss_iou: 0.2637  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:15:43 - mmengine - INFO - Epoch(train) [4][150/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:25:23  time: 1.0230  data_time: 0.0112  memory: 9505  grad_norm: 53.8786  loss: 8.2513  loss_cls: 0.4126  loss_bbox: 0.0489  loss_iou: 0.4207  d0.loss_cls: 0.4412  d0.loss_bbox: 0.0578  d0.loss_iou: 0.4444  d1.loss_cls: 0.4222  d1.loss_bbox: 0.0490  d1.loss_iou: 0.4229  d2.loss_cls: 0.4213  d2.loss_bbox: 0.0491  d2.loss_iou: 0.4258  d3.loss_cls: 0.4139  d3.loss_bbox: 0.0492  d3.loss_iou: 0.4226  d4.loss_cls: 0.4099  d4.loss_bbox: 0.0490  d4.loss_iou: 0.4209  enc_loss_cls: 0.4604  enc_loss_bbox: 0.0539  enc_loss_iou: 0.4500  dn_loss_cls: 0.0103  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2539  d0.dn_loss_cls: 0.0402  d0.dn_loss_bbox: 0.0406  d0.dn_loss_iou: 0.3303  d1.dn_loss_cls: 0.0145  d1.dn_loss_bbox: 0.0310  d1.dn_loss_iou: 0.2675  d2.dn_loss_cls: 0.0111  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2569  d3.dn_loss_cls: 0.0101  d3.dn_loss_bbox: 0.0290  d3.dn_loss_iou: 0.2544  d4.dn_loss_cls: 0.0103  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2540  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:16:35 - mmengine - INFO - Epoch(train) [4][200/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:24:31  time: 1.0299  data_time: 0.0102  memory: 9499  grad_norm: 50.9361  loss: 7.7076  loss_cls: 0.3361  loss_bbox: 0.0510  loss_iou: 0.3922  d0.loss_cls: 0.3689  d0.loss_bbox: 0.0502  d0.loss_iou: 0.3949  d1.loss_cls: 0.3483  d1.loss_bbox: 0.0538  d1.loss_iou: 0.3940  d2.loss_cls: 0.3451  d2.loss_bbox: 0.0506  d2.loss_iou: 0.3862  d3.loss_cls: 0.3382  d3.loss_bbox: 0.0511  d3.loss_iou: 0.3906  d4.loss_cls: 0.3373  d4.loss_bbox: 0.0516  d4.loss_iou: 0.3930  enc_loss_cls: 0.3819  enc_loss_bbox: 0.0529  enc_loss_iou: 0.4020  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0332  dn_loss_iou: 0.2886  d0.dn_loss_cls: 0.0395  d0.dn_loss_bbox: 0.0452  d0.dn_loss_iou: 0.3760  d1.dn_loss_cls: 0.0123  d1.dn_loss_bbox: 0.0356  d1.dn_loss_iou: 0.3064  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0336  d2.dn_loss_iou: 0.2915  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0332  d3.dn_loss_iou: 0.2886  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0332  d4.dn_loss_iou: 0.2885  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0007
2024/10/31 07:17:26 - mmengine - INFO - Epoch(train) [4][250/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:23:40  time: 1.0253  data_time: 0.0106  memory: 9505  grad_norm: 52.4254  loss: 7.2850  loss_cls: 0.3565  loss_bbox: 0.0440  loss_iou: 0.3430  d0.loss_cls: 0.3768  d0.loss_bbox: 0.0469  d0.loss_iou: 0.3575  d1.loss_cls: 0.3573  d1.loss_bbox: 0.0452  d1.loss_iou: 0.3453  d2.loss_cls: 0.3616  d2.loss_bbox: 0.0439  d2.loss_iou: 0.3426  d3.loss_cls: 0.3542  d3.loss_bbox: 0.0439  d3.loss_iou: 0.3405  d4.loss_cls: 0.3575  d4.loss_bbox: 0.0437  d4.loss_iou: 0.3398  enc_loss_cls: 0.3898  enc_loss_bbox: 0.0524  enc_loss_iou: 0.3783  dn_loss_cls: 0.0141  dn_loss_bbox: 0.0318  dn_loss_iou: 0.2562  d0.dn_loss_cls: 0.0395  d0.dn_loss_bbox: 0.0443  d0.dn_loss_iou: 0.3345  d1.dn_loss_cls: 0.0203  d1.dn_loss_bbox: 0.0341  d1.dn_loss_iou: 0.2719  d2.dn_loss_cls: 0.0162  d2.dn_loss_bbox: 0.0322  d2.dn_loss_iou: 0.2595  d3.dn_loss_cls: 0.0146  d3.dn_loss_bbox: 0.0318  d3.dn_loss_iou: 0.2562  d4.dn_loss_cls: 0.0142  d4.dn_loss_bbox: 0.0318  d4.dn_loss_iou: 0.2561  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/31 07:18:18 - mmengine - INFO - Epoch(train) [4][300/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:22:48  time: 1.0306  data_time: 0.0105  memory: 9514  grad_norm: 51.3083  loss: 7.6416  loss_cls: 0.3344  loss_bbox: 0.0506  loss_iou: 0.3933  d0.loss_cls: 0.3650  d0.loss_bbox: 0.0552  d0.loss_iou: 0.4192  d1.loss_cls: 0.3432  d1.loss_bbox: 0.0525  d1.loss_iou: 0.4055  d2.loss_cls: 0.3369  d2.loss_bbox: 0.0504  d2.loss_iou: 0.3943  d3.loss_cls: 0.3322  d3.loss_bbox: 0.0507  d3.loss_iou: 0.3960  d4.loss_cls: 0.3349  d4.loss_bbox: 0.0505  d4.loss_iou: 0.3918  enc_loss_cls: 0.3995  enc_loss_bbox: 0.0574  enc_loss_iou: 0.4243  dn_loss_cls: 0.0104  dn_loss_bbox: 0.0353  dn_loss_iou: 0.2617  d0.dn_loss_cls: 0.0396  d0.dn_loss_bbox: 0.0492  d0.dn_loss_iou: 0.3416  d1.dn_loss_cls: 0.0157  d1.dn_loss_bbox: 0.0380  d1.dn_loss_iou: 0.2792  d2.dn_loss_cls: 0.0120  d2.dn_loss_bbox: 0.0360  d2.dn_loss_iou: 0.2652  d3.dn_loss_cls: 0.0108  d3.dn_loss_bbox: 0.0354  d3.dn_loss_iou: 0.2620  d4.dn_loss_cls: 0.0104  d4.dn_loss_bbox: 0.0353  d4.dn_loss_iou: 0.2617  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:19:09 - mmengine - INFO - Epoch(train) [4][350/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:21:57  time: 1.0297  data_time: 0.0105  memory: 9514  grad_norm: 50.9974  loss: 7.1217  loss_cls: 0.3348  loss_bbox: 0.0434  loss_iou: 0.3573  d0.loss_cls: 0.3686  d0.loss_bbox: 0.0428  d0.loss_iou: 0.3532  d1.loss_cls: 0.3481  d1.loss_bbox: 0.0439  d1.loss_iou: 0.3554  d2.loss_cls: 0.3463  d2.loss_bbox: 0.0421  d2.loss_iou: 0.3544  d3.loss_cls: 0.3378  d3.loss_bbox: 0.0440  d3.loss_iou: 0.3568  d4.loss_cls: 0.3360  d4.loss_bbox: 0.0435  d4.loss_iou: 0.3586  enc_loss_cls: 0.3938  enc_loss_bbox: 0.0457  enc_loss_iou: 0.3747  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2485  d0.dn_loss_cls: 0.0323  d0.dn_loss_bbox: 0.0395  d0.dn_loss_iou: 0.3221  d1.dn_loss_cls: 0.0100  d1.dn_loss_bbox: 0.0312  d1.dn_loss_iou: 0.2612  d2.dn_loss_cls: 0.0069  d2.dn_loss_bbox: 0.0298  d2.dn_loss_iou: 0.2515  d3.dn_loss_cls: 0.0061  d3.dn_loss_bbox: 0.0295  d3.dn_loss_iou: 0.2486  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2485  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2024/10/31 07:20:01 - mmengine - INFO - Epoch(train) [4][400/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:21:05  time: 1.0304  data_time: 0.0108  memory: 9327  grad_norm: 50.5148  loss: 8.0069  loss_cls: 0.3716  loss_bbox: 0.0507  loss_iou: 0.4199  d0.loss_cls: 0.3966  d0.loss_bbox: 0.0508  d0.loss_iou: 0.4309  d1.loss_cls: 0.3831  d1.loss_bbox: 0.0484  d1.loss_iou: 0.4167  d2.loss_cls: 0.3873  d2.loss_bbox: 0.0463  d2.loss_iou: 0.4080  d3.loss_cls: 0.3759  d3.loss_bbox: 0.0470  d3.loss_iou: 0.4159  d4.loss_cls: 0.3738  d4.loss_bbox: 0.0475  d4.loss_iou: 0.4178  enc_loss_cls: 0.4166  enc_loss_bbox: 0.0551  enc_loss_iou: 0.4497  dn_loss_cls: 0.0097  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2670  d0.dn_loss_cls: 0.0456  d0.dn_loss_bbox: 0.0406  d0.dn_loss_iou: 0.3464  d1.dn_loss_cls: 0.0163  d1.dn_loss_bbox: 0.0312  d1.dn_loss_iou: 0.2828  d2.dn_loss_cls: 0.0115  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2700  d3.dn_loss_cls: 0.0103  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2673  d4.dn_loss_cls: 0.0098  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2669  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0009  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:20:52 - mmengine - INFO - Epoch(train) [4][450/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:20:14  time: 1.0353  data_time: 0.0106  memory: 9496  grad_norm: 44.2800  loss: 6.0161  loss_cls: 0.2726  loss_bbox: 0.0360  loss_iou: 0.2798  d0.loss_cls: 0.2910  d0.loss_bbox: 0.0369  d0.loss_iou: 0.2850  d1.loss_cls: 0.2732  d1.loss_bbox: 0.0379  d1.loss_iou: 0.2871  d2.loss_cls: 0.2702  d2.loss_bbox: 0.0361  d2.loss_iou: 0.2796  d3.loss_cls: 0.2680  d3.loss_bbox: 0.0361  d3.loss_iou: 0.2807  d4.loss_cls: 0.2720  d4.loss_bbox: 0.0360  d4.loss_iou: 0.2797  enc_loss_cls: 0.3202  enc_loss_bbox: 0.0375  enc_loss_iou: 0.2966  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0315  dn_loss_iou: 0.2422  d0.dn_loss_cls: 0.0306  d0.dn_loss_bbox: 0.0429  d0.dn_loss_iou: 0.3130  d1.dn_loss_cls: 0.0079  d1.dn_loss_bbox: 0.0337  d1.dn_loss_iou: 0.2558  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0318  d2.dn_loss_iou: 0.2444  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0315  d3.dn_loss_iou: 0.2425  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0315  d4.dn_loss_iou: 0.2421  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:21:44 - mmengine - INFO - Epoch(train) [4][500/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:19:22  time: 1.0317  data_time: 0.0111  memory: 9511  grad_norm: 53.0750  loss: 7.6104  loss_cls: 0.3262  loss_bbox: 0.0472  loss_iou: 0.4032  d0.loss_cls: 0.3586  d0.loss_bbox: 0.0484  d0.loss_iou: 0.4187  d1.loss_cls: 0.3330  d1.loss_bbox: 0.0484  d1.loss_iou: 0.4140  d2.loss_cls: 0.3231  d2.loss_bbox: 0.0476  d2.loss_iou: 0.4055  d3.loss_cls: 0.3231  d3.loss_bbox: 0.0475  d3.loss_iou: 0.4050  d4.loss_cls: 0.3269  d4.loss_bbox: 0.0472  d4.loss_iou: 0.4028  enc_loss_cls: 0.3727  enc_loss_bbox: 0.0526  enc_loss_iou: 0.4372  dn_loss_cls: 0.0065  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2750  d0.dn_loss_cls: 0.0348  d0.dn_loss_bbox: 0.0411  d0.dn_loss_iou: 0.3578  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0318  d1.dn_loss_iou: 0.2895  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2777  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2750  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2750  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/31 07:22:36 - mmengine - INFO - Epoch(train) [4][550/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:18:31  time: 1.0364  data_time: 0.0112  memory: 9511  grad_norm: 51.8297  loss: 8.1509  loss_cls: 0.3967  loss_bbox: 0.0495  loss_iou: 0.4093  d0.loss_cls: 0.4217  d0.loss_bbox: 0.0571  d0.loss_iou: 0.4317  d1.loss_cls: 0.4111  d1.loss_bbox: 0.0523  d1.loss_iou: 0.4201  d2.loss_cls: 0.3973  d2.loss_bbox: 0.0506  d2.loss_iou: 0.4124  d3.loss_cls: 0.3999  d3.loss_bbox: 0.0493  d3.loss_iou: 0.4077  d4.loss_cls: 0.3961  d4.loss_bbox: 0.0495  d4.loss_iou: 0.4092  enc_loss_cls: 0.4415  enc_loss_bbox: 0.0560  enc_loss_iou: 0.4385  dn_loss_cls: 0.0353  dn_loss_bbox: 0.0297  dn_loss_iou: 0.2489  d0.dn_loss_cls: 0.0502  d0.dn_loss_bbox: 0.0402  d0.dn_loss_iou: 0.3217  d1.dn_loss_cls: 0.0336  d1.dn_loss_bbox: 0.0316  d1.dn_loss_iou: 0.2643  d2.dn_loss_cls: 0.0307  d2.dn_loss_bbox: 0.0301  d2.dn_loss_iou: 0.2523  d3.dn_loss_cls: 0.0304  d3.dn_loss_bbox: 0.0297  d3.dn_loss_iou: 0.2494  d4.dn_loss_cls: 0.0327  d4.dn_loss_bbox: 0.0297  d4.dn_loss_iou: 0.2488  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2024/10/31 07:22:44 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 07:23:27 - mmengine - INFO - Epoch(train) [4][600/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:17:39  time: 1.0266  data_time: 0.0103  memory: 9333  grad_norm: 48.7270  loss: 7.3625  loss_cls: 0.3318  loss_bbox: 0.0465  loss_iou: 0.3594  d0.loss_cls: 0.3551  d0.loss_bbox: 0.0512  d0.loss_iou: 0.3791  d1.loss_cls: 0.3408  d1.loss_bbox: 0.0489  d1.loss_iou: 0.3619  d2.loss_cls: 0.3330  d2.loss_bbox: 0.0473  d2.loss_iou: 0.3594  d3.loss_cls: 0.3309  d3.loss_bbox: 0.0463  d3.loss_iou: 0.3581  d4.loss_cls: 0.3301  d4.loss_bbox: 0.0464  d4.loss_iou: 0.3576  enc_loss_cls: 0.3762  enc_loss_bbox: 0.0607  enc_loss_iou: 0.3969  dn_loss_cls: 0.0297  dn_loss_bbox: 0.0319  dn_loss_iou: 0.2518  d0.dn_loss_cls: 0.0585  d0.dn_loss_bbox: 0.0466  d0.dn_loss_iou: 0.3392  d1.dn_loss_cls: 0.0346  d1.dn_loss_bbox: 0.0344  d1.dn_loss_iou: 0.2683  d2.dn_loss_cls: 0.0288  d2.dn_loss_bbox: 0.0325  d2.dn_loss_iou: 0.2561  d3.dn_loss_cls: 0.0314  d3.dn_loss_bbox: 0.0320  d3.dn_loss_iou: 0.2524  d4.dn_loss_cls: 0.0290  d4.dn_loss_bbox: 0.0319  d4.dn_loss_iou: 0.2518  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0007
2024/10/31 07:24:19 - mmengine - INFO - Epoch(train) [4][650/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:16:48  time: 1.0283  data_time: 0.0100  memory: 9508  grad_norm: 47.7875  loss: 7.1364  loss_cls: 0.3057  loss_bbox: 0.0448  loss_iou: 0.3617  d0.loss_cls: 0.3440  d0.loss_bbox: 0.0475  d0.loss_iou: 0.3683  d1.loss_cls: 0.3179  d1.loss_bbox: 0.0483  d1.loss_iou: 0.3637  d2.loss_cls: 0.3156  d2.loss_bbox: 0.0448  d2.loss_iou: 0.3596  d3.loss_cls: 0.3098  d3.loss_bbox: 0.0449  d3.loss_iou: 0.3613  d4.loss_cls: 0.3058  d4.loss_bbox: 0.0448  d4.loss_iou: 0.3616  enc_loss_cls: 0.3633  enc_loss_bbox: 0.0498  enc_loss_iou: 0.3775  dn_loss_cls: 0.0103  dn_loss_bbox: 0.0325  dn_loss_iou: 0.2657  d0.dn_loss_cls: 0.0392  d0.dn_loss_bbox: 0.0444  d0.dn_loss_iou: 0.3412  d1.dn_loss_cls: 0.0148  d1.dn_loss_bbox: 0.0341  d1.dn_loss_iou: 0.2771  d2.dn_loss_cls: 0.0113  d2.dn_loss_bbox: 0.0331  d2.dn_loss_iou: 0.2691  d3.dn_loss_cls: 0.0108  d3.dn_loss_bbox: 0.0326  d3.dn_loss_iou: 0.2663  d4.dn_loss_cls: 0.0101  d4.dn_loss_bbox: 0.0325  d4.dn_loss_iou: 0.2657  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:25:10 - mmengine - INFO - Epoch(train) [4][700/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:15:56  time: 1.0354  data_time: 0.0104  memory: 9324  grad_norm: 51.0424  loss: 6.7216  loss_cls: 0.3361  loss_bbox: 0.0405  loss_iou: 0.3102  d0.loss_cls: 0.3512  d0.loss_bbox: 0.0384  d0.loss_iou: 0.3160  d1.loss_cls: 0.3422  d1.loss_bbox: 0.0414  d1.loss_iou: 0.3154  d2.loss_cls: 0.3394  d2.loss_bbox: 0.0377  d2.loss_iou: 0.3109  d3.loss_cls: 0.3400  d3.loss_bbox: 0.0399  d3.loss_iou: 0.3068  d4.loss_cls: 0.3378  d4.loss_bbox: 0.0405  d4.loss_iou: 0.3096  enc_loss_cls: 0.3805  enc_loss_bbox: 0.0399  enc_loss_iou: 0.3217  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0287  dn_loss_iou: 0.2477  d0.dn_loss_cls: 0.0321  d0.dn_loss_bbox: 0.0393  d0.dn_loss_iou: 0.3204  d1.dn_loss_cls: 0.0090  d1.dn_loss_bbox: 0.0302  d1.dn_loss_iou: 0.2597  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0289  d2.dn_loss_iou: 0.2498  d3.dn_loss_cls: 0.0059  d3.dn_loss_bbox: 0.0287  d3.dn_loss_iou: 0.2479  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0287  d4.dn_loss_iou: 0.2477  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0007  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2024/10/31 07:26:02 - mmengine - INFO - Epoch(train) [4][750/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:15:05  time: 1.0302  data_time: 0.0110  memory: 9511  grad_norm: 46.8134  loss: 7.2610  loss_cls: 0.3436  loss_bbox: 0.0437  loss_iou: 0.3603  d0.loss_cls: 0.3709  d0.loss_bbox: 0.0461  d0.loss_iou: 0.3751  d1.loss_cls: 0.3580  d1.loss_bbox: 0.0450  d1.loss_iou: 0.3635  d2.loss_cls: 0.3534  d2.loss_bbox: 0.0434  d2.loss_iou: 0.3594  d3.loss_cls: 0.3443  d3.loss_bbox: 0.0431  d3.loss_iou: 0.3595  d4.loss_cls: 0.3397  d4.loss_bbox: 0.0436  d4.loss_iou: 0.3605  enc_loss_cls: 0.3938  enc_loss_bbox: 0.0449  enc_loss_iou: 0.3780  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2525  d0.dn_loss_cls: 0.0406  d0.dn_loss_bbox: 0.0415  d0.dn_loss_iou: 0.3368  d1.dn_loss_cls: 0.0109  d1.dn_loss_bbox: 0.0311  d1.dn_loss_iou: 0.2673  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0297  d2.dn_loss_iou: 0.2561  d3.dn_loss_cls: 0.0069  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2524  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2524  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0009  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2024/10/31 07:26:53 - mmengine - INFO - Epoch(train) [4][800/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:14:13  time: 1.0261  data_time: 0.0115  memory: 9499  grad_norm: 48.7640  loss: 8.2208  loss_cls: 0.3601  loss_bbox: 0.0629  loss_iou: 0.4474  d0.loss_cls: 0.4075  d0.loss_bbox: 0.0669  d0.loss_iou: 0.4639  d1.loss_cls: 0.3785  d1.loss_bbox: 0.0669  d1.loss_iou: 0.4599  d2.loss_cls: 0.3695  d2.loss_bbox: 0.0623  d2.loss_iou: 0.4431  d3.loss_cls: 0.3646  d3.loss_bbox: 0.0625  d3.loss_iou: 0.4432  d4.loss_cls: 0.3598  d4.loss_bbox: 0.0629  d4.loss_iou: 0.4474  enc_loss_cls: 0.4125  enc_loss_bbox: 0.0663  enc_loss_iou: 0.4707  dn_loss_cls: 0.0095  dn_loss_bbox: 0.0304  dn_loss_iou: 0.2583  d0.dn_loss_cls: 0.0384  d0.dn_loss_bbox: 0.0424  d0.dn_loss_iou: 0.3392  d1.dn_loss_cls: 0.0155  d1.dn_loss_bbox: 0.0321  d1.dn_loss_iou: 0.2718  d2.dn_loss_cls: 0.0109  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2610  d3.dn_loss_cls: 0.0100  d3.dn_loss_bbox: 0.0305  d3.dn_loss_iou: 0.2586  d4.dn_loss_cls: 0.0096  d4.dn_loss_bbox: 0.0304  d4.dn_loss_iou: 0.2582  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0009  d2.loss_num: 0.0008  d3.loss_num: 0.0006  d4.loss_num: 0.0007
2024/10/31 07:27:08 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 07:27:08 - mmengine - INFO - Saving checkpoint at 4 epochs
2024/10/31 07:27:18 - mmengine - INFO - Epoch(val) [4][ 50/858]    eta: 0:01:12  time: 0.0903  data_time: 0.0027  memory: 9315  
2024/10/31 07:27:23 - mmengine - INFO - Epoch(val) [4][100/858]    eta: 0:01:07  time: 0.0874  data_time: 0.0021  memory: 3167  
2024/10/31 07:27:27 - mmengine - INFO - Epoch(val) [4][150/858]    eta: 0:01:02  time: 0.0872  data_time: 0.0020  memory: 3164  
2024/10/31 07:27:31 - mmengine - INFO - Epoch(val) [4][200/858]    eta: 0:00:57  time: 0.0873  data_time: 0.0021  memory: 3170  
2024/10/31 07:27:36 - mmengine - INFO - Epoch(val) [4][250/858]    eta: 0:00:53  time: 0.0872  data_time: 0.0021  memory: 3170  
2024/10/31 07:27:40 - mmengine - INFO - Epoch(val) [4][300/858]    eta: 0:00:49  time: 0.0875  data_time: 0.0021  memory: 3170  
2024/10/31 07:27:45 - mmengine - INFO - Epoch(val) [4][350/858]    eta: 0:00:44  time: 0.0873  data_time: 0.0020  memory: 3167  
2024/10/31 07:27:49 - mmengine - INFO - Epoch(val) [4][400/858]    eta: 0:00:40  time: 0.0877  data_time: 0.0021  memory: 3173  
2024/10/31 07:27:53 - mmengine - INFO - Epoch(val) [4][450/858]    eta: 0:00:35  time: 0.0875  data_time: 0.0020  memory: 3170  
2024/10/31 07:27:58 - mmengine - INFO - Epoch(val) [4][500/858]    eta: 0:00:31  time: 0.0871  data_time: 0.0020  memory: 3169  
2024/10/31 07:28:02 - mmengine - INFO - Epoch(val) [4][550/858]    eta: 0:00:26  time: 0.0870  data_time: 0.0020  memory: 3170  
2024/10/31 07:28:06 - mmengine - INFO - Epoch(val) [4][600/858]    eta: 0:00:22  time: 0.0872  data_time: 0.0020  memory: 3176  
2024/10/31 07:28:11 - mmengine - INFO - Epoch(val) [4][650/858]    eta: 0:00:18  time: 0.0877  data_time: 0.0020  memory: 3170  
2024/10/31 07:28:15 - mmengine - INFO - Epoch(val) [4][700/858]    eta: 0:00:13  time: 0.0874  data_time: 0.0020  memory: 3170  
2024/10/31 07:28:20 - mmengine - INFO - Epoch(val) [4][750/858]    eta: 0:00:09  time: 0.0878  data_time: 0.0020  memory: 3167  
2024/10/31 07:28:24 - mmengine - INFO - Epoch(val) [4][800/858]    eta: 0:00:05  time: 0.0870  data_time: 0.0020  memory: 3167  
2024/10/31 07:28:28 - mmengine - INFO - Epoch(val) [4][850/858]    eta: 0:00:00  time: 0.0856  data_time: 0.0019  memory: 3170  
2024/10/31 07:28:30 - mmengine - INFO - {'instance_F1_score': 0.26571407952659304, 'instance_acc': 0.16365280289330922, 'image_F1_score': 0.3003625064733299, 'image_acc': 0.2127039627039627}
2024/10/31 07:28:30 - mmengine - INFO - Epoch(val) [4][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.2657  grefcoco_val/refdrone/instance_acc: 0.1637  grefcoco_val/refdrone/image_F1_score: 0.3004  grefcoco_val/refdrone/image_acc: 0.2127  data_time: 0.0021  time: 0.0874
2024/10/31 07:29:22 - mmengine - INFO - Epoch(train) [5][ 50/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:13:07  time: 1.0261  data_time: 0.0094  memory: 9505  grad_norm: 57.1921  loss: 7.2336  loss_cls: 0.3526  loss_bbox: 0.0454  loss_iou: 0.3479  d0.loss_cls: 0.3710  d0.loss_bbox: 0.0501  d0.loss_iou: 0.3709  d1.loss_cls: 0.3600  d1.loss_bbox: 0.0473  d1.loss_iou: 0.3530  d2.loss_cls: 0.3536  d2.loss_bbox: 0.0452  d2.loss_iou: 0.3469  d3.loss_cls: 0.3502  d3.loss_bbox: 0.0467  d3.loss_iou: 0.3508  d4.loss_cls: 0.3494  d4.loss_bbox: 0.0468  d4.loss_iou: 0.3518  enc_loss_cls: 0.3987  enc_loss_bbox: 0.0513  enc_loss_iou: 0.3814  dn_loss_cls: 0.0152  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2416  d0.dn_loss_cls: 0.0440  d0.dn_loss_bbox: 0.0419  d0.dn_loss_iou: 0.3112  d1.dn_loss_cls: 0.0217  d1.dn_loss_bbox: 0.0320  d1.dn_loss_iou: 0.2540  d2.dn_loss_cls: 0.0158  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2438  d3.dn_loss_cls: 0.0165  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2419  d4.dn_loss_cls: 0.0156  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2415  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0006
2024/10/31 07:30:13 - mmengine - INFO - Epoch(train) [5][100/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:12:15  time: 1.0208  data_time: 0.0107  memory: 9505  grad_norm: 55.7555  loss: 7.6849  loss_cls: 0.3339  loss_bbox: 0.0494  loss_iou: 0.4003  d0.loss_cls: 0.3578  d0.loss_bbox: 0.0525  d0.loss_iou: 0.4199  d1.loss_cls: 0.3394  d1.loss_bbox: 0.0506  d1.loss_iou: 0.4086  d2.loss_cls: 0.3361  d2.loss_bbox: 0.0493  d2.loss_iou: 0.4007  d3.loss_cls: 0.3350  d3.loss_bbox: 0.0500  d3.loss_iou: 0.4027  d4.loss_cls: 0.3351  d4.loss_bbox: 0.0492  d4.loss_iou: 0.3983  enc_loss_cls: 0.3759  enc_loss_bbox: 0.0527  enc_loss_iou: 0.4304  dn_loss_cls: 0.0130  dn_loss_bbox: 0.0331  dn_loss_iou: 0.2719  d0.dn_loss_cls: 0.0434  d0.dn_loss_bbox: 0.0458  d0.dn_loss_iou: 0.3486  d1.dn_loss_cls: 0.0191  d1.dn_loss_bbox: 0.0344  d1.dn_loss_iou: 0.2841  d2.dn_loss_cls: 0.0155  d2.dn_loss_bbox: 0.0332  d2.dn_loss_iou: 0.2739  d3.dn_loss_cls: 0.0141  d3.dn_loss_bbox: 0.0330  d3.dn_loss_iou: 0.2717  d4.dn_loss_cls: 0.0130  d4.dn_loss_bbox: 0.0331  d4.dn_loss_iou: 0.2719  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:31:04 - mmengine - INFO - Epoch(train) [5][150/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:11:24  time: 1.0255  data_time: 0.0108  memory: 9499  grad_norm: 52.8978  loss: 6.3692  loss_cls: 0.2782  loss_bbox: 0.0400  loss_iou: 0.3162  d0.loss_cls: 0.3145  d0.loss_bbox: 0.0414  d0.loss_iou: 0.3287  d1.loss_cls: 0.2915  d1.loss_bbox: 0.0408  d1.loss_iou: 0.3217  d2.loss_cls: 0.2848  d2.loss_bbox: 0.0402  d2.loss_iou: 0.3171  d3.loss_cls: 0.2778  d3.loss_bbox: 0.0402  d3.loss_iou: 0.3177  d4.loss_cls: 0.2785  d4.loss_bbox: 0.0400  d4.loss_iou: 0.3165  enc_loss_cls: 0.3371  enc_loss_bbox: 0.0458  enc_loss_iou: 0.3433  dn_loss_cls: 0.0204  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2249  d0.dn_loss_cls: 0.0444  d0.dn_loss_bbox: 0.0381  d0.dn_loss_iou: 0.2911  d1.dn_loss_cls: 0.0222  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2373  d2.dn_loss_cls: 0.0192  d2.dn_loss_bbox: 0.0280  d2.dn_loss_iou: 0.2279  d3.dn_loss_cls: 0.0186  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2253  d4.dn_loss_cls: 0.0194  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2249  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2024/10/31 07:31:56 - mmengine - INFO - Epoch(train) [5][200/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:10:32  time: 1.0309  data_time: 0.0105  memory: 9511  grad_norm: 50.6805  loss: 7.6516  loss_cls: 0.3447  loss_bbox: 0.0500  loss_iou: 0.3852  d0.loss_cls: 0.3738  d0.loss_bbox: 0.0596  d0.loss_iou: 0.4086  d1.loss_cls: 0.3508  d1.loss_bbox: 0.0555  d1.loss_iou: 0.4000  d2.loss_cls: 0.3464  d2.loss_bbox: 0.0541  d2.loss_iou: 0.3937  d3.loss_cls: 0.3449  d3.loss_bbox: 0.0497  d3.loss_iou: 0.3848  d4.loss_cls: 0.3451  d4.loss_bbox: 0.0500  d4.loss_iou: 0.3852  enc_loss_cls: 0.4094  enc_loss_bbox: 0.0574  enc_loss_iou: 0.4210  dn_loss_cls: 0.0067  dn_loss_bbox: 0.0330  dn_loss_iou: 0.2634  d0.dn_loss_cls: 0.0379  d0.dn_loss_bbox: 0.0462  d0.dn_loss_iou: 0.3480  d1.dn_loss_cls: 0.0122  d1.dn_loss_bbox: 0.0354  d1.dn_loss_iou: 0.2795  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0334  d2.dn_loss_iou: 0.2659  d3.dn_loss_cls: 0.0073  d3.dn_loss_bbox: 0.0330  d3.dn_loss_iou: 0.2638  d4.dn_loss_cls: 0.0067  d4.dn_loss_bbox: 0.0330  d4.dn_loss_iou: 0.2634  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0009  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:32:47 - mmengine - INFO - Epoch(train) [5][250/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:09:41  time: 1.0248  data_time: 0.0111  memory: 9499  grad_norm: 49.2747  loss: 7.4719  loss_cls: 0.3407  loss_bbox: 0.0495  loss_iou: 0.3716  d0.loss_cls: 0.3626  d0.loss_bbox: 0.0527  d0.loss_iou: 0.3901  d1.loss_cls: 0.3596  d1.loss_bbox: 0.0492  d1.loss_iou: 0.3744  d2.loss_cls: 0.3489  d2.loss_bbox: 0.0510  d2.loss_iou: 0.3778  d3.loss_cls: 0.3451  d3.loss_bbox: 0.0502  d3.loss_iou: 0.3747  d4.loss_cls: 0.3406  d4.loss_bbox: 0.0492  d4.loss_iou: 0.3715  enc_loss_cls: 0.3949  enc_loss_bbox: 0.0545  enc_loss_iou: 0.3989  dn_loss_cls: 0.0077  dn_loss_bbox: 0.0321  dn_loss_iou: 0.2617  d0.dn_loss_cls: 0.0387  d0.dn_loss_bbox: 0.0445  d0.dn_loss_iou: 0.3428  d1.dn_loss_cls: 0.0119  d1.dn_loss_bbox: 0.0341  d1.dn_loss_iou: 0.2759  d2.dn_loss_cls: 0.0084  d2.dn_loss_bbox: 0.0326  d2.dn_loss_iou: 0.2650  d3.dn_loss_cls: 0.0079  d3.dn_loss_bbox: 0.0322  d3.dn_loss_iou: 0.2621  d4.dn_loss_cls: 0.0077  d4.dn_loss_bbox: 0.0321  d4.dn_loss_iou: 0.2618  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0007
2024/10/31 07:33:38 - mmengine - INFO - Epoch(train) [5][300/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:08:49  time: 1.0207  data_time: 0.0107  memory: 9511  grad_norm: 49.2591  loss: 6.6530  loss_cls: 0.2801  loss_bbox: 0.0423  loss_iou: 0.3196  d0.loss_cls: 0.2984  d0.loss_bbox: 0.0476  d0.loss_iou: 0.3439  d1.loss_cls: 0.2876  d1.loss_bbox: 0.0435  d1.loss_iou: 0.3245  d2.loss_cls: 0.2828  d2.loss_bbox: 0.0424  d2.loss_iou: 0.3192  d3.loss_cls: 0.2795  d3.loss_bbox: 0.0424  d3.loss_iou: 0.3215  d4.loss_cls: 0.2793  d4.loss_bbox: 0.0423  d4.loss_iou: 0.3199  enc_loss_cls: 0.3351  enc_loss_bbox: 0.0459  enc_loss_iou: 0.3406  dn_loss_cls: 0.0086  dn_loss_bbox: 0.0338  dn_loss_iou: 0.2665  d0.dn_loss_cls: 0.0413  d0.dn_loss_bbox: 0.0473  d0.dn_loss_iou: 0.3507  d1.dn_loss_cls: 0.0140  d1.dn_loss_bbox: 0.0358  d1.dn_loss_iou: 0.2799  d2.dn_loss_cls: 0.0102  d2.dn_loss_bbox: 0.0343  d2.dn_loss_iou: 0.2695  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0339  d3.dn_loss_iou: 0.2669  d4.dn_loss_cls: 0.0086  d4.dn_loss_bbox: 0.0338  d4.dn_loss_iou: 0.2664  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0008  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2024/10/31 07:34:29 - mmengine - INFO - Epoch(train) [5][350/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:07:58  time: 1.0274  data_time: 0.0114  memory: 9511  grad_norm: 50.9361  loss: 7.7338  loss_cls: 0.3423  loss_bbox: 0.0486  loss_iou: 0.4051  d0.loss_cls: 0.3717  d0.loss_bbox: 0.0588  d0.loss_iou: 0.4302  d1.loss_cls: 0.3625  d1.loss_bbox: 0.0509  d1.loss_iou: 0.4106  d2.loss_cls: 0.3571  d2.loss_bbox: 0.0493  d2.loss_iou: 0.4040  d3.loss_cls: 0.3491  d3.loss_bbox: 0.0492  d3.loss_iou: 0.4014  d4.loss_cls: 0.3412  d4.loss_bbox: 0.0498  d4.loss_iou: 0.4060  enc_loss_cls: 0.3917  enc_loss_bbox: 0.0544  enc_loss_iou: 0.4300  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0289  dn_loss_iou: 0.2710  d0.dn_loss_cls: 0.0333  d0.dn_loss_bbox: 0.0390  d0.dn_loss_iou: 0.3446  d1.dn_loss_cls: 0.0105  d1.dn_loss_bbox: 0.0302  d1.dn_loss_iou: 0.2818  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0290  d2.dn_loss_iou: 0.2723  d3.dn_loss_cls: 0.0065  d3.dn_loss_bbox: 0.0289  d3.dn_loss_iou: 0.2711  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0289  d4.dn_loss_iou: 0.2708  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2024/10/31 07:35:21 - mmengine - INFO - Epoch(train) [5][400/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:07:06  time: 1.0254  data_time: 0.0108  memory: 9502  grad_norm: 53.8875  loss: 6.8444  loss_cls: 0.2956  loss_bbox: 0.0456  loss_iou: 0.3164  d0.loss_cls: 0.3232  d0.loss_bbox: 0.0464  d0.loss_iou: 0.3355  d1.loss_cls: 0.3071  d1.loss_bbox: 0.0450  d1.loss_iou: 0.3146  d2.loss_cls: 0.3087  d2.loss_bbox: 0.0416  d2.loss_iou: 0.3056  d3.loss_cls: 0.3052  d3.loss_bbox: 0.0433  d3.loss_iou: 0.3105  d4.loss_cls: 0.2978  d4.loss_bbox: 0.0454  d4.loss_iou: 0.3141  enc_loss_cls: 0.3468  enc_loss_bbox: 0.0499  enc_loss_iou: 0.3511  dn_loss_cls: 0.0098  dn_loss_bbox: 0.0346  dn_loss_iou: 0.2767  d0.dn_loss_cls: 0.0430  d0.dn_loss_bbox: 0.0492  d0.dn_loss_iou: 0.3642  d1.dn_loss_cls: 0.0155  d1.dn_loss_bbox: 0.0368  d1.dn_loss_iou: 0.2931  d2.dn_loss_cls: 0.0113  d2.dn_loss_bbox: 0.0349  d2.dn_loss_iou: 0.2789  d3.dn_loss_cls: 0.0100  d3.dn_loss_bbox: 0.0346  d3.dn_loss_iou: 0.2766  d4.dn_loss_cls: 0.0098  d4.dn_loss_bbox: 0.0346  d4.dn_loss_iou: 0.2766  loss_num: 0.0008  d0.loss_num: 0.0010  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:36:12 - mmengine - INFO - Epoch(train) [5][450/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:06:14  time: 1.0293  data_time: 0.0113  memory: 9505  grad_norm: 53.0063  loss: 6.3391  loss_cls: 0.2989  loss_bbox: 0.0395  loss_iou: 0.2998  d0.loss_cls: 0.3188  d0.loss_bbox: 0.0403  d0.loss_iou: 0.3008  d1.loss_cls: 0.3055  d1.loss_bbox: 0.0392  d1.loss_iou: 0.2961  d2.loss_cls: 0.3036  d2.loss_bbox: 0.0385  d2.loss_iou: 0.2926  d3.loss_cls: 0.2990  d3.loss_bbox: 0.0387  d3.loss_iou: 0.2951  d4.loss_cls: 0.3008  d4.loss_bbox: 0.0388  d4.loss_iou: 0.2969  enc_loss_cls: 0.3410  enc_loss_bbox: 0.0428  enc_loss_iou: 0.3145  dn_loss_cls: 0.0071  dn_loss_bbox: 0.0326  dn_loss_iou: 0.2377  d0.dn_loss_cls: 0.0314  d0.dn_loss_bbox: 0.0440  d0.dn_loss_iou: 0.3107  d1.dn_loss_cls: 0.0105  d1.dn_loss_bbox: 0.0342  d1.dn_loss_iou: 0.2488  d2.dn_loss_cls: 0.0078  d2.dn_loss_bbox: 0.0329  d2.dn_loss_iou: 0.2399  d3.dn_loss_cls: 0.0073  d3.dn_loss_bbox: 0.0327  d3.dn_loss_iou: 0.2380  d4.dn_loss_cls: 0.0074  d4.dn_loss_bbox: 0.0326  d4.dn_loss_iou: 0.2377  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0009  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:37:03 - mmengine - INFO - Epoch(train) [5][500/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:05:23  time: 1.0181  data_time: 0.0110  memory: 9502  grad_norm: 48.6078  loss: 7.7284  loss_cls: 0.3243  loss_bbox: 0.0494  loss_iou: 0.4340  d0.loss_cls: 0.3558  d0.loss_bbox: 0.0518  d0.loss_iou: 0.4440  d1.loss_cls: 0.3424  d1.loss_bbox: 0.0499  d1.loss_iou: 0.4362  d2.loss_cls: 0.3312  d2.loss_bbox: 0.0498  d2.loss_iou: 0.4349  d3.loss_cls: 0.3277  d3.loss_bbox: 0.0496  d3.loss_iou: 0.4341  d4.loss_cls: 0.3248  d4.loss_bbox: 0.0496  d4.loss_iou: 0.4368  enc_loss_cls: 0.3784  enc_loss_bbox: 0.0535  enc_loss_iou: 0.4547  dn_loss_cls: 0.0184  dn_loss_bbox: 0.0280  dn_loss_iou: 0.2508  d0.dn_loss_cls: 0.0412  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.3248  d1.dn_loss_cls: 0.0234  d1.dn_loss_bbox: 0.0294  d1.dn_loss_iou: 0.2629  d2.dn_loss_cls: 0.0190  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2535  d3.dn_loss_cls: 0.0188  d3.dn_loss_bbox: 0.0280  d3.dn_loss_iou: 0.2505  d4.dn_loss_cls: 0.0184  d4.dn_loss_bbox: 0.0280  d4.dn_loss_iou: 0.2507  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2024/10/31 07:37:54 - mmengine - INFO - Epoch(train) [5][550/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:04:31  time: 1.0242  data_time: 0.0107  memory: 9499  grad_norm: 50.5049  loss: 7.4467  loss_cls: 0.3363  loss_bbox: 0.0496  loss_iou: 0.3690  d0.loss_cls: 0.3828  d0.loss_bbox: 0.0520  d0.loss_iou: 0.3880  d1.loss_cls: 0.3544  d1.loss_bbox: 0.0514  d1.loss_iou: 0.3763  d2.loss_cls: 0.3496  d2.loss_bbox: 0.0502  d2.loss_iou: 0.3717  d3.loss_cls: 0.3404  d3.loss_bbox: 0.0496  d3.loss_iou: 0.3690  d4.loss_cls: 0.3372  d4.loss_bbox: 0.0495  d4.loss_iou: 0.3685  enc_loss_cls: 0.4047  enc_loss_bbox: 0.0534  enc_loss_iou: 0.3982  dn_loss_cls: 0.0132  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2583  d0.dn_loss_cls: 0.0388  d0.dn_loss_bbox: 0.0403  d0.dn_loss_iou: 0.3292  d1.dn_loss_cls: 0.0181  d1.dn_loss_bbox: 0.0317  d1.dn_loss_iou: 0.2701  d2.dn_loss_cls: 0.0149  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2607  d3.dn_loss_cls: 0.0138  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2583  d4.dn_loss_cls: 0.0133  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2582  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0009  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:38:46 - mmengine - INFO - Epoch(train) [5][600/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:03:40  time: 1.0341  data_time: 0.0108  memory: 9508  grad_norm: 47.5992  loss: 7.9130  loss_cls: 0.3544  loss_bbox: 0.0494  loss_iou: 0.4279  d0.loss_cls: 0.3922  d0.loss_bbox: 0.0493  d0.loss_iou: 0.4255  d1.loss_cls: 0.3689  d1.loss_bbox: 0.0488  d1.loss_iou: 0.4210  d2.loss_cls: 0.3587  d2.loss_bbox: 0.0475  d2.loss_iou: 0.4235  d3.loss_cls: 0.3508  d3.loss_bbox: 0.0485  d3.loss_iou: 0.4227  d4.loss_cls: 0.3517  d4.loss_bbox: 0.0494  d4.loss_iou: 0.4277  enc_loss_cls: 0.4110  enc_loss_bbox: 0.0546  enc_loss_iou: 0.4526  dn_loss_cls: 0.0097  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2627  d0.dn_loss_cls: 0.0488  d0.dn_loss_bbox: 0.0409  d0.dn_loss_iou: 0.3456  d1.dn_loss_cls: 0.0165  d1.dn_loss_bbox: 0.0307  d1.dn_loss_iou: 0.2766  d2.dn_loss_cls: 0.0117  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2661  d3.dn_loss_cls: 0.0102  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2631  d4.dn_loss_cls: 0.0097  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2627  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:39:38 - mmengine - INFO - Epoch(train) [5][650/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:02:48  time: 1.0369  data_time: 0.0114  memory: 9514  grad_norm: 52.2214  loss: 6.9513  loss_cls: 0.3195  loss_bbox: 0.0409  loss_iou: 0.3376  d0.loss_cls: 0.3409  d0.loss_bbox: 0.0441  d0.loss_iou: 0.3558  d1.loss_cls: 0.3251  d1.loss_bbox: 0.0425  d1.loss_iou: 0.3454  d2.loss_cls: 0.3198  d2.loss_bbox: 0.0411  d2.loss_iou: 0.3393  d3.loss_cls: 0.3218  d3.loss_bbox: 0.0410  d3.loss_iou: 0.3380  d4.loss_cls: 0.3197  d4.loss_bbox: 0.0410  d4.loss_iou: 0.3394  enc_loss_cls: 0.3626  enc_loss_bbox: 0.0500  enc_loss_iou: 0.3750  dn_loss_cls: 0.0097  dn_loss_bbox: 0.0304  dn_loss_iou: 0.2535  d0.dn_loss_cls: 0.0393  d0.dn_loss_bbox: 0.0423  d0.dn_loss_iou: 0.3370  d1.dn_loss_cls: 0.0123  d1.dn_loss_bbox: 0.0320  d1.dn_loss_iou: 0.2668  d2.dn_loss_cls: 0.0097  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2554  d3.dn_loss_cls: 0.0092  d3.dn_loss_bbox: 0.0304  d3.dn_loss_iou: 0.2538  d4.dn_loss_cls: 0.0096  d4.dn_loss_bbox: 0.0304  d4.dn_loss_iou: 0.2536  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0007  d4.loss_num: 0.0008
2024/10/31 07:40:29 - mmengine - INFO - Epoch(train) [5][700/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:01:57  time: 1.0308  data_time: 0.0107  memory: 9502  grad_norm: 54.0374  loss: 6.6755  loss_cls: 0.3294  loss_bbox: 0.0402  loss_iou: 0.3030  d0.loss_cls: 0.3602  d0.loss_bbox: 0.0436  d0.loss_iou: 0.3164  d1.loss_cls: 0.3428  d1.loss_bbox: 0.0402  d1.loss_iou: 0.3024  d2.loss_cls: 0.3359  d2.loss_bbox: 0.0409  d2.loss_iou: 0.2990  d3.loss_cls: 0.3350  d3.loss_bbox: 0.0391  d3.loss_iou: 0.2961  d4.loss_cls: 0.3276  d4.loss_bbox: 0.0402  d4.loss_iou: 0.3029  enc_loss_cls: 0.3755  enc_loss_bbox: 0.0449  enc_loss_iou: 0.3222  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0335  dn_loss_iou: 0.2453  d0.dn_loss_cls: 0.0261  d0.dn_loss_bbox: 0.0461  d0.dn_loss_iou: 0.3177  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0354  d1.dn_loss_iou: 0.2550  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0339  d2.dn_loss_iou: 0.2481  d3.dn_loss_cls: 0.0064  d3.dn_loss_bbox: 0.0335  d3.dn_loss_iou: 0.2458  d4.dn_loss_cls: 0.0060  d4.dn_loss_bbox: 0.0335  d4.dn_loss_iou: 0.2451  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0007
2024/10/31 07:41:15 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 07:41:21 - mmengine - INFO - Epoch(train) [5][750/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:01:05  time: 1.0258  data_time: 0.0109  memory: 9499  grad_norm: 48.0835  loss: 6.3936  loss_cls: 0.2688  loss_bbox: 0.0426  loss_iou: 0.3036  d0.loss_cls: 0.2916  d0.loss_bbox: 0.0431  d0.loss_iou: 0.3129  d1.loss_cls: 0.2801  d1.loss_bbox: 0.0435  d1.loss_iou: 0.3114  d2.loss_cls: 0.2758  d2.loss_bbox: 0.0430  d2.loss_iou: 0.3068  d3.loss_cls: 0.2725  d3.loss_bbox: 0.0426  d3.loss_iou: 0.3039  d4.loss_cls: 0.2679  d4.loss_bbox: 0.0426  d4.loss_iou: 0.3035  enc_loss_cls: 0.3133  enc_loss_bbox: 0.0472  enc_loss_iou: 0.3318  dn_loss_cls: 0.0050  dn_loss_bbox: 0.0329  dn_loss_iou: 0.2612  d0.dn_loss_cls: 0.0325  d0.dn_loss_bbox: 0.0458  d0.dn_loss_iou: 0.3393  d1.dn_loss_cls: 0.0097  d1.dn_loss_bbox: 0.0354  d1.dn_loss_iou: 0.2763  d2.dn_loss_cls: 0.0068  d2.dn_loss_bbox: 0.0336  d2.dn_loss_iou: 0.2644  d3.dn_loss_cls: 0.0053  d3.dn_loss_bbox: 0.0329  d3.dn_loss_iou: 0.2614  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0329  d4.dn_loss_iou: 0.2610  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2024/10/31 07:42:12 - mmengine - INFO - Epoch(train) [5][800/814]  base_lr: 2.0000e-05 lr: 2.0000e-05  eta: 0:00:14  time: 1.0208  data_time: 0.0109  memory: 9330  grad_norm: 47.4162  loss: 7.3233  loss_cls: 0.3226  loss_bbox: 0.0440  loss_iou: 0.3764  d0.loss_cls: 0.3519  d0.loss_bbox: 0.0455  d0.loss_iou: 0.3852  d1.loss_cls: 0.3301  d1.loss_bbox: 0.0496  d1.loss_iou: 0.3880  d2.loss_cls: 0.3333  d2.loss_bbox: 0.0442  d2.loss_iou: 0.3731  d3.loss_cls: 0.3277  d3.loss_bbox: 0.0437  d3.loss_iou: 0.3753  d4.loss_cls: 0.3268  d4.loss_bbox: 0.0440  d4.loss_iou: 0.3762  enc_loss_cls: 0.3538  enc_loss_bbox: 0.0499  enc_loss_iou: 0.4017  dn_loss_cls: 0.0083  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2683  d0.dn_loss_cls: 0.0368  d0.dn_loss_bbox: 0.0402  d0.dn_loss_iou: 0.3398  d1.dn_loss_cls: 0.0127  d1.dn_loss_bbox: 0.0319  d1.dn_loss_iou: 0.2820  d2.dn_loss_cls: 0.0091  d2.dn_loss_bbox: 0.0306  d2.dn_loss_iou: 0.2715  d3.dn_loss_cls: 0.0084  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2688  d4.dn_loss_cls: 0.0083  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2684  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2024/10/31 07:42:26 - mmengine - INFO - Exp name: pretrain_num_grounding_dino_swin-t_finetune_8xb4_5e_refdrone_zero_31_v3_20241031_062636
2024/10/31 07:42:26 - mmengine - INFO - Saving checkpoint at 5 epochs
2024/10/31 07:42:37 - mmengine - INFO - Epoch(val) [5][ 50/858]    eta: 0:01:12  time: 0.0898  data_time: 0.0026  memory: 9117  
2024/10/31 07:42:41 - mmengine - INFO - Epoch(val) [5][100/858]    eta: 0:01:07  time: 0.0870  data_time: 0.0019  memory: 3167  
2024/10/31 07:42:45 - mmengine - INFO - Epoch(val) [5][150/858]    eta: 0:01:02  time: 0.0869  data_time: 0.0019  memory: 3164  
2024/10/31 07:42:50 - mmengine - INFO - Epoch(val) [5][200/858]    eta: 0:00:57  time: 0.0870  data_time: 0.0019  memory: 3170  
2024/10/31 07:42:54 - mmengine - INFO - Epoch(val) [5][250/858]    eta: 0:00:53  time: 0.0870  data_time: 0.0019  memory: 3170  
2024/10/31 07:42:58 - mmengine - INFO - Epoch(val) [5][300/858]    eta: 0:00:48  time: 0.0870  data_time: 0.0019  memory: 3170  
2024/10/31 07:43:03 - mmengine - INFO - Epoch(val) [5][350/858]    eta: 0:00:44  time: 0.0871  data_time: 0.0019  memory: 3167  
2024/10/31 07:43:07 - mmengine - INFO - Epoch(val) [5][400/858]    eta: 0:00:40  time: 0.0874  data_time: 0.0020  memory: 3173  
2024/10/31 07:43:11 - mmengine - INFO - Epoch(val) [5][450/858]    eta: 0:00:35  time: 0.0871  data_time: 0.0019  memory: 3170  
2024/10/31 07:43:16 - mmengine - INFO - Epoch(val) [5][500/858]    eta: 0:00:31  time: 0.0871  data_time: 0.0019  memory: 3169  
2024/10/31 07:43:20 - mmengine - INFO - Epoch(val) [5][550/858]    eta: 0:00:26  time: 0.0868  data_time: 0.0019  memory: 3170  
2024/10/31 07:43:25 - mmengine - INFO - Epoch(val) [5][600/858]    eta: 0:00:22  time: 0.0870  data_time: 0.0019  memory: 3176  
2024/10/31 07:43:29 - mmengine - INFO - Epoch(val) [5][650/858]    eta: 0:00:18  time: 0.0871  data_time: 0.0019  memory: 3170  
2024/10/31 07:43:33 - mmengine - INFO - Epoch(val) [5][700/858]    eta: 0:00:13  time: 0.0872  data_time: 0.0019  memory: 3170  
2024/10/31 07:43:38 - mmengine - INFO - Epoch(val) [5][750/858]    eta: 0:00:09  time: 0.0868  data_time: 0.0019  memory: 3167  
2024/10/31 07:43:42 - mmengine - INFO - Epoch(val) [5][800/858]    eta: 0:00:05  time: 0.0865  data_time: 0.0019  memory: 3167  
2024/10/31 07:43:46 - mmengine - INFO - Epoch(val) [5][850/858]    eta: 0:00:00  time: 0.0855  data_time: 0.0019  memory: 3170  
2024/10/31 07:43:49 - mmengine - INFO - {'instance_F1_score': 0.33342560553633216, 'instance_acc': 0.20977930921322505, 'image_F1_score': 0.32448979591836735, 'image_acc': 0.22843822843822845}
2024/10/31 07:43:49 - mmengine - INFO - Epoch(val) [5][858/858]    grefcoco_val/refdrone/instance_F1_score: 0.3334  grefcoco_val/refdrone/instance_acc: 0.2098  grefcoco_val/refdrone/image_F1_score: 0.3245  grefcoco_val/refdrone/image_acc: 0.2284  data_time: 0.0019  time: 0.0871
