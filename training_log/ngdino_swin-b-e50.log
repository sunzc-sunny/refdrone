2025/10/29 02:18:10 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1667664206
    GPU 0,1,2,3,4,5,6,7: NVIDIA A100-SXM4-40GB
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: x86_64-linux-gnu-gcc (Ubuntu 11.3.0-1ubuntu1~22.04.1) 11.3.0
    PyTorch: 2.1.0+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.1.1 (Git Hash 64f6bcbcbab628e96f33a62c3e975f8535a7bde4)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.16.0+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 1667664206
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 8
------------------------------------------------------------

2025/10/29 02:18:12 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=32, enable=True)
backend_args = None
coco_od_dataset = dict(
    ann_file='o365v1_train_odvg.json',
    backend_args=None,
    data_prefix=dict(img='train/'),
    data_root='data/objects365v1/',
    filter_cfg=dict(filter_empty_gt=False),
    label_map_file='o365v1_label_map.json',
    pipeline=[
        dict(backend_args=None, type='LoadImageFromFile'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(prob=0.5, type='RandomFlip'),
        dict(
            transforms=[
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
                [
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                400,
                                4200,
                            ),
                            (
                                500,
                                4200,
                            ),
                            (
                                600,
                                4200,
                            ),
                        ],
                        type='RandomChoiceResize'),
                    dict(
                        allow_negative_crop=True,
                        crop_size=(
                            384,
                            600,
                        ),
                        crop_type='absolute_range',
                        type='RandomCrop'),
                    dict(
                        keep_ratio=True,
                        scales=[
                            (
                                480,
                                1333,
                            ),
                            (
                                512,
                                1333,
                            ),
                            (
                                544,
                                1333,
                            ),
                            (
                                576,
                                1333,
                            ),
                            (
                                608,
                                1333,
                            ),
                            (
                                640,
                                1333,
                            ),
                            (
                                672,
                                1333,
                            ),
                            (
                                704,
                                1333,
                            ),
                            (
                                736,
                                1333,
                            ),
                            (
                                768,
                                1333,
                            ),
                            (
                                800,
                                1333,
                            ),
                        ],
                        type='RandomChoiceResize'),
                ],
            ],
            type='RandomChoice'),
        dict(min_gt_bbox_wh=(
            0.01,
            0.01,
        ), type='FilterAnnotations'),
        dict(
            max_tokens=256,
            num_sample_negative=85,
            tokenizer_name=
            '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
            type='RandomSamplingNegPos'),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'flip',
                'flip_direction',
                'text',
                'custom_entities',
                'tokens_positive',
                'dataset_mode',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    type='ODVGDataset')
data_root = 'data/objects365v1/'
dataset_prefixes = [
    'grefcoco_val',
]
dataset_type = 'ODVGDataset'
datasets = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        backend_args=None,
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        test_mode=True,
        type='MDETRStyleRefCocoDataset'),
]
default_hooks = dict(
    checkpoint=dict(interval=1, type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='GroundingVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
lang_model_name = '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased'
launcher = 'pytorch'
load_from = '/mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-b_numbranch_pretrain_32.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
max_epochs = 5
metrics = [
    dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        iou_thrs=0.5,
        metric='bbox',
        thresh_f1=1.0,
        thresh_score=0.7,
        type='RefDroneMetric'),
]
model = dict(
    as_two_stage=True,
    backbone=dict(
        attn_drop_rate=0.0,
        convert_weights=True,
        depths=[
            2,
            2,
            18,
            2,
        ],
        drop_path_rate=0.3,
        drop_rate=0.0,
        embed_dims=128,
        frozen_stages=-1,
        init_cfg=dict(
            checkpoint=
            '/mnt/public/usr/sunzhichao/mmdetection/swin_base_patch4_window12_384_22k.pth',
            type='Pretrained'),
        mlp_ratio=4,
        num_heads=[
            4,
            8,
            16,
            32,
        ],
        out_indices=(
            1,
            2,
            3,
        ),
        patch_norm=True,
        pretrain_img_size=384,
        qk_scale=None,
        qkv_bias=True,
        type='SwinTransformer',
        window_size=12,
        with_cp=True),
    bbox_head=dict(
        contrastive_cfg=dict(bias=True, log_scale='auto', max_text_len=256),
        loss_bbox=dict(loss_weight=5.0, type='L1Loss'),
        loss_cls=dict(
            alpha=0.25,
            gamma=2.0,
            loss_weight=1.0,
            type='FocalLoss',
            use_sigmoid=True),
        num_classes=256,
        sync_cls_avg_factor=True,
        type='GroundingDINOHeadNumv11'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        mean=[
            123.675,
            116.28,
            103.53,
        ],
        pad_mask=False,
        std=[
            58.395,
            57.12,
            57.375,
        ],
        type='DetDataPreprocessor'),
    decoder=dict(
        layer_cfg=dict(
            cross_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            cross_attn_text_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8),
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=8)),
        num_layers=6,
        post_norm_cfg=None,
        return_intermediate=True),
    dn_cfg=dict(
        box_noise_scale=1.0,
        group_cfg=dict(dynamic=True, num_dn_queries=100, num_groups=None),
        label_noise_scale=0.5),
    encoder=dict(
        fusion_layer_cfg=dict(
            embed_dim=1024,
            init_values=0.0001,
            l_dim=256,
            num_heads=4,
            v_dim=256),
        layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=2048, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_levels=4)),
        num_cp=6,
        num_layers=6,
        text_layer_cfg=dict(
            ffn_cfg=dict(
                embed_dims=256, feedforward_channels=1024, ffn_drop=0.0),
            self_attn_cfg=dict(dropout=0.0, embed_dims=256, num_heads=4))),
    language_model=dict(
        add_pooling_layer=False,
        max_tokens=256,
        name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        pad_to_max=False,
        special_tokens_list=[
            '[CLS]',
            '[SEP]',
            '.',
            '?',
        ],
        type='BertModel',
        use_sub_sentence_represent=True),
    neck=dict(
        act_cfg=None,
        bias=True,
        in_channels=[
            256,
            512,
            1024,
        ],
        kernel_size=1,
        norm_cfg=dict(num_groups=32, type='GN'),
        num_outs=4,
        out_channels=256,
        type='ChannelMapper'),
    num_queries=900,
    positional_encoding=dict(
        normalize=True, num_feats=128, offset=0.0, temperature=20),
    test_cfg=dict(max_per_img=300),
    train_cfg=dict(
        assigner=dict(
            match_costs=[
                dict(type='BinaryFocalLossCost', weight=2.0),
                dict(box_format='xywh', type='BBoxL1Cost', weight=5.0),
                dict(iou_mode='giou', type='IoUCost', weight=2.0),
            ],
            type='HungarianAssigner')),
    type='NumGroundingDINO',
    use_autocast=True,
    with_box_refine=True)
optim_wrapper = dict(
    clip_grad=dict(max_norm=0.1, norm_type=2),
    optimizer=dict(lr=0.0001, type='AdamW', weight_decay=0.0001),
    paramwise_cfg=dict(
        custom_keys=dict(
            absolute_pos_embed=dict(decay_mult=0.0),
            backbone=dict(lr_mult=0.1))),
    type='OptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        end=5,
        gamma=0.1,
        milestones=[
            4,
        ],
        type='MultiStepLR'),
]
pretrained = '/mnt/public/usr/sunzhichao/mmdetection/swin_base_patch4_window12_384_22k.pth'
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=1,
    dataset=dict(
        ann_file=
        '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
        backend_args=None,
        data_prefix=dict(
            img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
        data_root='data/coco/',
        pipeline=[
            dict(
                backend_args=None,
                imdecode_backend='pillow',
                type='LoadImageFromFile'),
            dict(
                backend='pillow',
                keep_ratio=True,
                scale=(
                    800,
                    1333,
                ),
                type='FixScaleResize'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                    'text',
                    'custom_entities',
                    'tokens_positive',
                ),
                type='PackDetInputs'),
        ],
        return_classes=True,
        test_mode=True,
        type='MDETRStyleRefCocoDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
test_pipeline = [
    dict(
        backend_args=None, imdecode_backend='pillow',
        type='LoadImageFromFile'),
    dict(
        backend='pillow',
        keep_ratio=True,
        scale=(
            800,
            1333,
        ),
        type='FixScaleResize'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'text',
            'custom_entities',
            'tokens_positive',
        ),
        type='PackDetInputs'),
]
train_cfg = dict(max_epochs=5, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=8,
    dataset=dict(
        dataset=dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_train7_vg.json',
            data_prefix=dict(
                img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
            filter_cfg=dict(filter_empty_gt=False),
            pipeline=[
                dict(backend_args=None, type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True),
                dict(prob=0.0, type='RandomFlip'),
                dict(
                    keep_ratio=True,
                    scales=[
                        (
                            480,
                            1333,
                        ),
                        (
                            512,
                            1333,
                        ),
                        (
                            544,
                            1333,
                        ),
                        (
                            576,
                            1333,
                        ),
                        (
                            608,
                            1333,
                        ),
                        (
                            640,
                            1333,
                        ),
                        (
                            672,
                            1333,
                        ),
                        (
                            704,
                            1333,
                        ),
                        (
                            736,
                            1333,
                        ),
                        (
                            768,
                            1333,
                        ),
                        (
                            800,
                            1333,
                        ),
                    ],
                    type='RandomChoiceResize'),
                dict(
                    max_tokens=256,
                    num_sample_negative=85,
                    tokenizer_name=
                    '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
                    type='RandomSamplingNegPos'),
                dict(
                    meta_keys=(
                        'img_id',
                        'img_path',
                        'ori_shape',
                        'img_shape',
                        'scale_factor',
                        'flip',
                        'flip_direction',
                        'text',
                        'custom_entities',
                        'tokens_positive',
                        'dataset_mode',
                    ),
                    type='PackDetInputs'),
            ],
            return_classes=True,
            type='ODVGDataset'),
        times=10,
        type='RepeatDataset'),
    num_workers=8,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(prob=0.0, type='RandomFlip'),
    dict(
        keep_ratio=True,
        scales=[
            (
                480,
                1333,
            ),
            (
                512,
                1333,
            ),
            (
                544,
                1333,
            ),
            (
                576,
                1333,
            ),
            (
                608,
                1333,
            ),
            (
                640,
                1333,
            ),
            (
                672,
                1333,
            ),
            (
                704,
                1333,
            ),
            (
                736,
                1333,
            ),
            (
                768,
                1333,
            ),
            (
                800,
                1333,
            ),
        ],
        type='RandomChoiceResize'),
    dict(
        max_tokens=256,
        num_sample_negative=85,
        tokenizer_name=
        '/mnt/public/usr/sunzhichao/hf_hub/models--google-bert--bert-base-uncased',
        type='RandomSamplingNegPos'),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
            'flip',
            'flip_direction',
            'text',
            'custom_entities',
            'tokens_positive',
            'dataset_mode',
        ),
        type='PackDetInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=1,
    dataset=dict(
        datasets=[
            dict(
                ann_file=
                '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
                backend_args=None,
                data_prefix=dict(
                    img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
                pipeline=[
                    dict(
                        backend_args=None,
                        imdecode_backend='pillow',
                        type='LoadImageFromFile'),
                    dict(
                        backend='pillow',
                        keep_ratio=True,
                        scale=(
                            800,
                            1333,
                        ),
                        type='FixScaleResize'),
                    dict(type='LoadAnnotations', with_bbox=True),
                    dict(
                        meta_keys=(
                            'img_id',
                            'img_path',
                            'ori_shape',
                            'img_shape',
                            'scale_factor',
                            'text',
                            'custom_entities',
                            'tokens_positive',
                        ),
                        type='PackDetInputs'),
                ],
                return_classes=True,
                test_mode=True,
                type='MDETRStyleRefCocoDataset'),
        ],
        type='ConcatDataset'),
    drop_last=False,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_dataset_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    backend_args=None,
    data_prefix=dict(img='/mnt/public/usr/sunzhichao/VisDrone2019/all_image/'),
    pipeline=[
        dict(
            backend_args=None,
            imdecode_backend='pillow',
            type='LoadImageFromFile'),
        dict(
            backend='pillow',
            keep_ratio=True,
            scale=(
                800,
                1333,
            ),
            type='FixScaleResize'),
        dict(type='LoadAnnotations', with_bbox=True),
        dict(
            meta_keys=(
                'img_id',
                'img_path',
                'ori_shape',
                'img_shape',
                'scale_factor',
                'text',
                'custom_entities',
                'tokens_positive',
            ),
            type='PackDetInputs'),
    ],
    return_classes=True,
    test_mode=True,
    type='MDETRStyleRefCocoDataset')
val_evaluator = dict(
    dataset_prefixes=[
        'grefcoco_val',
    ],
    metrics=[
        dict(
            ann_file=
            '/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
            iou_thrs=0.5,
            metric='bbox',
            thresh_f1=1.0,
            thresh_score=0.7,
            type='RefDroneMetric'),
    ],
    type='MultiDatasetsEvaluator')
val_evaluator_all_val = dict(
    ann_file='/mnt/public/usr/sunzhichao/RefDrone/finetune_RefDrone_test6.json',
    iou_thrs=0.5,
    metric='bbox',
    thresh_f1=1.0,
    thresh_score=0.7,
    type='RefDroneMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31'

2025/10/29 02:18:17 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) GroundingVisualizationHook         
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.word_embeddings.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.position_embeddings.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.embeddings.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight:lr_mult=0.1
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr=1e-05
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:weight_decay=0.0001
2025/10/29 02:18:20 - mmengine - INFO - paramwise_options -- language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias:lr_mult=0.1
2025/10/29 02:18:21 - mmengine - INFO - LR is set based on batch size of 32 and the current batch size is 64. Scaling the original LR by 2.0.
2025/10/29 02:18:23 - mmengine - INFO - Loads checkpoint by local backend from path: /mnt/public/usr/sunzhichao/mmdetection/swin_base_patch4_window12_384_22k.pth
Name of parameter - Initialization information

level_embed - torch.Size([4, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.patch_embed.projection.weight - torch.Size([128, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([1024, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

backbone.norm3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.conv.weight - torch.Size([256, 256, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.conv.weight - torch.Size([256, 1024, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.extra_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

neck.extra_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.0.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.1.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.2.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.3.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.5.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.cls_branches.6.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.0.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.0.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.1.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.1.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.2.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.2.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.3.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.3.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.4.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.4.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.5.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.5.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.reg_branches.6.4.weight - torch.Size([4, 256]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.reg_branches.6.4.bias - torch.Size([4]): 
Initialized by user-defined `init_weights` in GroundingDINOHeadNumv11  

bbox_head.single_value_branches.0.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.0.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.1.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.2.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.3.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.4.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.weight - torch.Size([1, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

bbox_head.single_value_branches.5.4.bias - torch.Size([1]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.text_layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.0.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.1.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.2.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.3.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.4.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_v - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.gamma_l - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_v.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.layer_norm_l.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_v_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.weight - torch.Size([1024, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.values_l_proj.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_v_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.weight - torch.Size([256, 1024]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

encoder.fusion_layers.5.attn.out_l_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.0.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.1.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.2.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.3.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.4.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.number_embed - torch.Size([50, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn_text.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.sampling_offsets.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.weight - torch.Size([128, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.attention_weights.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_weight - torch.Size([768, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.cross_atten_num.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.layers.5.norms.3.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.weight - torch.Size([256, 512]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.ref_point_head.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

decoder.norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

query_embedding.weight - torch.Size([900, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_fc.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

memory_trans_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.word_embeddings.weight - torch.Size([30522, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.position_embeddings.weight - torch.Size([512, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.token_type_embeddings.weight - torch.Size([2, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.embeddings.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.0.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.1.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.2.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.3.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.4.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.5.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.6.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.7.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.8.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.9.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.10.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.query.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.key.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.self.value.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.attention.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.intermediate.dense.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.dense.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

language_model.language_backbone.body.model.encoder.layer.11.output.LayerNorm.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.weight - torch.Size([256, 768]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

text_feat_map.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  

dn_query_generator.label_embedding.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of NumGroundingDINO  
2025/10/29 02:18:25 - mmengine - INFO - Load checkpoint from /mnt/public/usr/sunzhichao/mmdetection/grounding_dino_swin-b_numbranch_pretrain_32.pth
2025/10/29 02:18:25 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/10/29 02:18:25 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/10/29 02:18:25 - mmengine - INFO - Checkpoints will be saved to /mnt/public/usr/sunzhichao/mmdetection/work_dirs/epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31.
2025/10/29 02:20:19 - mmengine - INFO - Epoch(train) [1][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 6:24:10  time: 2.2766  data_time: 0.0316  memory: 17568  grad_norm: 35.0310  loss: 11.7581  loss_cls: 0.6627  loss_bbox: 0.0752  loss_iou: 0.5075  d0.loss_cls: 0.6814  d0.loss_bbox: 0.0769  d0.loss_iou: 0.5002  d1.loss_cls: 0.6653  d1.loss_bbox: 0.0748  d1.loss_iou: 0.5041  d2.loss_cls: 0.6559  d2.loss_bbox: 0.0790  d2.loss_iou: 0.5159  d3.loss_cls: 0.6554  d3.loss_bbox: 0.0787  d3.loss_iou: 0.5176  d4.loss_cls: 0.6600  d4.loss_bbox: 0.0781  d4.loss_iou: 0.5120  enc_loss_cls: 0.6792  enc_loss_bbox: 0.0728  enc_loss_iou: 0.4954  dn_loss_cls: 0.0805  dn_loss_bbox: 0.0442  dn_loss_iou: 0.3458  d0.dn_loss_cls: 0.1141  d0.dn_loss_bbox: 0.0607  d0.dn_loss_iou: 0.4403  d1.dn_loss_cls: 0.0875  d1.dn_loss_bbox: 0.0466  d1.dn_loss_iou: 0.3615  d2.dn_loss_cls: 0.0835  d2.dn_loss_bbox: 0.0449  d2.dn_loss_iou: 0.3497  d3.dn_loss_cls: 0.0781  d3.dn_loss_bbox: 0.0444  d3.dn_loss_iou: 0.3468  d4.dn_loss_cls: 0.0792  d4.dn_loss_bbox: 0.0442  d4.dn_loss_iou: 0.3456  loss_num: 0.0021  d0.loss_num: 0.0021  d1.loss_num: 0.0021  d2.loss_num: 0.0020  d3.loss_num: 0.0021  d4.loss_num: 0.0021
2025/10/29 02:22:06 - mmengine - INFO - Epoch(train) [1][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 6:09:54  time: 2.1292  data_time: 0.0175  memory: 17574  grad_norm: 37.6066  loss: 9.4162  loss_cls: 0.5292  loss_bbox: 0.0579  loss_iou: 0.4070  d0.loss_cls: 0.5508  d0.loss_bbox: 0.0599  d0.loss_iou: 0.4189  d1.loss_cls: 0.5365  d1.loss_bbox: 0.0602  d1.loss_iou: 0.4167  d2.loss_cls: 0.5328  d2.loss_bbox: 0.0586  d2.loss_iou: 0.4104  d3.loss_cls: 0.5308  d3.loss_bbox: 0.0583  d3.loss_iou: 0.4086  d4.loss_cls: 0.5303  d4.loss_bbox: 0.0579  d4.loss_iou: 0.4059  enc_loss_cls: 0.5505  enc_loss_bbox: 0.0617  enc_loss_iou: 0.4292  dn_loss_cls: 0.0282  dn_loss_bbox: 0.0358  dn_loss_iou: 0.2939  d0.dn_loss_cls: 0.0614  d0.dn_loss_bbox: 0.0524  d0.dn_loss_iou: 0.3917  d1.dn_loss_cls: 0.0357  d1.dn_loss_bbox: 0.0384  d1.dn_loss_iou: 0.3123  d2.dn_loss_cls: 0.0318  d2.dn_loss_bbox: 0.0363  d2.dn_loss_iou: 0.2981  d3.dn_loss_cls: 0.0293  d3.dn_loss_bbox: 0.0359  d3.dn_loss_iou: 0.2943  d4.dn_loss_cls: 0.0288  d4.dn_loss_bbox: 0.0358  d4.dn_loss_iou: 0.2937  loss_num: 0.0017  d0.loss_num: 0.0019  d1.loss_num: 0.0017  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0016
2025/10/29 02:23:52 - mmengine - INFO - Epoch(train) [1][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 6:03:33  time: 2.1220  data_time: 0.0177  memory: 17582  grad_norm: 45.4737  loss: 8.9660  loss_cls: 0.4782  loss_bbox: 0.0522  loss_iou: 0.4003  d0.loss_cls: 0.5074  d0.loss_bbox: 0.0555  d0.loss_iou: 0.4111  d1.loss_cls: 0.4925  d1.loss_bbox: 0.0545  d1.loss_iou: 0.4055  d2.loss_cls: 0.4855  d2.loss_bbox: 0.0529  d2.loss_iou: 0.4005  d3.loss_cls: 0.4807  d3.loss_bbox: 0.0532  d3.loss_iou: 0.4022  d4.loss_cls: 0.4791  d4.loss_bbox: 0.0525  d4.loss_iou: 0.4009  enc_loss_cls: 0.5015  enc_loss_bbox: 0.0588  enc_loss_iou: 0.4311  dn_loss_cls: 0.0143  dn_loss_bbox: 0.0362  dn_loss_iou: 0.3012  d0.dn_loss_cls: 0.0539  d0.dn_loss_bbox: 0.0525  d0.dn_loss_iou: 0.4053  d1.dn_loss_cls: 0.0206  d1.dn_loss_bbox: 0.0388  d1.dn_loss_iou: 0.3191  d2.dn_loss_cls: 0.0152  d2.dn_loss_bbox: 0.0366  d2.dn_loss_iou: 0.3040  d3.dn_loss_cls: 0.0141  d3.dn_loss_bbox: 0.0362  d3.dn_loss_iou: 0.3011  d4.dn_loss_cls: 0.0141  d4.dn_loss_bbox: 0.0362  d4.dn_loss_iou: 0.3009  loss_num: 0.0015  d0.loss_num: 0.0017  d1.loss_num: 0.0016  d2.loss_num: 0.0016  d3.loss_num: 0.0016  d4.loss_num: 0.0015
2025/10/29 02:25:39 - mmengine - INFO - Epoch(train) [1][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 6:00:16  time: 2.1402  data_time: 0.0167  memory: 17568  grad_norm: 41.9029  loss: 9.3685  loss_cls: 0.5345  loss_bbox: 0.0550  loss_iou: 0.4157  d0.loss_cls: 0.5578  d0.loss_bbox: 0.0554  d0.loss_iou: 0.4217  d1.loss_cls: 0.5447  d1.loss_bbox: 0.0559  d1.loss_iou: 0.4223  d2.loss_cls: 0.5393  d2.loss_bbox: 0.0573  d2.loss_iou: 0.4217  d3.loss_cls: 0.5357  d3.loss_bbox: 0.0557  d3.loss_iou: 0.4182  d4.loss_cls: 0.5369  d4.loss_bbox: 0.0550  d4.loss_iou: 0.4156  enc_loss_cls: 0.5605  enc_loss_bbox: 0.0614  enc_loss_iou: 0.4362  dn_loss_cls: 0.0202  dn_loss_bbox: 0.0333  dn_loss_iou: 0.2848  d0.dn_loss_cls: 0.0542  d0.dn_loss_bbox: 0.0474  d0.dn_loss_iou: 0.3798  d1.dn_loss_cls: 0.0246  d1.dn_loss_bbox: 0.0357  d1.dn_loss_iou: 0.3021  d2.dn_loss_cls: 0.0207  d2.dn_loss_bbox: 0.0339  d2.dn_loss_iou: 0.2890  d3.dn_loss_cls: 0.0201  d3.dn_loss_bbox: 0.0334  d3.dn_loss_iou: 0.2853  d4.dn_loss_cls: 0.0198  d4.dn_loss_bbox: 0.0333  d4.dn_loss_iou: 0.2846  loss_num: 0.0017  d0.loss_num: 0.0017  d1.loss_num: 0.0016  d2.loss_num: 0.0017  d3.loss_num: 0.0017  d4.loss_num: 0.0017
2025/10/29 02:27:25 - mmengine - INFO - Epoch(train) [1][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:57:14  time: 2.1304  data_time: 0.0176  memory: 17573  grad_norm: 42.6739  loss: 8.8144  loss_cls: 0.4594  loss_bbox: 0.0612  loss_iou: 0.3903  d0.loss_cls: 0.4885  d0.loss_bbox: 0.0612  d0.loss_iou: 0.3995  d1.loss_cls: 0.4719  d1.loss_bbox: 0.0615  d1.loss_iou: 0.3957  d2.loss_cls: 0.4622  d2.loss_bbox: 0.0611  d2.loss_iou: 0.3929  d3.loss_cls: 0.4626  d3.loss_bbox: 0.0590  d3.loss_iou: 0.3894  d4.loss_cls: 0.4603  d4.loss_bbox: 0.0613  d4.loss_iou: 0.3909  enc_loss_cls: 0.4978  enc_loss_bbox: 0.0578  enc_loss_iou: 0.4087  dn_loss_cls: 0.0152  dn_loss_bbox: 0.0393  dn_loss_iou: 0.2996  d0.dn_loss_cls: 0.0539  d0.dn_loss_bbox: 0.0565  d0.dn_loss_iou: 0.3973  d1.dn_loss_cls: 0.0228  d1.dn_loss_bbox: 0.0421  d1.dn_loss_iou: 0.3178  d2.dn_loss_cls: 0.0171  d2.dn_loss_bbox: 0.0397  d2.dn_loss_iou: 0.3025  d3.dn_loss_cls: 0.0157  d3.dn_loss_bbox: 0.0393  d3.dn_loss_iou: 0.2995  d4.dn_loss_cls: 0.0152  d4.dn_loss_bbox: 0.0393  d4.dn_loss_iou: 0.2993  loss_num: 0.0015  d0.loss_num: 0.0017  d1.loss_num: 0.0015  d2.loss_num: 0.0014  d3.loss_num: 0.0015  d4.loss_num: 0.0015
2025/10/29 02:29:12 - mmengine - INFO - Epoch(train) [1][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:55:02  time: 2.1449  data_time: 0.0169  memory: 17599  grad_norm: 37.5667  loss: 8.3778  loss_cls: 0.4431  loss_bbox: 0.0544  loss_iou: 0.3568  d0.loss_cls: 0.4671  d0.loss_bbox: 0.0584  d0.loss_iou: 0.3724  d1.loss_cls: 0.4528  d1.loss_bbox: 0.0573  d1.loss_iou: 0.3661  d2.loss_cls: 0.4449  d2.loss_bbox: 0.0576  d2.loss_iou: 0.3630  d3.loss_cls: 0.4445  d3.loss_bbox: 0.0551  d3.loss_iou: 0.3577  d4.loss_cls: 0.4413  d4.loss_bbox: 0.0553  d4.loss_iou: 0.3592  enc_loss_cls: 0.4746  enc_loss_bbox: 0.0605  enc_loss_iou: 0.3858  dn_loss_cls: 0.0127  dn_loss_bbox: 0.0371  dn_loss_iou: 0.2907  d0.dn_loss_cls: 0.0563  d0.dn_loss_bbox: 0.0539  d0.dn_loss_iou: 0.3930  d1.dn_loss_cls: 0.0188  d1.dn_loss_bbox: 0.0399  d1.dn_loss_iou: 0.3097  d2.dn_loss_cls: 0.0145  d2.dn_loss_bbox: 0.0377  d2.dn_loss_iou: 0.2947  d3.dn_loss_cls: 0.0132  d3.dn_loss_bbox: 0.0372  d3.dn_loss_iou: 0.2913  d4.dn_loss_cls: 0.0127  d4.dn_loss_bbox: 0.0371  d4.dn_loss_iou: 0.2906  loss_num: 0.0014  d0.loss_num: 0.0016  d1.loss_num: 0.0015  d2.loss_num: 0.0014  d3.loss_num: 0.0015  d4.loss_num: 0.0015
2025/10/29 02:30:58 - mmengine - INFO - Epoch(train) [1][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:52:21  time: 2.1190  data_time: 0.0171  memory: 17568  grad_norm: 44.4066  loss: 8.2939  loss_cls: 0.4307  loss_bbox: 0.0436  loss_iou: 0.3857  d0.loss_cls: 0.4603  d0.loss_bbox: 0.0455  d0.loss_iou: 0.3935  d1.loss_cls: 0.4425  d1.loss_bbox: 0.0454  d1.loss_iou: 0.3913  d2.loss_cls: 0.4384  d2.loss_bbox: 0.0441  d2.loss_iou: 0.3874  d3.loss_cls: 0.4347  d3.loss_bbox: 0.0440  d3.loss_iou: 0.3847  d4.loss_cls: 0.4336  d4.loss_bbox: 0.0433  d4.loss_iou: 0.3843  enc_loss_cls: 0.4676  enc_loss_bbox: 0.0476  enc_loss_iou: 0.4063  dn_loss_cls: 0.0157  dn_loss_bbox: 0.0311  dn_loss_iou: 0.2795  d0.dn_loss_cls: 0.0488  d0.dn_loss_bbox: 0.0445  d0.dn_loss_iou: 0.3749  d1.dn_loss_cls: 0.0221  d1.dn_loss_bbox: 0.0331  d1.dn_loss_iou: 0.2963  d2.dn_loss_cls: 0.0182  d2.dn_loss_bbox: 0.0314  d2.dn_loss_iou: 0.2830  d3.dn_loss_cls: 0.0165  d3.dn_loss_bbox: 0.0311  d3.dn_loss_iou: 0.2796  d4.dn_loss_cls: 0.0158  d4.dn_loss_bbox: 0.0311  d4.dn_loss_iou: 0.2791  loss_num: 0.0013  d0.loss_num: 0.0015  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2025/10/29 02:32:45 - mmengine - INFO - Epoch(train) [1][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:49:57  time: 2.1222  data_time: 0.0182  memory: 17568  grad_norm: 37.8335  loss: 8.6002  loss_cls: 0.4408  loss_bbox: 0.0502  loss_iou: 0.3945  d0.loss_cls: 0.4633  d0.loss_bbox: 0.0543  d0.loss_iou: 0.4120  d1.loss_cls: 0.4588  d1.loss_bbox: 0.0499  d1.loss_iou: 0.3979  d2.loss_cls: 0.4562  d2.loss_bbox: 0.0498  d2.loss_iou: 0.3932  d3.loss_cls: 0.4469  d3.loss_bbox: 0.0506  d3.loss_iou: 0.3955  d4.loss_cls: 0.4430  d4.loss_bbox: 0.0503  d4.loss_iou: 0.3947  enc_loss_cls: 0.4794  enc_loss_bbox: 0.0535  enc_loss_iou: 0.4210  dn_loss_cls: 0.0179  dn_loss_bbox: 0.0348  dn_loss_iou: 0.2887  d0.dn_loss_cls: 0.0538  d0.dn_loss_bbox: 0.0506  d0.dn_loss_iou: 0.3895  d1.dn_loss_cls: 0.0252  d1.dn_loss_bbox: 0.0373  d1.dn_loss_iou: 0.3069  d2.dn_loss_cls: 0.0195  d2.dn_loss_bbox: 0.0355  d2.dn_loss_iou: 0.2933  d3.dn_loss_cls: 0.0181  d3.dn_loss_bbox: 0.0349  d3.dn_loss_iou: 0.2893  d4.dn_loss_cls: 0.0179  d4.dn_loss_bbox: 0.0348  d4.dn_loss_iou: 0.2886  loss_num: 0.0012  d0.loss_num: 0.0014  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2025/10/29 02:34:32 - mmengine - INFO - Epoch(train) [1][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:48:08  time: 2.1471  data_time: 0.0173  memory: 17574  grad_norm: 46.0464  loss: 9.0995  loss_cls: 0.4827  loss_bbox: 0.0587  loss_iou: 0.4381  d0.loss_cls: 0.5082  d0.loss_bbox: 0.0627  d0.loss_iou: 0.4494  d1.loss_cls: 0.4925  d1.loss_bbox: 0.0604  d1.loss_iou: 0.4465  d2.loss_cls: 0.4864  d2.loss_bbox: 0.0590  d2.loss_iou: 0.4403  d3.loss_cls: 0.4841  d3.loss_bbox: 0.0588  d3.loss_iou: 0.4400  d4.loss_cls: 0.4831  d4.loss_bbox: 0.0587  d4.loss_iou: 0.4393  enc_loss_cls: 0.5253  enc_loss_bbox: 0.0631  enc_loss_iou: 0.4590  dn_loss_cls: 0.0109  dn_loss_bbox: 0.0332  dn_loss_iou: 0.2769  d0.dn_loss_cls: 0.0455  d0.dn_loss_bbox: 0.0470  d0.dn_loss_iou: 0.3677  d1.dn_loss_cls: 0.0169  d1.dn_loss_bbox: 0.0353  d1.dn_loss_iou: 0.2927  d2.dn_loss_cls: 0.0126  d2.dn_loss_bbox: 0.0338  d2.dn_loss_iou: 0.2808  d3.dn_loss_cls: 0.0111  d3.dn_loss_bbox: 0.0333  d3.dn_loss_iou: 0.2773  d4.dn_loss_cls: 0.0110  d4.dn_loss_bbox: 0.0332  d4.dn_loss_iou: 0.2765  loss_num: 0.0012  d0.loss_num: 0.0013  d1.loss_num: 0.0012  d2.loss_num: 0.0012  d3.loss_num: 0.0012  d4.loss_num: 0.0012
2025/10/29 02:36:18 - mmengine - INFO - Epoch(train) [1][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:45:51  time: 2.1171  data_time: 0.0168  memory: 17579  grad_norm: 41.1171  loss: 8.5918  loss_cls: 0.4535  loss_bbox: 0.0525  loss_iou: 0.4037  d0.loss_cls: 0.4797  d0.loss_bbox: 0.0564  d0.loss_iou: 0.4213  d1.loss_cls: 0.4645  d1.loss_bbox: 0.0537  d1.loss_iou: 0.4102  d2.loss_cls: 0.4628  d2.loss_bbox: 0.0525  d2.loss_iou: 0.4039  d3.loss_cls: 0.4582  d3.loss_bbox: 0.0525  d3.loss_iou: 0.4026  d4.loss_cls: 0.4533  d4.loss_bbox: 0.0526  d4.loss_iou: 0.4039  enc_loss_cls: 0.4848  enc_loss_bbox: 0.0588  enc_loss_iou: 0.4360  dn_loss_cls: 0.0114  dn_loss_bbox: 0.0330  dn_loss_iou: 0.2711  d0.dn_loss_cls: 0.0447  d0.dn_loss_bbox: 0.0485  d0.dn_loss_iou: 0.3652  d1.dn_loss_cls: 0.0181  d1.dn_loss_bbox: 0.0354  d1.dn_loss_iou: 0.2870  d2.dn_loss_cls: 0.0132  d2.dn_loss_bbox: 0.0335  d2.dn_loss_iou: 0.2740  d3.dn_loss_cls: 0.0120  d3.dn_loss_bbox: 0.0331  d3.dn_loss_iou: 0.2710  d4.dn_loss_cls: 0.0115  d4.dn_loss_bbox: 0.0330  d4.dn_loss_iou: 0.2707  loss_num: 0.0014  d0.loss_num: 0.0015  d1.loss_num: 0.0014  d2.loss_num: 0.0014  d3.loss_num: 0.0014  d4.loss_num: 0.0014
2025/10/29 02:38:04 - mmengine - INFO - Epoch(train) [1][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:43:44  time: 2.1218  data_time: 0.0165  memory: 17568  grad_norm: 40.8282  loss: 8.6105  loss_cls: 0.4584  loss_bbox: 0.0498  loss_iou: 0.4018  d0.loss_cls: 0.4888  d0.loss_bbox: 0.0557  d0.loss_iou: 0.4239  d1.loss_cls: 0.4720  d1.loss_bbox: 0.0542  d1.loss_iou: 0.4175  d2.loss_cls: 0.4672  d2.loss_bbox: 0.0517  d2.loss_iou: 0.4074  d3.loss_cls: 0.4670  d3.loss_bbox: 0.0499  d3.loss_iou: 0.4032  d4.loss_cls: 0.4601  d4.loss_bbox: 0.0499  d4.loss_iou: 0.4034  enc_loss_cls: 0.4986  enc_loss_bbox: 0.0565  enc_loss_iou: 0.4366  dn_loss_cls: 0.0111  dn_loss_bbox: 0.0320  dn_loss_iou: 0.2682  d0.dn_loss_cls: 0.0437  d0.dn_loss_bbox: 0.0450  d0.dn_loss_iou: 0.3553  d1.dn_loss_cls: 0.0173  d1.dn_loss_bbox: 0.0340  d1.dn_loss_iou: 0.2842  d2.dn_loss_cls: 0.0126  d2.dn_loss_bbox: 0.0323  d2.dn_loss_iou: 0.2713  d3.dn_loss_cls: 0.0116  d3.dn_loss_bbox: 0.0320  d3.dn_loss_iou: 0.2687  d4.dn_loss_cls: 0.0112  d4.dn_loss_bbox: 0.0319  d4.dn_loss_iou: 0.2679  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2025/10/29 02:39:51 - mmengine - INFO - Epoch(train) [1][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:41:54  time: 2.1401  data_time: 0.0181  memory: 17580  grad_norm: 36.1296  loss: 8.6147  loss_cls: 0.4457  loss_bbox: 0.0551  loss_iou: 0.3865  d0.loss_cls: 0.4741  d0.loss_bbox: 0.0629  d0.loss_iou: 0.4052  d1.loss_cls: 0.4573  d1.loss_bbox: 0.0610  d1.loss_iou: 0.3984  d2.loss_cls: 0.4524  d2.loss_bbox: 0.0579  d2.loss_iou: 0.3921  d3.loss_cls: 0.4487  d3.loss_bbox: 0.0564  d3.loss_iou: 0.3891  d4.loss_cls: 0.4439  d4.loss_bbox: 0.0557  d4.loss_iou: 0.3912  enc_loss_cls: 0.4857  enc_loss_bbox: 0.0611  enc_loss_iou: 0.4145  dn_loss_cls: 0.0091  dn_loss_bbox: 0.0355  dn_loss_iou: 0.2947  d0.dn_loss_cls: 0.0446  d0.dn_loss_bbox: 0.0508  d0.dn_loss_iou: 0.3921  d1.dn_loss_cls: 0.0143  d1.dn_loss_bbox: 0.0380  d1.dn_loss_iou: 0.3114  d2.dn_loss_cls: 0.0096  d2.dn_loss_bbox: 0.0359  d2.dn_loss_iou: 0.2975  d3.dn_loss_cls: 0.0090  d3.dn_loss_bbox: 0.0356  d3.dn_loss_iou: 0.2948  d4.dn_loss_cls: 0.0089  d4.dn_loss_bbox: 0.0355  d4.dn_loss_iou: 0.2945  loss_num: 0.0013  d0.loss_num: 0.0014  d1.loss_num: 0.0013  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2025/10/29 02:41:37 - mmengine - INFO - Epoch(train) [1][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:39:54  time: 2.1244  data_time: 0.0173  memory: 17568  grad_norm: 39.6878  loss: 7.5270  loss_cls: 0.3743  loss_bbox: 0.0414  loss_iou: 0.3436  d0.loss_cls: 0.4043  d0.loss_bbox: 0.0433  d0.loss_iou: 0.3519  d1.loss_cls: 0.3874  d1.loss_bbox: 0.0430  d1.loss_iou: 0.3492  d2.loss_cls: 0.3792  d2.loss_bbox: 0.0416  d2.loss_iou: 0.3435  d3.loss_cls: 0.3765  d3.loss_bbox: 0.0413  d3.loss_iou: 0.3434  d4.loss_cls: 0.3743  d4.loss_bbox: 0.0414  d4.loss_iou: 0.3433  enc_loss_cls: 0.4035  enc_loss_bbox: 0.0472  enc_loss_iou: 0.3749  dn_loss_cls: 0.0206  dn_loss_bbox: 0.0312  dn_loss_iou: 0.2681  d0.dn_loss_cls: 0.0486  d0.dn_loss_bbox: 0.0430  d0.dn_loss_iou: 0.3512  d1.dn_loss_cls: 0.0274  d1.dn_loss_bbox: 0.0329  d1.dn_loss_iou: 0.2810  d2.dn_loss_cls: 0.0233  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2713  d3.dn_loss_cls: 0.0209  d3.dn_loss_bbox: 0.0313  d3.dn_loss_iou: 0.2684  d4.dn_loss_cls: 0.0210  d4.dn_loss_bbox: 0.0312  d4.dn_loss_iou: 0.2680  loss_num: 0.0012  d0.loss_num: 0.0014  d1.loss_num: 0.0012  d2.loss_num: 0.0013  d3.loss_num: 0.0013  d4.loss_num: 0.0013
2025/10/29 02:43:24 - mmengine - INFO - Epoch(train) [1][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:38:11  time: 2.1469  data_time: 0.0173  memory: 17574  grad_norm: 37.7295  loss: 7.9555  loss_cls: 0.4000  loss_bbox: 0.0472  loss_iou: 0.3740  d0.loss_cls: 0.4197  d0.loss_bbox: 0.0540  d0.loss_iou: 0.4005  d1.loss_cls: 0.4067  d1.loss_bbox: 0.0507  d1.loss_iou: 0.3848  d2.loss_cls: 0.4011  d2.loss_bbox: 0.0503  d2.loss_iou: 0.3807  d3.loss_cls: 0.3983  d3.loss_bbox: 0.0491  d3.loss_iou: 0.3769  d4.loss_cls: 0.3982  d4.loss_bbox: 0.0480  d4.loss_iou: 0.3755  enc_loss_cls: 0.4342  enc_loss_bbox: 0.0574  enc_loss_iou: 0.4151  dn_loss_cls: 0.0114  dn_loss_bbox: 0.0313  dn_loss_iou: 0.2692  d0.dn_loss_cls: 0.0408  d0.dn_loss_bbox: 0.0442  d0.dn_loss_iou: 0.3562  d1.dn_loss_cls: 0.0160  d1.dn_loss_bbox: 0.0331  d1.dn_loss_iou: 0.2840  d2.dn_loss_cls: 0.0125  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2719  d3.dn_loss_cls: 0.0117  d3.dn_loss_bbox: 0.0313  d3.dn_loss_iou: 0.2693  d4.dn_loss_cls: 0.0115  d4.dn_loss_bbox: 0.0313  d4.dn_loss_iou: 0.2689  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0011
2025/10/29 02:45:10 - mmengine - INFO - Epoch(train) [1][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:36:11  time: 2.1209  data_time: 0.0171  memory: 17595  grad_norm: 41.6570  loss: 8.7728  loss_cls: 0.4282  loss_bbox: 0.0514  loss_iou: 0.4509  d0.loss_cls: 0.4668  d0.loss_bbox: 0.0584  d0.loss_iou: 0.4739  d1.loss_cls: 0.4526  d1.loss_bbox: 0.0540  d1.loss_iou: 0.4574  d2.loss_cls: 0.4450  d2.loss_bbox: 0.0501  d2.loss_iou: 0.4472  d3.loss_cls: 0.4373  d3.loss_bbox: 0.0508  d3.loss_iou: 0.4476  d4.loss_cls: 0.4306  d4.loss_bbox: 0.0518  d4.loss_iou: 0.4507  enc_loss_cls: 0.4706  enc_loss_bbox: 0.0641  enc_loss_iou: 0.4888  dn_loss_cls: 0.0108  dn_loss_bbox: 0.0301  dn_loss_iou: 0.2719  d0.dn_loss_cls: 0.0433  d0.dn_loss_bbox: 0.0433  d0.dn_loss_iou: 0.3572  d1.dn_loss_cls: 0.0181  d1.dn_loss_bbox: 0.0323  d1.dn_loss_iou: 0.2863  d2.dn_loss_cls: 0.0130  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2748  d3.dn_loss_cls: 0.0113  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2722  d4.dn_loss_cls: 0.0108  d4.dn_loss_bbox: 0.0301  d4.dn_loss_iou: 0.2718  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0011  d4.loss_num: 0.0010
2025/10/29 02:46:57 - mmengine - INFO - Epoch(train) [1][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:34:12  time: 2.1207  data_time: 0.0173  memory: 17581  grad_norm: 39.7861  loss: 8.0006  loss_cls: 0.3896  loss_bbox: 0.0428  loss_iou: 0.3810  d0.loss_cls: 0.4183  d0.loss_bbox: 0.0437  d0.loss_iou: 0.3905  d1.loss_cls: 0.4036  d1.loss_bbox: 0.0450  d1.loss_iou: 0.3869  d2.loss_cls: 0.3972  d2.loss_bbox: 0.0427  d2.loss_iou: 0.3784  d3.loss_cls: 0.3923  d3.loss_bbox: 0.0431  d3.loss_iou: 0.3795  d4.loss_cls: 0.3897  d4.loss_bbox: 0.0427  d4.loss_iou: 0.3803  enc_loss_cls: 0.4278  enc_loss_bbox: 0.0481  enc_loss_iou: 0.4092  dn_loss_cls: 0.0138  dn_loss_bbox: 0.0320  dn_loss_iou: 0.2863  d0.dn_loss_cls: 0.0438  d0.dn_loss_bbox: 0.0465  d0.dn_loss_iou: 0.3827  d1.dn_loss_cls: 0.0185  d1.dn_loss_bbox: 0.0343  d1.dn_loss_iou: 0.3030  d2.dn_loss_cls: 0.0148  d2.dn_loss_bbox: 0.0324  d2.dn_loss_iou: 0.2895  d3.dn_loss_cls: 0.0143  d3.dn_loss_bbox: 0.0321  d3.dn_loss_iou: 0.2864  d4.dn_loss_cls: 0.0139  d4.dn_loss_bbox: 0.0320  d4.dn_loss_iou: 0.2861  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2025/10/29 02:48:43 - mmengine - INFO - Epoch(train) [1][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:32:26  time: 2.1394  data_time: 0.0170  memory: 17568  grad_norm: 38.3059  loss: 7.6608  loss_cls: 0.3694  loss_bbox: 0.0448  loss_iou: 0.3689  d0.loss_cls: 0.3905  d0.loss_bbox: 0.0484  d0.loss_iou: 0.3832  d1.loss_cls: 0.3830  d1.loss_bbox: 0.0461  d1.loss_iou: 0.3716  d2.loss_cls: 0.3753  d2.loss_bbox: 0.0457  d2.loss_iou: 0.3695  d3.loss_cls: 0.3728  d3.loss_bbox: 0.0450  d3.loss_iou: 0.3686  d4.loss_cls: 0.3712  d4.loss_bbox: 0.0451  d4.loss_iou: 0.3689  enc_loss_cls: 0.4139  enc_loss_bbox: 0.0487  enc_loss_iou: 0.3933  dn_loss_cls: 0.0110  dn_loss_bbox: 0.0315  dn_loss_iou: 0.2691  d0.dn_loss_cls: 0.0416  d0.dn_loss_bbox: 0.0454  d0.dn_loss_iou: 0.3596  d1.dn_loss_cls: 0.0167  d1.dn_loss_bbox: 0.0334  d1.dn_loss_iou: 0.2830  d2.dn_loss_cls: 0.0123  d2.dn_loss_bbox: 0.0319  d2.dn_loss_iou: 0.2716  d3.dn_loss_cls: 0.0114  d3.dn_loss_bbox: 0.0315  d3.dn_loss_iou: 0.2689  d4.dn_loss_cls: 0.0110  d4.dn_loss_bbox: 0.0315  d4.dn_loss_iou: 0.2688  loss_num: 0.0011  d0.loss_num: 0.0012  d1.loss_num: 0.0011  d2.loss_num: 0.0011  d3.loss_num: 0.0012  d4.loss_num: 0.0011
2025/10/29 02:50:29 - mmengine - INFO - Epoch(train) [1][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:30:28  time: 2.1191  data_time: 0.0166  memory: 17568  grad_norm: 38.9489  loss: 7.3597  loss_cls: 0.3770  loss_bbox: 0.0422  loss_iou: 0.3346  d0.loss_cls: 0.4053  d0.loss_bbox: 0.0469  d0.loss_iou: 0.3560  d1.loss_cls: 0.3913  d1.loss_bbox: 0.0453  d1.loss_iou: 0.3474  d2.loss_cls: 0.3823  d2.loss_bbox: 0.0428  d2.loss_iou: 0.3398  d3.loss_cls: 0.3788  d3.loss_bbox: 0.0422  d3.loss_iou: 0.3360  d4.loss_cls: 0.3759  d4.loss_bbox: 0.0426  d4.loss_iou: 0.3379  enc_loss_cls: 0.4156  enc_loss_bbox: 0.0495  enc_loss_iou: 0.3713  dn_loss_cls: 0.0156  dn_loss_bbox: 0.0306  dn_loss_iou: 0.2457  d0.dn_loss_cls: 0.0398  d0.dn_loss_bbox: 0.0435  d0.dn_loss_iou: 0.3302  d1.dn_loss_cls: 0.0183  d1.dn_loss_bbox: 0.0328  d1.dn_loss_iou: 0.2608  d2.dn_loss_cls: 0.0150  d2.dn_loss_bbox: 0.0310  d2.dn_loss_iou: 0.2484  d3.dn_loss_cls: 0.0145  d3.dn_loss_bbox: 0.0307  d3.dn_loss_iou: 0.2457  d4.dn_loss_cls: 0.0150  d4.dn_loss_bbox: 0.0306  d4.dn_loss_iou: 0.2455  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2025/10/29 02:52:16 - mmengine - INFO - Epoch(train) [1][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:28:35  time: 2.1254  data_time: 0.0170  memory: 17568  grad_norm: 33.6852  loss: 7.2988  loss_cls: 0.3554  loss_bbox: 0.0437  loss_iou: 0.3462  d0.loss_cls: 0.3883  d0.loss_bbox: 0.0476  d0.loss_iou: 0.3606  d1.loss_cls: 0.3668  d1.loss_bbox: 0.0440  d1.loss_iou: 0.3475  d2.loss_cls: 0.3571  d2.loss_bbox: 0.0446  d2.loss_iou: 0.3522  d3.loss_cls: 0.3580  d3.loss_bbox: 0.0440  d3.loss_iou: 0.3484  d4.loss_cls: 0.3573  d4.loss_bbox: 0.0432  d4.loss_iou: 0.3438  enc_loss_cls: 0.3969  enc_loss_bbox: 0.0491  enc_loss_iou: 0.3766  dn_loss_cls: 0.0079  dn_loss_bbox: 0.0300  dn_loss_iou: 0.2564  d0.dn_loss_cls: 0.0386  d0.dn_loss_bbox: 0.0432  d0.dn_loss_iou: 0.3409  d1.dn_loss_cls: 0.0149  d1.dn_loss_bbox: 0.0319  d1.dn_loss_iou: 0.2704  d2.dn_loss_cls: 0.0096  d2.dn_loss_bbox: 0.0304  d2.dn_loss_iou: 0.2594  d3.dn_loss_cls: 0.0084  d3.dn_loss_bbox: 0.0301  d3.dn_loss_iou: 0.2565  d4.dn_loss_cls: 0.0081  d4.dn_loss_bbox: 0.0300  d4.dn_loss_iou: 0.2562  loss_num: 0.0008  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 02:54:03 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 02:54:03 - mmengine - INFO - Epoch(train) [1][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:26:52  time: 2.1451  data_time: 0.0168  memory: 17574  grad_norm: 35.2579  loss: 7.5433  loss_cls: 0.3318  loss_bbox: 0.0492  loss_iou: 0.3684  d0.loss_cls: 0.3727  d0.loss_bbox: 0.0488  d0.loss_iou: 0.3710  d1.loss_cls: 0.3511  d1.loss_bbox: 0.0474  d1.loss_iou: 0.3658  d2.loss_cls: 0.3409  d2.loss_bbox: 0.0466  d2.loss_iou: 0.3648  d3.loss_cls: 0.3382  d3.loss_bbox: 0.0472  d3.loss_iou: 0.3651  d4.loss_cls: 0.3382  d4.loss_bbox: 0.0469  d4.loss_iou: 0.3628  enc_loss_cls: 0.3842  enc_loss_bbox: 0.0526  enc_loss_iou: 0.3904  dn_loss_cls: 0.0084  dn_loss_bbox: 0.0345  dn_loss_iou: 0.2859  d0.dn_loss_cls: 0.0425  d0.dn_loss_bbox: 0.0502  d0.dn_loss_iou: 0.3865  d1.dn_loss_cls: 0.0142  d1.dn_loss_bbox: 0.0371  d1.dn_loss_iou: 0.3028  d2.dn_loss_cls: 0.0097  d2.dn_loss_bbox: 0.0351  d2.dn_loss_iou: 0.2896  d3.dn_loss_cls: 0.0086  d3.dn_loss_bbox: 0.0347  d3.dn_loss_iou: 0.2861  d4.dn_loss_cls: 0.0084  d4.dn_loss_bbox: 0.0345  d4.dn_loss_iou: 0.2857  loss_num: 0.0008  d0.loss_num: 0.0008  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 02:55:50 - mmengine - INFO - Epoch(train) [1][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:25:03  time: 2.1313  data_time: 0.0184  memory: 17574  grad_norm: 38.0315  loss: 7.4777  loss_cls: 0.3456  loss_bbox: 0.0483  loss_iou: 0.3601  d0.loss_cls: 0.3888  d0.loss_bbox: 0.0534  d0.loss_iou: 0.3794  d1.loss_cls: 0.3615  d1.loss_bbox: 0.0526  d1.loss_iou: 0.3715  d2.loss_cls: 0.3573  d2.loss_bbox: 0.0493  d2.loss_iou: 0.3616  d3.loss_cls: 0.3515  d3.loss_bbox: 0.0493  d3.loss_iou: 0.3619  d4.loss_cls: 0.3481  d4.loss_bbox: 0.0488  d4.loss_iou: 0.3612  enc_loss_cls: 0.3975  enc_loss_bbox: 0.0550  enc_loss_iou: 0.3915  dn_loss_cls: 0.0100  dn_loss_bbox: 0.0321  dn_loss_iou: 0.2606  d0.dn_loss_cls: 0.0417  d0.dn_loss_bbox: 0.0450  d0.dn_loss_iou: 0.3453  d1.dn_loss_cls: 0.0165  d1.dn_loss_bbox: 0.0342  d1.dn_loss_iou: 0.2765  d2.dn_loss_cls: 0.0120  d2.dn_loss_bbox: 0.0325  d2.dn_loss_iou: 0.2643  d3.dn_loss_cls: 0.0107  d3.dn_loss_bbox: 0.0322  d3.dn_loss_iou: 0.2615  d4.dn_loss_cls: 0.0103  d4.dn_loss_bbox: 0.0320  d4.dn_loss_iou: 0.2604  loss_num: 0.0010  d0.loss_num: 0.0010  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2025/10/29 02:57:37 - mmengine - INFO - Epoch(train) [1][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:23:17  time: 2.1409  data_time: 0.0166  memory: 17575  grad_norm: 35.5147  loss: 7.3206  loss_cls: 0.3110  loss_bbox: 0.0453  loss_iou: 0.3829  d0.loss_cls: 0.3390  d0.loss_bbox: 0.0472  d0.loss_iou: 0.3918  d1.loss_cls: 0.3201  d1.loss_bbox: 0.0459  d1.loss_iou: 0.3883  d2.loss_cls: 0.3161  d2.loss_bbox: 0.0454  d2.loss_iou: 0.3830  d3.loss_cls: 0.3149  d3.loss_bbox: 0.0456  d3.loss_iou: 0.3839  d4.loss_cls: 0.3126  d4.loss_bbox: 0.0453  d4.loss_iou: 0.3836  enc_loss_cls: 0.3531  enc_loss_bbox: 0.0512  enc_loss_iou: 0.4136  dn_loss_cls: 0.0084  dn_loss_bbox: 0.0318  dn_loss_iou: 0.2674  d0.dn_loss_cls: 0.0342  d0.dn_loss_bbox: 0.0447  d0.dn_loss_iou: 0.3525  d1.dn_loss_cls: 0.0131  d1.dn_loss_bbox: 0.0337  d1.dn_loss_iou: 0.2824  d2.dn_loss_cls: 0.0099  d2.dn_loss_bbox: 0.0321  d2.dn_loss_iou: 0.2696  d3.dn_loss_cls: 0.0089  d3.dn_loss_bbox: 0.0319  d3.dn_loss_iou: 0.2678  d4.dn_loss_cls: 0.0085  d4.dn_loss_bbox: 0.0318  d4.dn_loss_iou: 0.2672  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 02:59:23 - mmengine - INFO - Epoch(train) [1][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:21:25  time: 2.1228  data_time: 0.0171  memory: 17574  grad_norm: 33.4376  loss: 6.6640  loss_cls: 0.2971  loss_bbox: 0.0384  loss_iou: 0.3248  d0.loss_cls: 0.3336  d0.loss_bbox: 0.0389  d0.loss_iou: 0.3310  d1.loss_cls: 0.3137  d1.loss_bbox: 0.0386  d1.loss_iou: 0.3257  d2.loss_cls: 0.3031  d2.loss_bbox: 0.0382  d2.loss_iou: 0.3217  d3.loss_cls: 0.3010  d3.loss_bbox: 0.0379  d3.loss_iou: 0.3216  d4.loss_cls: 0.2979  d4.loss_bbox: 0.0387  d4.loss_iou: 0.3267  enc_loss_cls: 0.3537  enc_loss_bbox: 0.0402  enc_loss_iou: 0.3417  dn_loss_cls: 0.0061  dn_loss_bbox: 0.0297  dn_loss_iou: 0.2553  d0.dn_loss_cls: 0.0343  d0.dn_loss_bbox: 0.0418  d0.dn_loss_iou: 0.3401  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0312  d1.dn_loss_iou: 0.2692  d2.dn_loss_cls: 0.0068  d2.dn_loss_bbox: 0.0301  d2.dn_loss_iou: 0.2580  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0298  d3.dn_loss_iou: 0.2559  d4.dn_loss_cls: 0.0061  d4.dn_loss_bbox: 0.0297  d4.dn_loss_iou: 0.2550  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:01:09 - mmengine - INFO - Epoch(train) [1][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:19:35  time: 2.1286  data_time: 0.0167  memory: 17581  grad_norm: 42.5049  loss: 6.6892  loss_cls: 0.2819  loss_bbox: 0.0409  loss_iou: 0.3069  d0.loss_cls: 0.3168  d0.loss_bbox: 0.0432  d0.loss_iou: 0.3201  d1.loss_cls: 0.3045  d1.loss_bbox: 0.0395  d1.loss_iou: 0.3105  d2.loss_cls: 0.2889  d2.loss_bbox: 0.0419  d2.loss_iou: 0.3103  d3.loss_cls: 0.2846  d3.loss_bbox: 0.0410  d3.loss_iou: 0.3057  d4.loss_cls: 0.2837  d4.loss_bbox: 0.0409  d4.loss_iou: 0.3068  enc_loss_cls: 0.3376  enc_loss_bbox: 0.0467  enc_loss_iou: 0.3370  dn_loss_cls: 0.0107  dn_loss_bbox: 0.0332  dn_loss_iou: 0.2754  d0.dn_loss_cls: 0.0460  d0.dn_loss_bbox: 0.0478  d0.dn_loss_iou: 0.3706  d1.dn_loss_cls: 0.0178  d1.dn_loss_bbox: 0.0356  d1.dn_loss_iou: 0.2922  d2.dn_loss_cls: 0.0130  d2.dn_loss_bbox: 0.0335  d2.dn_loss_iou: 0.2788  d3.dn_loss_cls: 0.0118  d3.dn_loss_bbox: 0.0333  d3.dn_loss_iou: 0.2755  d4.dn_loss_cls: 0.0112  d4.dn_loss_bbox: 0.0332  d4.dn_loss_iou: 0.2752  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2025/10/29 03:02:56 - mmengine - INFO - Epoch(train) [1][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:17:50  time: 2.1432  data_time: 0.0177  memory: 17568  grad_norm: 37.3826  loss: 6.6304  loss_cls: 0.2883  loss_bbox: 0.0408  loss_iou: 0.3118  d0.loss_cls: 0.3104  d0.loss_bbox: 0.0420  d0.loss_iou: 0.3220  d1.loss_cls: 0.2952  d1.loss_bbox: 0.0415  d1.loss_iou: 0.3184  d2.loss_cls: 0.2885  d2.loss_bbox: 0.0414  d2.loss_iou: 0.3151  d3.loss_cls: 0.2878  d3.loss_bbox: 0.0408  d3.loss_iou: 0.3113  d4.loss_cls: 0.2895  d4.loss_bbox: 0.0397  d4.loss_iou: 0.3104  enc_loss_cls: 0.3270  enc_loss_bbox: 0.0447  enc_loss_iou: 0.3403  dn_loss_cls: 0.0138  dn_loss_bbox: 0.0313  dn_loss_iou: 0.2653  d0.dn_loss_cls: 0.0423  d0.dn_loss_bbox: 0.0437  d0.dn_loss_iou: 0.3530  d1.dn_loss_cls: 0.0187  d1.dn_loss_bbox: 0.0331  d1.dn_loss_iou: 0.2822  d2.dn_loss_cls: 0.0144  d2.dn_loss_bbox: 0.0316  d2.dn_loss_iou: 0.2687  d3.dn_loss_cls: 0.0137  d3.dn_loss_bbox: 0.0313  d3.dn_loss_iou: 0.2656  d4.dn_loss_cls: 0.0141  d4.dn_loss_bbox: 0.0313  d4.dn_loss_iou: 0.2652  loss_num: 0.0007  d0.loss_num: 0.0007  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:04:43 - mmengine - INFO - Epoch(train) [1][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:16:00  time: 2.1276  data_time: 0.0174  memory: 17580  grad_norm: 39.7973  loss: 6.8605  loss_cls: 0.2898  loss_bbox: 0.0410  loss_iou: 0.3381  d0.loss_cls: 0.3205  d0.loss_bbox: 0.0430  d0.loss_iou: 0.3475  d1.loss_cls: 0.3017  d1.loss_bbox: 0.0419  d1.loss_iou: 0.3418  d2.loss_cls: 0.2963  d2.loss_bbox: 0.0415  d2.loss_iou: 0.3411  d3.loss_cls: 0.2959  d3.loss_bbox: 0.0408  d3.loss_iou: 0.3363  d4.loss_cls: 0.2900  d4.loss_bbox: 0.0410  d4.loss_iou: 0.3379  enc_loss_cls: 0.3330  enc_loss_bbox: 0.0468  enc_loss_iou: 0.3647  dn_loss_cls: 0.0097  dn_loss_bbox: 0.0310  dn_loss_iou: 0.2715  d0.dn_loss_cls: 0.0401  d0.dn_loss_bbox: 0.0433  d0.dn_loss_iou: 0.3557  d1.dn_loss_cls: 0.0142  d1.dn_loss_bbox: 0.0327  d1.dn_loss_iou: 0.2857  d2.dn_loss_cls: 0.0109  d2.dn_loss_bbox: 0.0314  d2.dn_loss_iou: 0.2743  d3.dn_loss_cls: 0.0098  d3.dn_loss_bbox: 0.0311  d3.dn_loss_iou: 0.2717  d4.dn_loss_cls: 0.0098  d4.dn_loss_bbox: 0.0310  d4.dn_loss_iou: 0.2711  loss_num: 0.0008  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:06:28 - mmengine - INFO - Epoch(train) [1][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:14:04  time: 2.1071  data_time: 0.0168  memory: 17568  grad_norm: 35.4451  loss: 7.3929  loss_cls: 0.3274  loss_bbox: 0.0465  loss_iou: 0.3768  d0.loss_cls: 0.3681  d0.loss_bbox: 0.0469  d0.loss_iou: 0.3820  d1.loss_cls: 0.3463  d1.loss_bbox: 0.0468  d1.loss_iou: 0.3775  d2.loss_cls: 0.3418  d2.loss_bbox: 0.0451  d2.loss_iou: 0.3698  d3.loss_cls: 0.3371  d3.loss_bbox: 0.0441  d3.loss_iou: 0.3665  d4.loss_cls: 0.3337  d4.loss_bbox: 0.0447  d4.loss_iou: 0.3718  enc_loss_cls: 0.3733  enc_loss_bbox: 0.0526  enc_loss_iou: 0.4118  dn_loss_cls: 0.0146  dn_loss_bbox: 0.0298  dn_loss_iou: 0.2597  d0.dn_loss_cls: 0.0387  d0.dn_loss_bbox: 0.0433  d0.dn_loss_iou: 0.3465  d1.dn_loss_cls: 0.0200  d1.dn_loss_bbox: 0.0317  d1.dn_loss_iou: 0.2738  d2.dn_loss_cls: 0.0167  d2.dn_loss_bbox: 0.0302  d2.dn_loss_iou: 0.2621  d3.dn_loss_cls: 0.0160  d3.dn_loss_bbox: 0.0299  d3.dn_loss_iou: 0.2598  d4.dn_loss_cls: 0.0144  d4.dn_loss_bbox: 0.0298  d4.dn_loss_iou: 0.2596  loss_num: 0.0009  d0.loss_num: 0.0009  d1.loss_num: 0.0009  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2025/10/29 03:08:15 - mmengine - INFO - Epoch(train) [1][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:12:19  time: 2.1409  data_time: 0.0166  memory: 17585  grad_norm: 46.4634  loss: 6.7451  loss_cls: 0.2751  loss_bbox: 0.0448  loss_iou: 0.3307  d0.loss_cls: 0.3043  d0.loss_bbox: 0.0458  d0.loss_iou: 0.3405  d1.loss_cls: 0.2882  d1.loss_bbox: 0.0433  d1.loss_iou: 0.3310  d2.loss_cls: 0.2811  d2.loss_bbox: 0.0444  d2.loss_iou: 0.3297  d3.loss_cls: 0.2753  d3.loss_bbox: 0.0449  d3.loss_iou: 0.3316  d4.loss_cls: 0.2759  d4.loss_bbox: 0.0444  d4.loss_iou: 0.3285  enc_loss_cls: 0.3244  enc_loss_bbox: 0.0481  enc_loss_iou: 0.3567  dn_loss_cls: 0.0123  dn_loss_bbox: 0.0322  dn_loss_iou: 0.2706  d0.dn_loss_cls: 0.0415  d0.dn_loss_bbox: 0.0450  d0.dn_loss_iou: 0.3619  d1.dn_loss_cls: 0.0191  d1.dn_loss_bbox: 0.0340  d1.dn_loss_iou: 0.2844  d2.dn_loss_cls: 0.0143  d2.dn_loss_bbox: 0.0326  d2.dn_loss_iou: 0.2732  d3.dn_loss_cls: 0.0127  d3.dn_loss_bbox: 0.0323  d3.dn_loss_iou: 0.2705  d4.dn_loss_cls: 0.0123  d4.dn_loss_bbox: 0.0322  d4.dn_loss_iou: 0.2703  loss_num: 0.0008  d0.loss_num: 0.0010  d1.loss_num: 0.0008  d2.loss_num: 0.0009  d3.loss_num: 0.0009  d4.loss_num: 0.0008
2025/10/29 03:10:01 - mmengine - INFO - Epoch(train) [1][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:10:27  time: 2.1188  data_time: 0.0172  memory: 17560  grad_norm: 41.1079  loss: 7.9542  loss_cls: 0.3319  loss_bbox: 0.0490  loss_iou: 0.4500  d0.loss_cls: 0.3729  d0.loss_bbox: 0.0525  d0.loss_iou: 0.4740  d1.loss_cls: 0.3483  d1.loss_bbox: 0.0514  d1.loss_iou: 0.4587  d2.loss_cls: 0.3414  d2.loss_bbox: 0.0494  d2.loss_iou: 0.4545  d3.loss_cls: 0.3363  d3.loss_bbox: 0.0489  d3.loss_iou: 0.4502  d4.loss_cls: 0.3324  d4.loss_bbox: 0.0490  d4.loss_iou: 0.4500  enc_loss_cls: 0.3806  enc_loss_bbox: 0.0573  enc_loss_iou: 0.4951  dn_loss_cls: 0.0087  dn_loss_bbox: 0.0294  dn_loss_iou: 0.2569  d0.dn_loss_cls: 0.0355  d0.dn_loss_bbox: 0.0411  d0.dn_loss_iou: 0.3399  d1.dn_loss_cls: 0.0129  d1.dn_loss_bbox: 0.0310  d1.dn_loss_iou: 0.2712  d2.dn_loss_cls: 0.0094  d2.dn_loss_bbox: 0.0297  d2.dn_loss_iou: 0.2599  d3.dn_loss_cls: 0.0088  d3.dn_loss_bbox: 0.0294  d3.dn_loss_iou: 0.2573  d4.dn_loss_cls: 0.0086  d4.dn_loss_bbox: 0.0294  d4.dn_loss_iou: 0.2567  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:11:48 - mmengine - INFO - Epoch(train) [1][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:08:40  time: 2.1358  data_time: 0.0190  memory: 17568  grad_norm: 40.3423  loss: 7.6687  loss_cls: 0.3233  loss_bbox: 0.0471  loss_iou: 0.3753  d0.loss_cls: 0.3663  d0.loss_bbox: 0.0473  d0.loss_iou: 0.3815  d1.loss_cls: 0.3448  d1.loss_bbox: 0.0469  d1.loss_iou: 0.3782  d2.loss_cls: 0.3320  d2.loss_bbox: 0.0464  d2.loss_iou: 0.3780  d3.loss_cls: 0.3324  d3.loss_bbox: 0.0447  d3.loss_iou: 0.3671  d4.loss_cls: 0.3272  d4.loss_bbox: 0.0450  d4.loss_iou: 0.3689  enc_loss_cls: 0.3854  enc_loss_bbox: 0.0531  enc_loss_iou: 0.4000  dn_loss_cls: 0.0347  dn_loss_bbox: 0.0328  dn_loss_iou: 0.2854  d0.dn_loss_cls: 0.0650  d0.dn_loss_bbox: 0.0459  d0.dn_loss_iou: 0.3795  d1.dn_loss_cls: 0.0376  d1.dn_loss_bbox: 0.0347  d1.dn_loss_iou: 0.3014  d2.dn_loss_cls: 0.0337  d2.dn_loss_bbox: 0.0331  d2.dn_loss_iou: 0.2882  d3.dn_loss_cls: 0.0329  d3.dn_loss_bbox: 0.0328  d3.dn_loss_iou: 0.2853  d4.dn_loss_cls: 0.0328  d4.dn_loss_bbox: 0.0327  d4.dn_loss_iou: 0.2849  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:13:34 - mmengine - INFO - Epoch(train) [1][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:06:50  time: 2.1231  data_time: 0.0173  memory: 17573  grad_norm: 41.2757  loss: 6.5248  loss_cls: 0.2854  loss_bbox: 0.0366  loss_iou: 0.3103  d0.loss_cls: 0.3067  d0.loss_bbox: 0.0405  d0.loss_iou: 0.3296  d1.loss_cls: 0.2975  d1.loss_bbox: 0.0391  d1.loss_iou: 0.3171  d2.loss_cls: 0.2913  d2.loss_bbox: 0.0368  d2.loss_iou: 0.3122  d3.loss_cls: 0.2877  d3.loss_bbox: 0.0366  d3.loss_iou: 0.3110  d4.loss_cls: 0.2829  d4.loss_bbox: 0.0369  d4.loss_iou: 0.3125  enc_loss_cls: 0.3263  enc_loss_bbox: 0.0424  enc_loss_iou: 0.3400  dn_loss_cls: 0.0092  dn_loss_bbox: 0.0305  dn_loss_iou: 0.2590  d0.dn_loss_cls: 0.0391  d0.dn_loss_bbox: 0.0427  d0.dn_loss_iou: 0.3429  d1.dn_loss_cls: 0.0141  d1.dn_loss_bbox: 0.0320  d1.dn_loss_iou: 0.2716  d2.dn_loss_cls: 0.0104  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2611  d3.dn_loss_cls: 0.0094  d3.dn_loss_bbox: 0.0305  d3.dn_loss_iou: 0.2588  d4.dn_loss_cls: 0.0093  d4.dn_loss_bbox: 0.0305  d4.dn_loss_iou: 0.2588  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:15:20 - mmengine - INFO - Epoch(train) [1][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:05:03  time: 2.1303  data_time: 0.0172  memory: 17574  grad_norm: 39.0526  loss: 6.2644  loss_cls: 0.2652  loss_bbox: 0.0373  loss_iou: 0.3005  d0.loss_cls: 0.2874  d0.loss_bbox: 0.0392  d0.loss_iou: 0.3158  d1.loss_cls: 0.2668  d1.loss_bbox: 0.0393  d1.loss_iou: 0.3124  d2.loss_cls: 0.2661  d2.loss_bbox: 0.0363  d2.loss_iou: 0.3022  d3.loss_cls: 0.2648  d3.loss_bbox: 0.0371  d3.loss_iou: 0.3021  d4.loss_cls: 0.2677  d4.loss_bbox: 0.0362  d4.loss_iou: 0.2985  enc_loss_cls: 0.2980  enc_loss_bbox: 0.0421  enc_loss_iou: 0.3269  dn_loss_cls: 0.0113  dn_loss_bbox: 0.0288  dn_loss_iou: 0.2541  d0.dn_loss_cls: 0.0396  d0.dn_loss_bbox: 0.0405  d0.dn_loss_iou: 0.3396  d1.dn_loss_cls: 0.0173  d1.dn_loss_bbox: 0.0306  d1.dn_loss_iou: 0.2679  d2.dn_loss_cls: 0.0135  d2.dn_loss_bbox: 0.0292  d2.dn_loss_iou: 0.2574  d3.dn_loss_cls: 0.0121  d3.dn_loss_bbox: 0.0289  d3.dn_loss_iou: 0.2544  d4.dn_loss_cls: 0.0103  d4.dn_loss_bbox: 0.0288  d4.dn_loss_iou: 0.2538  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:17:08 - mmengine - INFO - Epoch(train) [1][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:03:21  time: 2.1543  data_time: 0.0177  memory: 17581  grad_norm: 38.2980  loss: 7.2026  loss_cls: 0.2960  loss_bbox: 0.0508  loss_iou: 0.4018  d0.loss_cls: 0.3331  d0.loss_bbox: 0.0528  d0.loss_iou: 0.4121  d1.loss_cls: 0.3129  d1.loss_bbox: 0.0535  d1.loss_iou: 0.4035  d2.loss_cls: 0.3054  d2.loss_bbox: 0.0509  d2.loss_iou: 0.4014  d3.loss_cls: 0.3003  d3.loss_bbox: 0.0515  d3.loss_iou: 0.4044  d4.loss_cls: 0.2932  d4.loss_bbox: 0.0513  d4.loss_iou: 0.4044  enc_loss_cls: 0.3388  enc_loss_bbox: 0.0587  enc_loss_iou: 0.4319  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2393  d0.dn_loss_cls: 0.0346  d0.dn_loss_bbox: 0.0421  d0.dn_loss_iou: 0.3177  d1.dn_loss_cls: 0.0109  d1.dn_loss_bbox: 0.0313  d1.dn_loss_iou: 0.2508  d2.dn_loss_cls: 0.0073  d2.dn_loss_bbox: 0.0297  d2.dn_loss_iou: 0.2401  d3.dn_loss_cls: 0.0064  d3.dn_loss_bbox: 0.0295  d3.dn_loss_iou: 0.2394  d4.dn_loss_cls: 0.0060  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2391  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0009  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:18:55 - mmengine - INFO - Epoch(train) [1][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 5:01:32  time: 2.1263  data_time: 0.0164  memory: 17580  grad_norm: 36.9078  loss: 6.4426  loss_cls: 0.2734  loss_bbox: 0.0404  loss_iou: 0.3104  d0.loss_cls: 0.3118  d0.loss_bbox: 0.0398  d0.loss_iou: 0.3140  d1.loss_cls: 0.2837  d1.loss_bbox: 0.0402  d1.loss_iou: 0.3139  d2.loss_cls: 0.2780  d2.loss_bbox: 0.0404  d2.loss_iou: 0.3145  d3.loss_cls: 0.2767  d3.loss_bbox: 0.0404  d3.loss_iou: 0.3110  d4.loss_cls: 0.2764  d4.loss_bbox: 0.0390  d4.loss_iou: 0.3105  enc_loss_cls: 0.3211  enc_loss_bbox: 0.0451  enc_loss_iou: 0.3362  dn_loss_cls: 0.0071  dn_loss_bbox: 0.0321  dn_loss_iou: 0.2555  d0.dn_loss_cls: 0.0347  d0.dn_loss_bbox: 0.0456  d0.dn_loss_iou: 0.3406  d1.dn_loss_cls: 0.0115  d1.dn_loss_bbox: 0.0342  d1.dn_loss_iou: 0.2700  d2.dn_loss_cls: 0.0084  d2.dn_loss_bbox: 0.0325  d2.dn_loss_iou: 0.2580  d3.dn_loss_cls: 0.0073  d3.dn_loss_bbox: 0.0322  d3.dn_loss_iou: 0.2555  d4.dn_loss_cls: 0.0071  d4.dn_loss_bbox: 0.0320  d4.dn_loss_iou: 0.2552  loss_num: 0.0010  d0.loss_num: 0.0012  d1.loss_num: 0.0010  d2.loss_num: 0.0011  d3.loss_num: 0.0011  d4.loss_num: 0.0010
2025/10/29 03:20:41 - mmengine - INFO - Epoch(train) [1][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:59:44  time: 2.1307  data_time: 0.0170  memory: 17830  grad_norm: 43.9095  loss: 8.0350  loss_cls: 0.3162  loss_bbox: 0.0479  loss_iou: 0.4730  d0.loss_cls: 0.3629  d0.loss_bbox: 0.0504  d0.loss_iou: 0.4796  d1.loss_cls: 0.3353  d1.loss_bbox: 0.0485  d1.loss_iou: 0.4785  d2.loss_cls: 0.3279  d2.loss_bbox: 0.0476  d2.loss_iou: 0.4719  d3.loss_cls: 0.3240  d3.loss_bbox: 0.0473  d3.loss_iou: 0.4690  d4.loss_cls: 0.3164  d4.loss_bbox: 0.0483  d4.loss_iou: 0.4733  enc_loss_cls: 0.3701  enc_loss_bbox: 0.0522  enc_loss_iou: 0.4954  dn_loss_cls: 0.0102  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2670  d0.dn_loss_cls: 0.0395  d0.dn_loss_bbox: 0.0413  d0.dn_loss_iou: 0.3538  d1.dn_loss_cls: 0.0159  d1.dn_loss_bbox: 0.0308  d1.dn_loss_iou: 0.2817  d2.dn_loss_cls: 0.0119  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2699  d3.dn_loss_cls: 0.0107  d3.dn_loss_bbox: 0.0290  d3.dn_loss_iou: 0.2672  d4.dn_loss_cls: 0.0102  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2667  loss_num: 0.0010  d0.loss_num: 0.0011  d1.loss_num: 0.0010  d2.loss_num: 0.0010  d3.loss_num: 0.0010  d4.loss_num: 0.0010
2025/10/29 03:22:29 - mmengine - INFO - Epoch(train) [1][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:58:01  time: 2.1509  data_time: 0.0168  memory: 17573  grad_norm: 40.0977  loss: 6.2845  loss_cls: 0.2617  loss_bbox: 0.0378  loss_iou: 0.3079  d0.loss_cls: 0.2967  d0.loss_bbox: 0.0386  d0.loss_iou: 0.3101  d1.loss_cls: 0.2809  d1.loss_bbox: 0.0372  d1.loss_iou: 0.3071  d2.loss_cls: 0.2703  d2.loss_bbox: 0.0369  d2.loss_iou: 0.3053  d3.loss_cls: 0.2695  d3.loss_bbox: 0.0365  d3.loss_iou: 0.3025  d4.loss_cls: 0.2662  d4.loss_bbox: 0.0368  d4.loss_iou: 0.3032  enc_loss_cls: 0.3125  enc_loss_bbox: 0.0399  enc_loss_iou: 0.3231  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0315  dn_loss_iou: 0.2579  d0.dn_loss_cls: 0.0309  d0.dn_loss_bbox: 0.0435  d0.dn_loss_iou: 0.3381  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0329  d1.dn_loss_iou: 0.2696  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0318  d2.dn_loss_iou: 0.2598  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0315  d3.dn_loss_iou: 0.2579  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0314  d4.dn_loss_iou: 0.2576  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:24:15 - mmengine - INFO - Epoch(train) [1][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:56:12  time: 2.1240  data_time: 0.0172  memory: 17567  grad_norm: 34.6495  loss: 6.8335  loss_cls: 0.2788  loss_bbox: 0.0429  loss_iou: 0.3542  d0.loss_cls: 0.3177  d0.loss_bbox: 0.0462  d0.loss_iou: 0.3709  d1.loss_cls: 0.2894  d1.loss_bbox: 0.0434  d1.loss_iou: 0.3601  d2.loss_cls: 0.2795  d2.loss_bbox: 0.0429  d2.loss_iou: 0.3572  d3.loss_cls: 0.2774  d3.loss_bbox: 0.0433  d3.loss_iou: 0.3571  d4.loss_cls: 0.2783  d4.loss_bbox: 0.0431  d4.loss_iou: 0.3551  enc_loss_cls: 0.3336  enc_loss_bbox: 0.0478  enc_loss_iou: 0.3875  dn_loss_cls: 0.0084  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2580  d0.dn_loss_cls: 0.0354  d0.dn_loss_bbox: 0.0418  d0.dn_loss_iou: 0.3352  d1.dn_loss_cls: 0.0135  d1.dn_loss_bbox: 0.0321  d1.dn_loss_iou: 0.2721  d2.dn_loss_cls: 0.0098  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2612  d3.dn_loss_cls: 0.0086  d3.dn_loss_bbox: 0.0303  d3.dn_loss_iou: 0.2584  d4.dn_loss_cls: 0.0082  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2579  loss_num: 0.0008  d0.loss_num: 0.0010  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:26:01 - mmengine - INFO - Epoch(train) [1][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:54:22  time: 2.1200  data_time: 0.0165  memory: 17568  grad_norm: 41.5769  loss: 7.1065  loss_cls: 0.3056  loss_bbox: 0.0427  loss_iou: 0.3676  d0.loss_cls: 0.3451  d0.loss_bbox: 0.0460  d0.loss_iou: 0.3917  d1.loss_cls: 0.3179  d1.loss_bbox: 0.0438  d1.loss_iou: 0.3780  d2.loss_cls: 0.3117  d2.loss_bbox: 0.0427  d2.loss_iou: 0.3705  d3.loss_cls: 0.3086  d3.loss_bbox: 0.0427  d3.loss_iou: 0.3684  d4.loss_cls: 0.3065  d4.loss_bbox: 0.0425  d4.loss_iou: 0.3660  enc_loss_cls: 0.3584  enc_loss_bbox: 0.0477  enc_loss_iou: 0.4016  dn_loss_cls: 0.0076  dn_loss_bbox: 0.0291  dn_loss_iou: 0.2561  d0.dn_loss_cls: 0.0338  d0.dn_loss_bbox: 0.0400  d0.dn_loss_iou: 0.3372  d1.dn_loss_cls: 0.0119  d1.dn_loss_bbox: 0.0306  d1.dn_loss_iou: 0.2679  d2.dn_loss_cls: 0.0085  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2587  d3.dn_loss_cls: 0.0075  d3.dn_loss_bbox: 0.0291  d3.dn_loss_iou: 0.2562  d4.dn_loss_cls: 0.0074  d4.dn_loss_bbox: 0.0291  d4.dn_loss_iou: 0.2558  loss_num: 0.0008  d0.loss_num: 0.0010  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:27:48 - mmengine - INFO - Epoch(train) [1][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:52:38  time: 2.1450  data_time: 0.0196  memory: 17594  grad_norm: 36.9486  loss: 6.9380  loss_cls: 0.2690  loss_bbox: 0.0448  loss_iou: 0.3746  d0.loss_cls: 0.3103  d0.loss_bbox: 0.0467  d0.loss_iou: 0.3893  d1.loss_cls: 0.2876  d1.loss_bbox: 0.0445  d1.loss_iou: 0.3768  d2.loss_cls: 0.2731  d2.loss_bbox: 0.0458  d2.loss_iou: 0.3753  d3.loss_cls: 0.2675  d3.loss_bbox: 0.0461  d3.loss_iou: 0.3760  d4.loss_cls: 0.2676  d4.loss_bbox: 0.0450  d4.loss_iou: 0.3766  enc_loss_cls: 0.3272  enc_loss_bbox: 0.0498  enc_loss_iou: 0.4068  dn_loss_cls: 0.0100  dn_loss_bbox: 0.0289  dn_loss_iou: 0.2580  d0.dn_loss_cls: 0.0377  d0.dn_loss_bbox: 0.0404  d0.dn_loss_iou: 0.3430  d1.dn_loss_cls: 0.0156  d1.dn_loss_bbox: 0.0306  d1.dn_loss_iou: 0.2725  d2.dn_loss_cls: 0.0113  d2.dn_loss_bbox: 0.0292  d2.dn_loss_iou: 0.2606  d3.dn_loss_cls: 0.0101  d3.dn_loss_bbox: 0.0289  d3.dn_loss_iou: 0.2583  d4.dn_loss_cls: 0.0101  d4.dn_loss_bbox: 0.0289  d4.dn_loss_iou: 0.2578  loss_num: 0.0009  d0.loss_num: 0.0011  d1.loss_num: 0.0009  d2.loss_num: 0.0010  d3.loss_num: 0.0009  d4.loss_num: 0.0009
2025/10/29 03:29:34 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 03:29:34 - mmengine - INFO - Epoch(train) [1][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:50:49  time: 2.1269  data_time: 0.0165  memory: 17581  grad_norm: 45.5465  loss: 5.8582  loss_cls: 0.2183  loss_bbox: 0.0360  loss_iou: 0.3131  d0.loss_cls: 0.2551  d0.loss_bbox: 0.0380  d0.loss_iou: 0.3272  d1.loss_cls: 0.2297  d1.loss_bbox: 0.0368  d1.loss_iou: 0.3196  d2.loss_cls: 0.2205  d2.loss_bbox: 0.0374  d2.loss_iou: 0.3199  d3.loss_cls: 0.2185  d3.loss_bbox: 0.0362  d3.loss_iou: 0.3151  d4.loss_cls: 0.2185  d4.loss_bbox: 0.0360  d4.loss_iou: 0.3140  enc_loss_cls: 0.2672  enc_loss_bbox: 0.0413  enc_loss_iou: 0.3491  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0272  dn_loss_iou: 0.2295  d0.dn_loss_cls: 0.0275  d0.dn_loss_bbox: 0.0385  d0.dn_loss_iou: 0.3124  d1.dn_loss_cls: 0.0082  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2436  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2321  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0271  d3.dn_loss_iou: 0.2298  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.0271  d4.dn_loss_iou: 0.2293  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:30:49 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 03:30:49 - mmengine - INFO - Saving checkpoint at 1 epochs
2025/10/29 03:31:03 - mmengine - INFO - Epoch(val) [1][ 50/429]    eta: 0:00:41  time: 0.1102  data_time: 0.0088  memory: 17565  
2025/10/29 03:31:08 - mmengine - INFO - Epoch(val) [1][100/429]    eta: 0:00:34  time: 0.1013  data_time: 0.0024  memory: 4269  
2025/10/29 03:31:13 - mmengine - INFO - Epoch(val) [1][150/429]    eta: 0:00:29  time: 0.1014  data_time: 0.0025  memory: 4269  
2025/10/29 03:31:18 - mmengine - INFO - Epoch(val) [1][200/429]    eta: 0:00:23  time: 0.1007  data_time: 0.0025  memory: 4269  
2025/10/29 03:31:23 - mmengine - INFO - Epoch(val) [1][250/429]    eta: 0:00:18  time: 0.1003  data_time: 0.0024  memory: 4269  
2025/10/29 03:31:28 - mmengine - INFO - Epoch(val) [1][300/429]    eta: 0:00:13  time: 0.1005  data_time: 0.0028  memory: 4269  
2025/10/29 03:31:33 - mmengine - INFO - Epoch(val) [1][350/429]    eta: 0:00:08  time: 0.1012  data_time: 0.0025  memory: 4269  
2025/10/29 03:31:38 - mmengine - INFO - Epoch(val) [1][400/429]    eta: 0:00:02  time: 0.0999  data_time: 0.0025  memory: 4269  
2025/10/29 03:31:43 - mmengine - INFO - {'instance_F1_score': 0.3269097462914183, 'instance_acc': 0.20501064352382511, 'image_F1_score': 0.2978502978502979, 'image_acc': 0.2100815850815851}
2025/10/29 03:31:43 - mmengine - INFO - Epoch(val) [1][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.3269  grefcoco_val/refdrone/instance_acc: 0.2050  grefcoco_val/refdrone/image_F1_score: 0.2979  grefcoco_val/refdrone/image_acc: 0.2101  data_time: 0.0032  time: 0.1018
2025/10/29 03:33:30 - mmengine - INFO - Epoch(train) [2][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:47:48  time: 2.1458  data_time: 0.0165  memory: 17600  grad_norm: 39.0910  loss: 6.1008  loss_cls: 0.2296  loss_bbox: 0.0358  loss_iou: 0.3082  d0.loss_cls: 0.2646  d0.loss_bbox: 0.0375  d0.loss_iou: 0.3254  d1.loss_cls: 0.2366  d1.loss_bbox: 0.0368  d1.loss_iou: 0.3165  d2.loss_cls: 0.2318  d2.loss_bbox: 0.0356  d2.loss_iou: 0.3089  d3.loss_cls: 0.2326  d3.loss_bbox: 0.0358  d3.loss_iou: 0.3083  d4.loss_cls: 0.2281  d4.loss_bbox: 0.0365  d4.loss_iou: 0.3107  enc_loss_cls: 0.2745  enc_loss_bbox: 0.0416  enc_loss_iou: 0.3436  dn_loss_cls: 0.0068  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2585  d0.dn_loss_cls: 0.0332  d0.dn_loss_bbox: 0.0405  d0.dn_loss_iou: 0.3453  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0307  d1.dn_loss_iou: 0.2727  d2.dn_loss_cls: 0.0083  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2619  d3.dn_loss_cls: 0.0071  d3.dn_loss_bbox: 0.0290  d3.dn_loss_iou: 0.2589  d4.dn_loss_cls: 0.0067  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2584  loss_num: 0.0007  d0.loss_num: 0.0010  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:35:16 - mmengine - INFO - Epoch(train) [2][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:46:00  time: 2.1251  data_time: 0.0162  memory: 17591  grad_norm: 38.5762  loss: 6.4784  loss_cls: 0.2319  loss_bbox: 0.0450  loss_iou: 0.3600  d0.loss_cls: 0.2668  d0.loss_bbox: 0.0462  d0.loss_iou: 0.3655  d1.loss_cls: 0.2435  d1.loss_bbox: 0.0458  d1.loss_iou: 0.3650  d2.loss_cls: 0.2386  d2.loss_bbox: 0.0448  d2.loss_iou: 0.3610  d3.loss_cls: 0.2369  d3.loss_bbox: 0.0448  d3.loss_iou: 0.3602  d4.loss_cls: 0.2332  d4.loss_bbox: 0.0447  d4.loss_iou: 0.3590  enc_loss_cls: 0.2818  enc_loss_bbox: 0.0490  enc_loss_iou: 0.3844  dn_loss_cls: 0.0071  dn_loss_bbox: 0.0301  dn_loss_iou: 0.2501  d0.dn_loss_cls: 0.0316  d0.dn_loss_bbox: 0.0430  d0.dn_loss_iou: 0.3298  d1.dn_loss_cls: 0.0115  d1.dn_loss_bbox: 0.0322  d1.dn_loss_iou: 0.2638  d2.dn_loss_cls: 0.0086  d2.dn_loss_bbox: 0.0304  d2.dn_loss_iou: 0.2525  d3.dn_loss_cls: 0.0075  d3.dn_loss_bbox: 0.0301  d3.dn_loss_iou: 0.2503  d4.dn_loss_cls: 0.0071  d4.dn_loss_bbox: 0.0301  d4.dn_loss_iou: 0.2499  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0007
2025/10/29 03:37:03 - mmengine - INFO - Epoch(train) [2][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:44:14  time: 2.1413  data_time: 0.0166  memory: 17568  grad_norm: 40.4331  loss: 6.5779  loss_cls: 0.2371  loss_bbox: 0.0419  loss_iou: 0.3488  d0.loss_cls: 0.2706  d0.loss_bbox: 0.0459  d0.loss_iou: 0.3712  d1.loss_cls: 0.2418  d1.loss_bbox: 0.0447  d1.loss_iou: 0.3629  d2.loss_cls: 0.2365  d2.loss_bbox: 0.0436  d2.loss_iou: 0.3586  d3.loss_cls: 0.2373  d3.loss_bbox: 0.0418  d3.loss_iou: 0.3484  d4.loss_cls: 0.2385  d4.loss_bbox: 0.0418  d4.loss_iou: 0.3473  enc_loss_cls: 0.2743  enc_loss_bbox: 0.0506  enc_loss_iou: 0.3937  dn_loss_cls: 0.0073  dn_loss_bbox: 0.0327  dn_loss_iou: 0.2676  d0.dn_loss_cls: 0.0337  d0.dn_loss_bbox: 0.0456  d0.dn_loss_iou: 0.3508  d1.dn_loss_cls: 0.0130  d1.dn_loss_bbox: 0.0345  d1.dn_loss_iou: 0.2826  d2.dn_loss_cls: 0.0091  d2.dn_loss_bbox: 0.0329  d2.dn_loss_iou: 0.2704  d3.dn_loss_cls: 0.0080  d3.dn_loss_bbox: 0.0327  d3.dn_loss_iou: 0.2677  d4.dn_loss_cls: 0.0074  d4.dn_loss_bbox: 0.0326  d4.dn_loss_iou: 0.2674  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:38:50 - mmengine - INFO - Epoch(train) [2][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:42:26  time: 2.1258  data_time: 0.0163  memory: 17591  grad_norm: 40.5537  loss: 5.8294  loss_cls: 0.2090  loss_bbox: 0.0353  loss_iou: 0.2869  d0.loss_cls: 0.2439  d0.loss_bbox: 0.0370  d0.loss_iou: 0.2957  d1.loss_cls: 0.2214  d1.loss_bbox: 0.0365  d1.loss_iou: 0.2894  d2.loss_cls: 0.2152  d2.loss_bbox: 0.0355  d2.loss_iou: 0.2879  d3.loss_cls: 0.2108  d3.loss_bbox: 0.0354  d3.loss_iou: 0.2870  d4.loss_cls: 0.2075  d4.loss_bbox: 0.0353  d4.loss_iou: 0.2867  enc_loss_cls: 0.2580  enc_loss_bbox: 0.0397  enc_loss_iou: 0.3112  dn_loss_cls: 0.0085  dn_loss_bbox: 0.0306  dn_loss_iou: 0.2601  d0.dn_loss_cls: 0.0387  d0.dn_loss_bbox: 0.0439  d0.dn_loss_iou: 0.3541  d1.dn_loss_cls: 0.0132  d1.dn_loss_bbox: 0.0324  d1.dn_loss_iou: 0.2753  d2.dn_loss_cls: 0.0098  d2.dn_loss_bbox: 0.0310  d2.dn_loss_iou: 0.2634  d3.dn_loss_cls: 0.0083  d3.dn_loss_bbox: 0.0306  d3.dn_loss_iou: 0.2604  d4.dn_loss_cls: 0.0085  d4.dn_loss_bbox: 0.0306  d4.dn_loss_iou: 0.2599  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0007
2025/10/29 03:40:35 - mmengine - INFO - Epoch(train) [2][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:40:35  time: 2.1102  data_time: 0.0167  memory: 17581  grad_norm: 34.9479  loss: 7.5573  loss_cls: 0.2725  loss_bbox: 0.0569  loss_iou: 0.4655  d0.loss_cls: 0.3031  d0.loss_bbox: 0.0582  d0.loss_iou: 0.4795  d1.loss_cls: 0.2837  d1.loss_bbox: 0.0572  d1.loss_iou: 0.4697  d2.loss_cls: 0.2764  d2.loss_bbox: 0.0572  d2.loss_iou: 0.4621  d3.loss_cls: 0.2709  d3.loss_bbox: 0.0572  d3.loss_iou: 0.4650  d4.loss_cls: 0.2744  d4.loss_bbox: 0.0563  d4.loss_iou: 0.4618  enc_loss_cls: 0.3192  enc_loss_bbox: 0.0643  enc_loss_iou: 0.5049  dn_loss_cls: 0.0094  dn_loss_bbox: 0.0276  dn_loss_iou: 0.2459  d0.dn_loss_cls: 0.0367  d0.dn_loss_bbox: 0.0386  d0.dn_loss_iou: 0.3228  d1.dn_loss_cls: 0.0138  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2588  d2.dn_loss_cls: 0.0109  d2.dn_loss_bbox: 0.0280  d2.dn_loss_iou: 0.2482  d3.dn_loss_cls: 0.0098  d3.dn_loss_bbox: 0.0277  d3.dn_loss_iou: 0.2465  d4.dn_loss_cls: 0.0095  d4.dn_loss_bbox: 0.0276  d4.dn_loss_iou: 0.2458  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:42:22 - mmengine - INFO - Epoch(train) [2][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:38:50  time: 2.1423  data_time: 0.0166  memory: 17574  grad_norm: 36.6371  loss: 6.7003  loss_cls: 0.2582  loss_bbox: 0.0419  loss_iou: 0.3752  d0.loss_cls: 0.2962  d0.loss_bbox: 0.0461  d0.loss_iou: 0.3904  d1.loss_cls: 0.2727  d1.loss_bbox: 0.0433  d1.loss_iou: 0.3823  d2.loss_cls: 0.2573  d2.loss_bbox: 0.0438  d2.loss_iou: 0.3821  d3.loss_cls: 0.2542  d3.loss_bbox: 0.0434  d3.loss_iou: 0.3780  d4.loss_cls: 0.2519  d4.loss_bbox: 0.0430  d4.loss_iou: 0.3789  enc_loss_cls: 0.2962  enc_loss_bbox: 0.0514  enc_loss_iou: 0.4152  dn_loss_cls: 0.0065  dn_loss_bbox: 0.0267  dn_loss_iou: 0.2442  d0.dn_loss_cls: 0.0288  d0.dn_loss_bbox: 0.0371  d0.dn_loss_iou: 0.3179  d1.dn_loss_cls: 0.0111  d1.dn_loss_bbox: 0.0282  d1.dn_loss_iou: 0.2565  d2.dn_loss_cls: 0.0073  d2.dn_loss_bbox: 0.0271  d2.dn_loss_iou: 0.2474  d3.dn_loss_cls: 0.0067  d3.dn_loss_bbox: 0.0268  d3.dn_loss_iou: 0.2445  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0267  d4.dn_loss_iou: 0.2441  loss_num: 0.0007  d0.loss_num: 0.0010  d1.loss_num: 0.0007  d2.loss_num: 0.0008  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:44:08 - mmengine - INFO - Epoch(train) [2][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:37:00  time: 2.1146  data_time: 0.0166  memory: 17566  grad_norm: 42.1402  loss: 6.2661  loss_cls: 0.2254  loss_bbox: 0.0409  loss_iou: 0.3357  d0.loss_cls: 0.2761  d0.loss_bbox: 0.0442  d0.loss_iou: 0.3507  d1.loss_cls: 0.2394  d1.loss_bbox: 0.0431  d1.loss_iou: 0.3442  d2.loss_cls: 0.2320  d2.loss_bbox: 0.0416  d2.loss_iou: 0.3390  d3.loss_cls: 0.2283  d3.loss_bbox: 0.0416  d3.loss_iou: 0.3347  d4.loss_cls: 0.2257  d4.loss_bbox: 0.0415  d4.loss_iou: 0.3343  enc_loss_cls: 0.2828  enc_loss_bbox: 0.0457  enc_loss_iou: 0.3623  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2490  d0.dn_loss_cls: 0.0311  d0.dn_loss_bbox: 0.0423  d0.dn_loss_iou: 0.3370  d1.dn_loss_cls: 0.0107  d1.dn_loss_bbox: 0.0309  d1.dn_loss_iou: 0.2623  d2.dn_loss_cls: 0.0069  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2516  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0291  d3.dn_loss_iou: 0.2493  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2487  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:45:55 - mmengine - INFO - Epoch(train) [2][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:35:14  time: 2.1381  data_time: 0.0208  memory: 17590  grad_norm: 45.0493  loss: 6.9093  loss_cls: 0.2494  loss_bbox: 0.0454  loss_iou: 0.3872  d0.loss_cls: 0.2841  d0.loss_bbox: 0.0492  d0.loss_iou: 0.4101  d1.loss_cls: 0.2681  d1.loss_bbox: 0.0470  d1.loss_iou: 0.3997  d2.loss_cls: 0.2579  d2.loss_bbox: 0.0464  d2.loss_iou: 0.3916  d3.loss_cls: 0.2523  d3.loss_bbox: 0.0460  d3.loss_iou: 0.3910  d4.loss_cls: 0.2501  d4.loss_bbox: 0.0455  d4.loss_iou: 0.3884  enc_loss_cls: 0.2929  enc_loss_bbox: 0.0527  enc_loss_iou: 0.4370  dn_loss_cls: 0.0068  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2595  d0.dn_loss_cls: 0.0350  d0.dn_loss_bbox: 0.0404  d0.dn_loss_iou: 0.3399  d1.dn_loss_cls: 0.0111  d1.dn_loss_bbox: 0.0307  d1.dn_loss_iou: 0.2716  d2.dn_loss_cls: 0.0074  d2.dn_loss_bbox: 0.0295  d2.dn_loss_iou: 0.2616  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2596  d4.dn_loss_cls: 0.0069  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2590  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0006  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0007
2025/10/29 03:47:42 - mmengine - INFO - Epoch(train) [2][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:33:29  time: 2.1440  data_time: 0.0175  memory: 17575  grad_norm: 47.0989  loss: 6.2914  loss_cls: 0.2301  loss_bbox: 0.0406  loss_iou: 0.3407  d0.loss_cls: 0.2629  d0.loss_bbox: 0.0425  d0.loss_iou: 0.3530  d1.loss_cls: 0.2426  d1.loss_bbox: 0.0414  d1.loss_iou: 0.3467  d2.loss_cls: 0.2339  d2.loss_bbox: 0.0409  d2.loss_iou: 0.3409  d3.loss_cls: 0.2315  d3.loss_bbox: 0.0408  d3.loss_iou: 0.3388  d4.loss_cls: 0.2281  d4.loss_bbox: 0.0407  d4.loss_iou: 0.3400  enc_loss_cls: 0.2822  enc_loss_bbox: 0.0442  enc_loss_iou: 0.3653  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0285  dn_loss_iou: 0.2518  d0.dn_loss_cls: 0.0302  d0.dn_loss_bbox: 0.0405  d0.dn_loss_iou: 0.3354  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0302  d1.dn_loss_iou: 0.2649  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0289  d2.dn_loss_iou: 0.2551  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0286  d3.dn_loss_iou: 0.2518  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0285  d4.dn_loss_iou: 0.2515  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 03:49:28 - mmengine - INFO - Epoch(train) [2][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:31:41  time: 2.1257  data_time: 0.0164  memory: 17579  grad_norm: 44.4884  loss: 6.4279  loss_cls: 0.2180  loss_bbox: 0.0435  loss_iou: 0.3568  d0.loss_cls: 0.2526  d0.loss_bbox: 0.0469  d0.loss_iou: 0.3742  d1.loss_cls: 0.2276  d1.loss_bbox: 0.0444  d1.loss_iou: 0.3626  d2.loss_cls: 0.2200  d2.loss_bbox: 0.0446  d2.loss_iou: 0.3618  d3.loss_cls: 0.2177  d3.loss_bbox: 0.0441  d3.loss_iou: 0.3595  d4.loss_cls: 0.2177  d4.loss_bbox: 0.0436  d4.loss_iou: 0.3576  enc_loss_cls: 0.2637  enc_loss_bbox: 0.0496  enc_loss_iou: 0.3941  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0330  dn_loss_iou: 0.2568  d0.dn_loss_cls: 0.0289  d0.dn_loss_bbox: 0.0468  d0.dn_loss_iou: 0.3473  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0350  d1.dn_loss_iou: 0.2731  d2.dn_loss_cls: 0.0058  d2.dn_loss_bbox: 0.0335  d2.dn_loss_iou: 0.2604  d3.dn_loss_cls: 0.0051  d3.dn_loss_bbox: 0.0330  d3.dn_loss_iou: 0.2569  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0330  d4.dn_loss_iou: 0.2566  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 03:51:16 - mmengine - INFO - Epoch(train) [2][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:29:56  time: 2.1460  data_time: 0.0165  memory: 17574  grad_norm: 42.4502  loss: 5.9556  loss_cls: 0.2053  loss_bbox: 0.0384  loss_iou: 0.3182  d0.loss_cls: 0.2375  d0.loss_bbox: 0.0424  d0.loss_iou: 0.3400  d1.loss_cls: 0.2132  d1.loss_bbox: 0.0416  d1.loss_iou: 0.3359  d2.loss_cls: 0.2104  d2.loss_bbox: 0.0391  d2.loss_iou: 0.3261  d3.loss_cls: 0.2064  d3.loss_bbox: 0.0387  d3.loss_iou: 0.3211  d4.loss_cls: 0.2051  d4.loss_bbox: 0.0388  d4.loss_iou: 0.3211  enc_loss_cls: 0.2520  enc_loss_bbox: 0.0443  enc_loss_iou: 0.3532  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0301  dn_loss_iou: 0.2453  d0.dn_loss_cls: 0.0294  d0.dn_loss_bbox: 0.0418  d0.dn_loss_iou: 0.3224  d1.dn_loss_cls: 0.0100  d1.dn_loss_bbox: 0.0320  d1.dn_loss_iou: 0.2592  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2475  d3.dn_loss_cls: 0.0061  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2452  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0301  d4.dn_loss_iou: 0.2449  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0006  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 03:53:01 - mmengine - INFO - Epoch(train) [2][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:28:07  time: 2.1156  data_time: 0.0182  memory: 17568  grad_norm: 37.0565  loss: 5.6544  loss_cls: 0.2112  loss_bbox: 0.0355  loss_iou: 0.2796  d0.loss_cls: 0.2447  d0.loss_bbox: 0.0382  d0.loss_iou: 0.2936  d1.loss_cls: 0.2244  d1.loss_bbox: 0.0370  d1.loss_iou: 0.2889  d2.loss_cls: 0.2174  d2.loss_bbox: 0.0361  d2.loss_iou: 0.2810  d3.loss_cls: 0.2107  d3.loss_bbox: 0.0356  d3.loss_iou: 0.2813  d4.loss_cls: 0.2111  d4.loss_bbox: 0.0356  d4.loss_iou: 0.2810  enc_loss_cls: 0.2592  enc_loss_bbox: 0.0417  enc_loss_iou: 0.3099  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2428  d0.dn_loss_cls: 0.0270  d0.dn_loss_bbox: 0.0404  d0.dn_loss_iou: 0.3235  d1.dn_loss_cls: 0.0078  d1.dn_loss_bbox: 0.0309  d1.dn_loss_iou: 0.2566  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2454  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0290  d3.dn_loss_iou: 0.2431  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0289  d4.dn_loss_iou: 0.2426  loss_num: 0.0008  d0.loss_num: 0.0009  d1.loss_num: 0.0008  d2.loss_num: 0.0008  d3.loss_num: 0.0008  d4.loss_num: 0.0008
2025/10/29 03:54:48 - mmengine - INFO - Epoch(train) [2][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:26:19  time: 2.1285  data_time: 0.0168  memory: 17568  grad_norm: 36.5892  loss: 5.3046  loss_cls: 0.1750  loss_bbox: 0.0323  loss_iou: 0.2775  d0.loss_cls: 0.2037  d0.loss_bbox: 0.0350  d0.loss_iou: 0.2948  d1.loss_cls: 0.1815  d1.loss_bbox: 0.0333  d1.loss_iou: 0.2867  d2.loss_cls: 0.1753  d2.loss_bbox: 0.0327  d2.loss_iou: 0.2816  d3.loss_cls: 0.1737  d3.loss_bbox: 0.0323  d3.loss_iou: 0.2773  d4.loss_cls: 0.1721  d4.loss_bbox: 0.0324  d4.loss_iou: 0.2793  enc_loss_cls: 0.2173  enc_loss_bbox: 0.0364  enc_loss_iou: 0.3044  dn_loss_cls: 0.0077  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2357  d0.dn_loss_cls: 0.0278  d0.dn_loss_bbox: 0.0409  d0.dn_loss_iou: 0.3131  d1.dn_loss_cls: 0.0106  d1.dn_loss_bbox: 0.0310  d1.dn_loss_iou: 0.2478  d2.dn_loss_cls: 0.0088  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2387  d3.dn_loss_cls: 0.0079  d3.dn_loss_bbox: 0.0292  d3.dn_loss_iou: 0.2360  d4.dn_loss_cls: 0.0074  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2355  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 03:56:35 - mmengine - INFO - Epoch(train) [2][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:24:34  time: 2.1466  data_time: 0.0172  memory: 17575  grad_norm: 39.1108  loss: 6.2840  loss_cls: 0.2255  loss_bbox: 0.0424  loss_iou: 0.3389  d0.loss_cls: 0.2582  d0.loss_bbox: 0.0446  d0.loss_iou: 0.3469  d1.loss_cls: 0.2341  d1.loss_bbox: 0.0428  d1.loss_iou: 0.3390  d2.loss_cls: 0.2281  d2.loss_bbox: 0.0426  d2.loss_iou: 0.3390  d3.loss_cls: 0.2257  d3.loss_bbox: 0.0418  d3.loss_iou: 0.3382  d4.loss_cls: 0.2254  d4.loss_bbox: 0.0424  d4.loss_iou: 0.3390  enc_loss_cls: 0.2747  enc_loss_bbox: 0.0457  enc_loss_iou: 0.3633  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0301  dn_loss_iou: 0.2572  d0.dn_loss_cls: 0.0317  d0.dn_loss_bbox: 0.0426  d0.dn_loss_iou: 0.3385  d1.dn_loss_cls: 0.0096  d1.dn_loss_bbox: 0.0321  d1.dn_loss_iou: 0.2712  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0305  d2.dn_loss_iou: 0.2594  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2576  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0301  d4.dn_loss_iou: 0.2570  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0006  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 03:58:22 - mmengine - INFO - Epoch(train) [2][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:22:47  time: 2.1272  data_time: 0.0166  memory: 17584  grad_norm: 42.5071  loss: 6.2176  loss_cls: 0.2225  loss_bbox: 0.0389  loss_iou: 0.3454  d0.loss_cls: 0.2571  d0.loss_bbox: 0.0418  d0.loss_iou: 0.3618  d1.loss_cls: 0.2332  d1.loss_bbox: 0.0406  d1.loss_iou: 0.3559  d2.loss_cls: 0.2259  d2.loss_bbox: 0.0387  d2.loss_iou: 0.3447  d3.loss_cls: 0.2245  d3.loss_bbox: 0.0389  d3.loss_iou: 0.3447  d4.loss_cls: 0.2219  d4.loss_bbox: 0.0389  d4.loss_iou: 0.3456  enc_loss_cls: 0.2683  enc_loss_bbox: 0.0463  enc_loss_iou: 0.3888  dn_loss_cls: 0.0066  dn_loss_bbox: 0.0280  dn_loss_iou: 0.2411  d0.dn_loss_cls: 0.0283  d0.dn_loss_bbox: 0.0393  d0.dn_loss_iou: 0.3193  d1.dn_loss_cls: 0.0105  d1.dn_loss_bbox: 0.0297  d1.dn_loss_iou: 0.2540  d2.dn_loss_cls: 0.0073  d2.dn_loss_bbox: 0.0285  d2.dn_loss_iou: 0.2443  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0281  d3.dn_loss_iou: 0.2415  d4.dn_loss_cls: 0.0066  d4.dn_loss_bbox: 0.0280  d4.dn_loss_iou: 0.2409  loss_num: 0.0007  d0.loss_num: 0.0008  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 04:00:08 - mmengine - INFO - Epoch(train) [2][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:20:59  time: 2.1257  data_time: 0.0181  memory: 17574  grad_norm: 44.0377  loss: 5.7491  loss_cls: 0.1977  loss_bbox: 0.0348  loss_iou: 0.3097  d0.loss_cls: 0.2244  d0.loss_bbox: 0.0371  d0.loss_iou: 0.3218  d1.loss_cls: 0.2026  d1.loss_bbox: 0.0367  d1.loss_iou: 0.3186  d2.loss_cls: 0.2020  d2.loss_bbox: 0.0354  d2.loss_iou: 0.3109  d3.loss_cls: 0.2002  d3.loss_bbox: 0.0344  d3.loss_iou: 0.3076  d4.loss_cls: 0.1954  d4.loss_bbox: 0.0349  d4.loss_iou: 0.3110  enc_loss_cls: 0.2372  enc_loss_bbox: 0.0398  enc_loss_iou: 0.3392  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0281  dn_loss_iou: 0.2477  d0.dn_loss_cls: 0.0260  d0.dn_loss_bbox: 0.0390  d0.dn_loss_iou: 0.3268  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0295  d1.dn_loss_iou: 0.2603  d2.dn_loss_cls: 0.0055  d2.dn_loss_bbox: 0.0283  d2.dn_loss_iou: 0.2500  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0281  d3.dn_loss_iou: 0.2480  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.0281  d4.dn_loss_iou: 0.2476  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:01:55 - mmengine - INFO - Epoch(train) [2][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:19:14  time: 2.1421  data_time: 0.0165  memory: 17589  grad_norm: 42.5794  loss: 6.3708  loss_cls: 0.2206  loss_bbox: 0.0402  loss_iou: 0.3512  d0.loss_cls: 0.2691  d0.loss_bbox: 0.0411  d0.loss_iou: 0.3563  d1.loss_cls: 0.2418  d1.loss_bbox: 0.0396  d1.loss_iou: 0.3497  d2.loss_cls: 0.2275  d2.loss_bbox: 0.0401  d2.loss_iou: 0.3511  d3.loss_cls: 0.2237  d3.loss_bbox: 0.0397  d3.loss_iou: 0.3477  d4.loss_cls: 0.2215  d4.loss_bbox: 0.0402  d4.loss_iou: 0.3515  enc_loss_cls: 0.2837  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3717  dn_loss_cls: 0.0061  dn_loss_bbox: 0.0294  dn_loss_iou: 0.2572  d0.dn_loss_cls: 0.0352  d0.dn_loss_bbox: 0.0421  d0.dn_loss_iou: 0.3458  d1.dn_loss_cls: 0.0105  d1.dn_loss_bbox: 0.0314  d1.dn_loss_iou: 0.2730  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0299  d2.dn_loss_iou: 0.2611  d3.dn_loss_cls: 0.0065  d3.dn_loss_bbox: 0.0295  d3.dn_loss_iou: 0.2575  d4.dn_loss_cls: 0.0061  d4.dn_loss_bbox: 0.0294  d4.dn_loss_iou: 0.2569  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 04:03:41 - mmengine - INFO - Epoch(train) [2][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:17:25  time: 2.1220  data_time: 0.0177  memory: 17581  grad_norm: 36.3629  loss: 6.3261  loss_cls: 0.2111  loss_bbox: 0.0419  loss_iou: 0.3665  d0.loss_cls: 0.2489  d0.loss_bbox: 0.0451  d0.loss_iou: 0.3830  d1.loss_cls: 0.2214  d1.loss_bbox: 0.0436  d1.loss_iou: 0.3744  d2.loss_cls: 0.2181  d2.loss_bbox: 0.0425  d2.loss_iou: 0.3681  d3.loss_cls: 0.2134  d3.loss_bbox: 0.0420  d3.loss_iou: 0.3675  d4.loss_cls: 0.2093  d4.loss_bbox: 0.0421  d4.loss_iou: 0.3684  enc_loss_cls: 0.2583  enc_loss_bbox: 0.0480  enc_loss_iou: 0.4052  dn_loss_cls: 0.0065  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2438  d0.dn_loss_cls: 0.0303  d0.dn_loss_bbox: 0.0384  d0.dn_loss_iou: 0.3228  d1.dn_loss_cls: 0.0106  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2577  d2.dn_loss_cls: 0.0077  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2460  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2438  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2435  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:05:28 - mmengine - INFO - Epoch(train) [2][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:15:40  time: 2.1437  data_time: 0.0175  memory: 17593  grad_norm: 44.8605  loss: 6.5662  loss_cls: 0.2373  loss_bbox: 0.0436  loss_iou: 0.3785  d0.loss_cls: 0.2724  d0.loss_bbox: 0.0468  d0.loss_iou: 0.3932  d1.loss_cls: 0.2521  d1.loss_bbox: 0.0426  d1.loss_iou: 0.3800  d2.loss_cls: 0.2487  d2.loss_bbox: 0.0406  d2.loss_iou: 0.3736  d3.loss_cls: 0.2456  d3.loss_bbox: 0.0407  d3.loss_iou: 0.3698  d4.loss_cls: 0.2404  d4.loss_bbox: 0.0415  d4.loss_iou: 0.3752  enc_loss_cls: 0.2920  enc_loss_bbox: 0.0482  enc_loss_iou: 0.4077  dn_loss_cls: 0.0070  dn_loss_bbox: 0.0278  dn_loss_iou: 0.2403  d0.dn_loss_cls: 0.0316  d0.dn_loss_bbox: 0.0395  d0.dn_loss_iou: 0.3194  d1.dn_loss_cls: 0.0119  d1.dn_loss_bbox: 0.0295  d1.dn_loss_iou: 0.2542  d2.dn_loss_cls: 0.0081  d2.dn_loss_bbox: 0.0283  d2.dn_loss_iou: 0.2435  d3.dn_loss_cls: 0.0073  d3.dn_loss_bbox: 0.0278  d3.dn_loss_iou: 0.2405  d4.dn_loss_cls: 0.0069  d4.dn_loss_bbox: 0.0278  d4.dn_loss_iou: 0.2400  loss_num: 0.0007  d0.loss_num: 0.0009  d1.loss_num: 0.0007  d2.loss_num: 0.0007  d3.loss_num: 0.0007  d4.loss_num: 0.0007
2025/10/29 04:06:00 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 04:07:14 - mmengine - INFO - Epoch(train) [2][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:13:51  time: 2.1145  data_time: 0.0163  memory: 17585  grad_norm: 39.2750  loss: 6.0391  loss_cls: 0.2082  loss_bbox: 0.0367  loss_iou: 0.3324  d0.loss_cls: 0.2557  d0.loss_bbox: 0.0414  d0.loss_iou: 0.3558  d1.loss_cls: 0.2195  d1.loss_bbox: 0.0380  d1.loss_iou: 0.3448  d2.loss_cls: 0.2133  d2.loss_bbox: 0.0380  d2.loss_iou: 0.3414  d3.loss_cls: 0.2087  d3.loss_bbox: 0.0361  d3.loss_iou: 0.3342  d4.loss_cls: 0.2061  d4.loss_bbox: 0.0370  d4.loss_iou: 0.3349  enc_loss_cls: 0.2573  enc_loss_bbox: 0.0429  enc_loss_iou: 0.3701  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0270  dn_loss_iou: 0.2405  d0.dn_loss_cls: 0.0332  d0.dn_loss_bbox: 0.0380  d0.dn_loss_iou: 0.3175  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0288  d1.dn_loss_iou: 0.2545  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0273  d2.dn_loss_iou: 0.2427  d3.dn_loss_cls: 0.0067  d3.dn_loss_bbox: 0.0271  d3.dn_loss_iou: 0.2404  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0270  d4.dn_loss_iou: 0.2403  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:09:01 - mmengine - INFO - Epoch(train) [2][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:12:04  time: 2.1331  data_time: 0.0173  memory: 17585  grad_norm: 36.3471  loss: 6.1952  loss_cls: 0.2042  loss_bbox: 0.0375  loss_iou: 0.3317  d0.loss_cls: 0.2476  d0.loss_bbox: 0.0391  d0.loss_iou: 0.3459  d1.loss_cls: 0.2276  d1.loss_bbox: 0.0372  d1.loss_iou: 0.3341  d2.loss_cls: 0.2129  d2.loss_bbox: 0.0365  d2.loss_iou: 0.3296  d3.loss_cls: 0.2053  d3.loss_bbox: 0.0374  d3.loss_iou: 0.3308  d4.loss_cls: 0.2033  d4.loss_bbox: 0.0378  d4.loss_iou: 0.3332  enc_loss_cls: 0.2661  enc_loss_bbox: 0.0408  enc_loss_iou: 0.3639  dn_loss_cls: 0.0066  dn_loss_bbox: 0.0308  dn_loss_iou: 0.2690  d0.dn_loss_cls: 0.0334  d0.dn_loss_bbox: 0.0429  d0.dn_loss_iou: 0.3551  d1.dn_loss_cls: 0.0116  d1.dn_loss_bbox: 0.0325  d1.dn_loss_iou: 0.2829  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0312  d2.dn_loss_iou: 0.2725  d3.dn_loss_cls: 0.0067  d3.dn_loss_bbox: 0.0308  d3.dn_loss_iou: 0.2695  d4.dn_loss_cls: 0.0066  d4.dn_loss_bbox: 0.0307  d4.dn_loss_iou: 0.2687  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 04:10:48 - mmengine - INFO - Epoch(train) [2][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:10:18  time: 2.1384  data_time: 0.0172  memory: 17585  grad_norm: 38.0098  loss: 6.2515  loss_cls: 0.2232  loss_bbox: 0.0442  loss_iou: 0.3445  d0.loss_cls: 0.2767  d0.loss_bbox: 0.0463  d0.loss_iou: 0.3543  d1.loss_cls: 0.2460  d1.loss_bbox: 0.0438  d1.loss_iou: 0.3470  d2.loss_cls: 0.2340  d2.loss_bbox: 0.0440  d2.loss_iou: 0.3460  d3.loss_cls: 0.2319  d3.loss_bbox: 0.0437  d3.loss_iou: 0.3438  d4.loss_cls: 0.2250  d4.loss_bbox: 0.0442  d4.loss_iou: 0.3435  enc_loss_cls: 0.2885  enc_loss_bbox: 0.0507  enc_loss_iou: 0.3745  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0272  dn_loss_iou: 0.2353  d0.dn_loss_cls: 0.0336  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.3130  d1.dn_loss_cls: 0.0108  d1.dn_loss_bbox: 0.0287  d1.dn_loss_iou: 0.2490  d2.dn_loss_cls: 0.0072  d2.dn_loss_bbox: 0.0275  d2.dn_loss_iou: 0.2388  d3.dn_loss_cls: 0.0060  d3.dn_loss_bbox: 0.0273  d3.dn_loss_iou: 0.2359  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0272  d4.dn_loss_iou: 0.2353  loss_num: 0.0006  d0.loss_num: 0.0009  d1.loss_num: 0.0006  d2.loss_num: 0.0007  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 04:12:34 - mmengine - INFO - Epoch(train) [2][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:08:30  time: 2.1226  data_time: 0.0182  memory: 17575  grad_norm: 35.0625  loss: 5.5377  loss_cls: 0.1872  loss_bbox: 0.0380  loss_iou: 0.2909  d0.loss_cls: 0.2146  d0.loss_bbox: 0.0398  d0.loss_iou: 0.3008  d1.loss_cls: 0.1942  d1.loss_bbox: 0.0388  d1.loss_iou: 0.2954  d2.loss_cls: 0.1916  d2.loss_bbox: 0.0381  d2.loss_iou: 0.2915  d3.loss_cls: 0.1889  d3.loss_bbox: 0.0377  d3.loss_iou: 0.2903  d4.loss_cls: 0.1895  d4.loss_bbox: 0.0379  d4.loss_iou: 0.2905  enc_loss_cls: 0.2347  enc_loss_bbox: 0.0426  enc_loss_iou: 0.3178  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0305  dn_loss_iou: 0.2395  d0.dn_loss_cls: 0.0279  d0.dn_loss_bbox: 0.0432  d0.dn_loss_iou: 0.3170  d1.dn_loss_cls: 0.0079  d1.dn_loss_bbox: 0.0328  d1.dn_loss_iou: 0.2539  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0310  d2.dn_loss_iou: 0.2419  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0306  d3.dn_loss_iou: 0.2400  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0305  d4.dn_loss_iou: 0.2393  loss_num: 0.0005  d0.loss_num: 0.0008  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:14:20 - mmengine - INFO - Epoch(train) [2][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:06:44  time: 2.1327  data_time: 0.0180  memory: 17568  grad_norm: 45.2901  loss: 5.8102  loss_cls: 0.2003  loss_bbox: 0.0394  loss_iou: 0.3109  d0.loss_cls: 0.2357  d0.loss_bbox: 0.0415  d0.loss_iou: 0.3250  d1.loss_cls: 0.2099  d1.loss_bbox: 0.0405  d1.loss_iou: 0.3169  d2.loss_cls: 0.2038  d2.loss_bbox: 0.0402  d2.loss_iou: 0.3161  d3.loss_cls: 0.2017  d3.loss_bbox: 0.0393  d3.loss_iou: 0.3082  d4.loss_cls: 0.2011  d4.loss_bbox: 0.0392  d4.loss_iou: 0.3082  enc_loss_cls: 0.2520  enc_loss_bbox: 0.0439  enc_loss_iou: 0.3390  dn_loss_cls: 0.0049  dn_loss_bbox: 0.0299  dn_loss_iou: 0.2406  d0.dn_loss_cls: 0.0300  d0.dn_loss_bbox: 0.0427  d0.dn_loss_iou: 0.3227  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0319  d1.dn_loss_iou: 0.2530  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0304  d2.dn_loss_iou: 0.2430  d3.dn_loss_cls: 0.0051  d3.dn_loss_bbox: 0.0300  d3.dn_loss_iou: 0.2404  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0299  d4.dn_loss_iou: 0.2402  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 04:16:07 - mmengine - INFO - Epoch(train) [2][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:04:58  time: 2.1406  data_time: 0.0172  memory: 17573  grad_norm: 39.1428  loss: 6.0379  loss_cls: 0.2091  loss_bbox: 0.0402  loss_iou: 0.3254  d0.loss_cls: 0.2452  d0.loss_bbox: 0.0428  d0.loss_iou: 0.3454  d1.loss_cls: 0.2279  d1.loss_bbox: 0.0415  d1.loss_iou: 0.3372  d2.loss_cls: 0.2157  d2.loss_bbox: 0.0406  d2.loss_iou: 0.3325  d3.loss_cls: 0.2108  d3.loss_bbox: 0.0404  d3.loss_iou: 0.3302  d4.loss_cls: 0.2097  d4.loss_bbox: 0.0406  d4.loss_iou: 0.3272  enc_loss_cls: 0.2607  enc_loss_bbox: 0.0480  enc_loss_iou: 0.3729  dn_loss_cls: 0.0075  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2419  d0.dn_loss_cls: 0.0327  d0.dn_loss_bbox: 0.0374  d0.dn_loss_iou: 0.3152  d1.dn_loss_cls: 0.0114  d1.dn_loss_bbox: 0.0288  d1.dn_loss_iou: 0.2542  d2.dn_loss_cls: 0.0081  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2447  d3.dn_loss_cls: 0.0075  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2420  d4.dn_loss_cls: 0.0075  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2418  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:17:53 - mmengine - INFO - Epoch(train) [2][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:03:10  time: 2.1223  data_time: 0.0169  memory: 17586  grad_norm: 39.2795  loss: 5.5859  loss_cls: 0.1909  loss_bbox: 0.0359  loss_iou: 0.2894  d0.loss_cls: 0.2323  d0.loss_bbox: 0.0372  d0.loss_iou: 0.2970  d1.loss_cls: 0.2129  d1.loss_bbox: 0.0364  d1.loss_iou: 0.2925  d2.loss_cls: 0.1999  d2.loss_bbox: 0.0361  d2.loss_iou: 0.2906  d3.loss_cls: 0.1941  d3.loss_bbox: 0.0359  d3.loss_iou: 0.2891  d4.loss_cls: 0.1907  d4.loss_bbox: 0.0358  d4.loss_iou: 0.2886  enc_loss_cls: 0.2510  enc_loss_bbox: 0.0416  enc_loss_iou: 0.3162  dn_loss_cls: 0.0049  dn_loss_bbox: 0.0303  dn_loss_iou: 0.2398  d0.dn_loss_cls: 0.0287  d0.dn_loss_bbox: 0.0421  d0.dn_loss_iou: 0.3205  d1.dn_loss_cls: 0.0087  d1.dn_loss_bbox: 0.0321  d1.dn_loss_iou: 0.2525  d2.dn_loss_cls: 0.0059  d2.dn_loss_bbox: 0.0308  d2.dn_loss_iou: 0.2424  d3.dn_loss_cls: 0.0051  d3.dn_loss_bbox: 0.0304  d3.dn_loss_iou: 0.2396  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0303  d4.dn_loss_iou: 0.2394  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:19:40 - mmengine - INFO - Epoch(train) [2][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 4:01:23  time: 2.1324  data_time: 0.0167  memory: 17565  grad_norm: 35.2143  loss: 5.6611  loss_cls: 0.1786  loss_bbox: 0.0372  loss_iou: 0.3139  d0.loss_cls: 0.2075  d0.loss_bbox: 0.0410  d0.loss_iou: 0.3331  d1.loss_cls: 0.1970  d1.loss_bbox: 0.0373  d1.loss_iou: 0.3182  d2.loss_cls: 0.1871  d2.loss_bbox: 0.0369  d2.loss_iou: 0.3135  d3.loss_cls: 0.1835  d3.loss_bbox: 0.0362  d3.loss_iou: 0.3124  d4.loss_cls: 0.1812  d4.loss_bbox: 0.0364  d4.loss_iou: 0.3127  enc_loss_cls: 0.2157  enc_loss_bbox: 0.0439  enc_loss_iou: 0.3485  dn_loss_cls: 0.0050  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2426  d0.dn_loss_cls: 0.0294  d0.dn_loss_bbox: 0.0385  d0.dn_loss_iou: 0.3210  d1.dn_loss_cls: 0.0094  d1.dn_loss_bbox: 0.0289  d1.dn_loss_iou: 0.2550  d2.dn_loss_cls: 0.0060  d2.dn_loss_bbox: 0.0278  d2.dn_loss_iou: 0.2448  d3.dn_loss_cls: 0.0050  d3.dn_loss_bbox: 0.0275  d3.dn_loss_iou: 0.2426  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0275  d4.dn_loss_iou: 0.2423  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:21:27 - mmengine - INFO - Epoch(train) [2][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:59:37  time: 2.1376  data_time: 0.0170  memory: 17581  grad_norm: 39.4601  loss: 5.5015  loss_cls: 0.1739  loss_bbox: 0.0357  loss_iou: 0.2909  d0.loss_cls: 0.2370  d0.loss_bbox: 0.0379  d0.loss_iou: 0.3038  d1.loss_cls: 0.1977  d1.loss_bbox: 0.0354  d1.loss_iou: 0.2914  d2.loss_cls: 0.1840  d2.loss_bbox: 0.0351  d2.loss_iou: 0.2891  d3.loss_cls: 0.1784  d3.loss_bbox: 0.0352  d3.loss_iou: 0.2895  d4.loss_cls: 0.1754  d4.loss_bbox: 0.0351  d4.loss_iou: 0.2888  enc_loss_cls: 0.2298  enc_loss_bbox: 0.0434  enc_loss_iou: 0.3309  dn_loss_cls: 0.0069  dn_loss_bbox: 0.0280  dn_loss_iou: 0.2383  d0.dn_loss_cls: 0.0307  d0.dn_loss_bbox: 0.0397  d0.dn_loss_iou: 0.3220  d1.dn_loss_cls: 0.0108  d1.dn_loss_bbox: 0.0295  d1.dn_loss_iou: 0.2510  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0283  d2.dn_loss_iou: 0.2407  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0281  d3.dn_loss_iou: 0.2387  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0280  d4.dn_loss_iou: 0.2381  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:23:13 - mmengine - INFO - Epoch(train) [2][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:57:49  time: 2.1214  data_time: 0.0170  memory: 17568  grad_norm: 43.7600  loss: 5.1934  loss_cls: 0.1586  loss_bbox: 0.0314  loss_iou: 0.2764  d0.loss_cls: 0.2121  d0.loss_bbox: 0.0336  d0.loss_iou: 0.2883  d1.loss_cls: 0.1794  d1.loss_bbox: 0.0319  d1.loss_iou: 0.2791  d2.loss_cls: 0.1698  d2.loss_bbox: 0.0315  d2.loss_iou: 0.2756  d3.loss_cls: 0.1619  d3.loss_bbox: 0.0315  d3.loss_iou: 0.2768  d4.loss_cls: 0.1583  d4.loss_bbox: 0.0316  d4.loss_iou: 0.2782  enc_loss_cls: 0.2045  enc_loss_bbox: 0.0368  enc_loss_iou: 0.3099  dn_loss_cls: 0.0057  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2343  d0.dn_loss_cls: 0.0273  d0.dn_loss_bbox: 0.0372  d0.dn_loss_iou: 0.3098  d1.dn_loss_cls: 0.0103  d1.dn_loss_bbox: 0.0286  d1.dn_loss_iou: 0.2462  d2.dn_loss_cls: 0.0069  d2.dn_loss_bbox: 0.0276  d2.dn_loss_iou: 0.2369  d3.dn_loss_cls: 0.0059  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2347  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2340  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:25:01 - mmengine - INFO - Epoch(train) [2][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:56:04  time: 2.1516  data_time: 0.0161  memory: 17586  grad_norm: 38.2388  loss: 6.6267  loss_cls: 0.2064  loss_bbox: 0.0418  loss_iou: 0.3933  d0.loss_cls: 0.2755  d0.loss_bbox: 0.0451  d0.loss_iou: 0.4107  d1.loss_cls: 0.2266  d1.loss_bbox: 0.0437  d1.loss_iou: 0.4022  d2.loss_cls: 0.2219  d2.loss_bbox: 0.0417  d2.loss_iou: 0.3917  d3.loss_cls: 0.2089  d3.loss_bbox: 0.0428  d3.loss_iou: 0.3952  d4.loss_cls: 0.2067  d4.loss_bbox: 0.0419  d4.loss_iou: 0.3945  enc_loss_cls: 0.2735  enc_loss_bbox: 0.0472  enc_loss_iou: 0.4336  dn_loss_cls: 0.0074  dn_loss_bbox: 0.0279  dn_loss_iou: 0.2552  d0.dn_loss_cls: 0.0281  d0.dn_loss_bbox: 0.0386  d0.dn_loss_iou: 0.3383  d1.dn_loss_cls: 0.0094  d1.dn_loss_bbox: 0.0293  d1.dn_loss_iou: 0.2687  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0281  d2.dn_loss_iou: 0.2580  d3.dn_loss_cls: 0.0071  d3.dn_loss_bbox: 0.0279  d3.dn_loss_iou: 0.2559  d4.dn_loss_cls: 0.0073  d4.dn_loss_bbox: 0.0279  d4.dn_loss_iou: 0.2552  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 04:26:47 - mmengine - INFO - Epoch(train) [2][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:54:17  time: 2.1249  data_time: 0.0174  memory: 17575  grad_norm: 44.5038  loss: 6.0715  loss_cls: 0.1946  loss_bbox: 0.0442  loss_iou: 0.3467  d0.loss_cls: 0.2396  d0.loss_bbox: 0.0459  d0.loss_iou: 0.3636  d1.loss_cls: 0.2107  d1.loss_bbox: 0.0456  d1.loss_iou: 0.3573  d2.loss_cls: 0.2083  d2.loss_bbox: 0.0434  d2.loss_iou: 0.3470  d3.loss_cls: 0.2030  d3.loss_bbox: 0.0432  d3.loss_iou: 0.3438  d4.loss_cls: 0.1963  d4.loss_bbox: 0.0443  d4.loss_iou: 0.3467  enc_loss_cls: 0.2462  enc_loss_bbox: 0.0509  enc_loss_iou: 0.3883  dn_loss_cls: 0.0064  dn_loss_bbox: 0.0283  dn_loss_iou: 0.2348  d0.dn_loss_cls: 0.0311  d0.dn_loss_bbox: 0.0395  d0.dn_loss_iou: 0.3145  d1.dn_loss_cls: 0.0121  d1.dn_loss_bbox: 0.0300  d1.dn_loss_iou: 0.2478  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0286  d2.dn_loss_iou: 0.2378  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0284  d3.dn_loss_iou: 0.2354  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0283  d4.dn_loss_iou: 0.2345  loss_num: 0.0006  d0.loss_num: 0.0007  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 04:28:33 - mmengine - INFO - Epoch(train) [2][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:52:30  time: 2.1306  data_time: 0.0172  memory: 17580  grad_norm: 45.0744  loss: 5.8707  loss_cls: 0.1781  loss_bbox: 0.0396  loss_iou: 0.3318  d0.loss_cls: 0.2199  d0.loss_bbox: 0.0440  d0.loss_iou: 0.3566  d1.loss_cls: 0.1923  d1.loss_bbox: 0.0418  d1.loss_iou: 0.3415  d2.loss_cls: 0.1819  d2.loss_bbox: 0.0408  d2.loss_iou: 0.3358  d3.loss_cls: 0.1835  d3.loss_bbox: 0.0390  d3.loss_iou: 0.3253  d4.loss_cls: 0.1768  d4.loss_bbox: 0.0397  d4.loss_iou: 0.3319  enc_loss_cls: 0.2264  enc_loss_bbox: 0.0454  enc_loss_iou: 0.3665  dn_loss_cls: 0.0060  dn_loss_bbox: 0.0298  dn_loss_iou: 0.2469  d0.dn_loss_cls: 0.0276  d0.dn_loss_bbox: 0.0407  d0.dn_loss_iou: 0.3264  d1.dn_loss_cls: 0.0090  d1.dn_loss_bbox: 0.0315  d1.dn_loss_iou: 0.2597  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0300  d2.dn_loss_iou: 0.2489  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0299  d3.dn_loss_iou: 0.2467  d4.dn_loss_cls: 0.0060  d4.dn_loss_bbox: 0.0298  d4.dn_loss_iou: 0.2465  loss_num: 0.0005  d0.loss_num: 0.0008  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:30:20 - mmengine - INFO - Epoch(train) [2][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:50:44  time: 2.1369  data_time: 0.0168  memory: 17568  grad_norm: 40.9480  loss: 5.6503  loss_cls: 0.1790  loss_bbox: 0.0391  loss_iou: 0.3146  d0.loss_cls: 0.2218  d0.loss_bbox: 0.0417  d0.loss_iou: 0.3343  d1.loss_cls: 0.1924  d1.loss_bbox: 0.0399  d1.loss_iou: 0.3218  d2.loss_cls: 0.1876  d2.loss_bbox: 0.0391  d2.loss_iou: 0.3174  d3.loss_cls: 0.1832  d3.loss_bbox: 0.0392  d3.loss_iou: 0.3143  d4.loss_cls: 0.1770  d4.loss_bbox: 0.0390  d4.loss_iou: 0.3167  enc_loss_cls: 0.2333  enc_loss_bbox: 0.0436  enc_loss_iou: 0.3489  dn_loss_cls: 0.0114  dn_loss_bbox: 0.0259  dn_loss_iou: 0.2279  d0.dn_loss_cls: 0.0341  d0.dn_loss_bbox: 0.0370  d0.dn_loss_iou: 0.3044  d1.dn_loss_cls: 0.0147  d1.dn_loss_bbox: 0.0276  d1.dn_loss_iou: 0.2408  d2.dn_loss_cls: 0.0125  d2.dn_loss_bbox: 0.0263  d2.dn_loss_iou: 0.2301  d3.dn_loss_cls: 0.0110  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2282  d4.dn_loss_cls: 0.0118  d4.dn_loss_bbox: 0.0259  d4.dn_loss_iou: 0.2277  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:32:06 - mmengine - INFO - Epoch(train) [2][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:48:56  time: 2.1257  data_time: 0.0167  memory: 17562  grad_norm: 38.3123  loss: 5.1806  loss_cls: 0.1575  loss_bbox: 0.0326  loss_iou: 0.2766  d0.loss_cls: 0.1931  d0.loss_bbox: 0.0340  d0.loss_iou: 0.2873  d1.loss_cls: 0.1679  d1.loss_bbox: 0.0335  d1.loss_iou: 0.2832  d2.loss_cls: 0.1633  d2.loss_bbox: 0.0329  d2.loss_iou: 0.2772  d3.loss_cls: 0.1603  d3.loss_bbox: 0.0322  d3.loss_iou: 0.2742  d4.loss_cls: 0.1576  d4.loss_bbox: 0.0323  d4.loss_iou: 0.2758  enc_loss_cls: 0.2067  enc_loss_bbox: 0.0372  enc_loss_iou: 0.3024  dn_loss_cls: 0.0070  dn_loss_bbox: 0.0283  dn_loss_iou: 0.2343  d0.dn_loss_cls: 0.0362  d0.dn_loss_bbox: 0.0400  d0.dn_loss_iou: 0.3111  d1.dn_loss_cls: 0.0117  d1.dn_loss_bbox: 0.0300  d1.dn_loss_iou: 0.2475  d2.dn_loss_cls: 0.0088  d2.dn_loss_bbox: 0.0287  d2.dn_loss_iou: 0.2365  d3.dn_loss_cls: 0.0074  d3.dn_loss_bbox: 0.0284  d3.dn_loss_iou: 0.2347  d4.dn_loss_cls: 0.0070  d4.dn_loss_bbox: 0.0283  d4.dn_loss_iou: 0.2340  loss_num: 0.0004  d0.loss_num: 0.0007  d1.loss_num: 0.0004  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 04:33:53 - mmengine - INFO - Epoch(train) [2][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:47:10  time: 2.1346  data_time: 0.0169  memory: 17567  grad_norm: 42.7626  loss: 5.8157  loss_cls: 0.1814  loss_bbox: 0.0371  loss_iou: 0.3290  d0.loss_cls: 0.2112  d0.loss_bbox: 0.0416  d0.loss_iou: 0.3481  d1.loss_cls: 0.1853  d1.loss_bbox: 0.0394  d1.loss_iou: 0.3391  d2.loss_cls: 0.1857  d2.loss_bbox: 0.0380  d2.loss_iou: 0.3306  d3.loss_cls: 0.1865  d3.loss_bbox: 0.0371  d3.loss_iou: 0.3300  d4.loss_cls: 0.1821  d4.loss_bbox: 0.0370  d4.loss_iou: 0.3294  enc_loss_cls: 0.2258  enc_loss_bbox: 0.0440  enc_loss_iou: 0.3688  dn_loss_cls: 0.0083  dn_loss_bbox: 0.0265  dn_loss_iou: 0.2420  d0.dn_loss_cls: 0.0354  d0.dn_loss_bbox: 0.0372  d0.dn_loss_iou: 0.3231  d1.dn_loss_cls: 0.0138  d1.dn_loss_bbox: 0.0282  d1.dn_loss_iou: 0.2560  d2.dn_loss_cls: 0.0101  d2.dn_loss_bbox: 0.0267  d2.dn_loss_iou: 0.2434  d3.dn_loss_cls: 0.0087  d3.dn_loss_bbox: 0.0265  d3.dn_loss_iou: 0.2422  d4.dn_loss_cls: 0.0082  d4.dn_loss_bbox: 0.0265  d4.dn_loss_iou: 0.2418  loss_num: 0.0006  d0.loss_num: 0.0008  d1.loss_num: 0.0006  d2.loss_num: 0.0006  d3.loss_num: 0.0006  d4.loss_num: 0.0006
2025/10/29 04:35:40 - mmengine - INFO - Epoch(train) [2][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:45:23  time: 2.1299  data_time: 0.0170  memory: 17580  grad_norm: 35.4077  loss: 5.7249  loss_cls: 0.1705  loss_bbox: 0.0406  loss_iou: 0.3396  d0.loss_cls: 0.2206  d0.loss_bbox: 0.0430  d0.loss_iou: 0.3595  d1.loss_cls: 0.1798  d1.loss_bbox: 0.0430  d1.loss_iou: 0.3522  d2.loss_cls: 0.1776  d2.loss_bbox: 0.0401  d2.loss_iou: 0.3395  d3.loss_cls: 0.1744  d3.loss_bbox: 0.0400  d3.loss_iou: 0.3387  d4.loss_cls: 0.1698  d4.loss_bbox: 0.0401  d4.loss_iou: 0.3388  enc_loss_cls: 0.2292  enc_loss_bbox: 0.0475  enc_loss_iou: 0.3816  dn_loss_cls: 0.0070  dn_loss_bbox: 0.0259  dn_loss_iou: 0.2213  d0.dn_loss_cls: 0.0338  d0.dn_loss_bbox: 0.0364  d0.dn_loss_iou: 0.2946  d1.dn_loss_cls: 0.0110  d1.dn_loss_bbox: 0.0274  d1.dn_loss_iou: 0.2334  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0262  d2.dn_loss_iou: 0.2235  d3.dn_loss_cls: 0.0071  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2212  d4.dn_loss_cls: 0.0068  d4.dn_loss_bbox: 0.0259  d4.dn_loss_iou: 0.2211  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 04:37:26 - mmengine - INFO - Epoch(train) [2][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:43:36  time: 2.1276  data_time: 0.0169  memory: 17584  grad_norm: 36.8165  loss: 5.5104  loss_cls: 0.1560  loss_bbox: 0.0371  loss_iou: 0.3175  d0.loss_cls: 0.2157  d0.loss_bbox: 0.0393  d0.loss_iou: 0.3306  d1.loss_cls: 0.1689  d1.loss_bbox: 0.0391  d1.loss_iou: 0.3256  d2.loss_cls: 0.1632  d2.loss_bbox: 0.0382  d2.loss_iou: 0.3223  d3.loss_cls: 0.1547  d3.loss_bbox: 0.0379  d3.loss_iou: 0.3194  d4.loss_cls: 0.1542  d4.loss_bbox: 0.0379  d4.loss_iou: 0.3183  enc_loss_cls: 0.2115  enc_loss_bbox: 0.0428  enc_loss_iou: 0.3512  dn_loss_cls: 0.0063  dn_loss_bbox: 0.0282  dn_loss_iou: 0.2308  d0.dn_loss_cls: 0.0301  d0.dn_loss_bbox: 0.0398  d0.dn_loss_iou: 0.3076  d1.dn_loss_cls: 0.0103  d1.dn_loss_bbox: 0.0298  d1.dn_loss_iou: 0.2425  d2.dn_loss_cls: 0.0072  d2.dn_loss_bbox: 0.0285  d2.dn_loss_iou: 0.2339  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0282  d3.dn_loss_iou: 0.2314  d4.dn_loss_cls: 0.0062  d4.dn_loss_bbox: 0.0282  d4.dn_loss_iou: 0.2306  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:39:13 - mmengine - INFO - Epoch(train) [2][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:41:50  time: 2.1397  data_time: 0.0167  memory: 17581  grad_norm: 36.1137  loss: 5.4146  loss_cls: 0.1552  loss_bbox: 0.0343  loss_iou: 0.3019  d0.loss_cls: 0.1987  d0.loss_bbox: 0.0369  d0.loss_iou: 0.3116  d1.loss_cls: 0.1696  d1.loss_bbox: 0.0357  d1.loss_iou: 0.3065  d2.loss_cls: 0.1609  d2.loss_bbox: 0.0347  d2.loss_iou: 0.3048  d3.loss_cls: 0.1549  d3.loss_bbox: 0.0344  d3.loss_iou: 0.3020  d4.loss_cls: 0.1545  d4.loss_bbox: 0.0341  d4.loss_iou: 0.3008  enc_loss_cls: 0.2016  enc_loss_bbox: 0.0442  enc_loss_iou: 0.3384  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0286  dn_loss_iou: 0.2428  d0.dn_loss_cls: 0.0295  d0.dn_loss_bbox: 0.0396  d0.dn_loss_iou: 0.3222  d1.dn_loss_cls: 0.0095  d1.dn_loss_bbox: 0.0299  d1.dn_loss_iou: 0.2559  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0288  d2.dn_loss_iou: 0.2454  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.0286  d3.dn_loss_iou: 0.2429  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0286  d4.dn_loss_iou: 0.2427  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 04:41:00 - mmengine - INFO - Epoch(train) [2][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:40:03  time: 2.1325  data_time: 0.0182  memory: 17580  grad_norm: 40.9854  loss: 5.3793  loss_cls: 0.1744  loss_bbox: 0.0321  loss_iou: 0.2833  d0.loss_cls: 0.2236  d0.loss_bbox: 0.0334  d0.loss_iou: 0.2964  d1.loss_cls: 0.1940  d1.loss_bbox: 0.0329  d1.loss_iou: 0.2909  d2.loss_cls: 0.1824  d2.loss_bbox: 0.0323  d2.loss_iou: 0.2863  d3.loss_cls: 0.1776  d3.loss_bbox: 0.0317  d3.loss_iou: 0.2809  d4.loss_cls: 0.1738  d4.loss_bbox: 0.0321  d4.loss_iou: 0.2835  enc_loss_cls: 0.2319  enc_loss_bbox: 0.0360  enc_loss_iou: 0.3160  dn_loss_cls: 0.0093  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2338  d0.dn_loss_cls: 0.0314  d0.dn_loss_bbox: 0.0374  d0.dn_loss_iou: 0.3077  d1.dn_loss_cls: 0.0123  d1.dn_loss_bbox: 0.0290  d1.dn_loss_iou: 0.2469  d2.dn_loss_cls: 0.0098  d2.dn_loss_bbox: 0.0279  d2.dn_loss_iou: 0.2364  d3.dn_loss_cls: 0.0094  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2341  d4.dn_loss_cls: 0.0091  d4.dn_loss_bbox: 0.0275  d4.dn_loss_iou: 0.2335  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:41:32 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 04:42:46 - mmengine - INFO - Epoch(train) [2][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:38:16  time: 2.1310  data_time: 0.0177  memory: 17573  grad_norm: 40.3521  loss: 5.7612  loss_cls: 0.1734  loss_bbox: 0.0368  loss_iou: 0.3167  d0.loss_cls: 0.2179  d0.loss_bbox: 0.0400  d0.loss_iou: 0.3308  d1.loss_cls: 0.1893  d1.loss_bbox: 0.0376  d1.loss_iou: 0.3196  d2.loss_cls: 0.1830  d2.loss_bbox: 0.0369  d2.loss_iou: 0.3166  d3.loss_cls: 0.1783  d3.loss_bbox: 0.0373  d3.loss_iou: 0.3161  d4.loss_cls: 0.1749  d4.loss_bbox: 0.0367  d4.loss_iou: 0.3154  enc_loss_cls: 0.2181  enc_loss_bbox: 0.0423  enc_loss_iou: 0.3515  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0299  dn_loss_iou: 0.2550  d0.dn_loss_cls: 0.0296  d0.dn_loss_bbox: 0.0425  d0.dn_loss_iou: 0.3420  d1.dn_loss_cls: 0.0088  d1.dn_loss_bbox: 0.0321  d1.dn_loss_iou: 0.2714  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0303  d2.dn_loss_iou: 0.2582  d3.dn_loss_cls: 0.0048  d3.dn_loss_bbox: 0.0299  d3.dn_loss_iou: 0.2551  d4.dn_loss_cls: 0.0045  d4.dn_loss_bbox: 0.0299  d4.dn_loss_iou: 0.2549  loss_num: 0.0005  d0.loss_num: 0.0008  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:44:01 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 04:44:01 - mmengine - INFO - Saving checkpoint at 2 epochs
2025/10/29 04:44:16 - mmengine - INFO - Epoch(val) [2][ 50/429]    eta: 0:00:39  time: 0.1031  data_time: 0.0036  memory: 17594  
2025/10/29 04:44:21 - mmengine - INFO - Epoch(val) [2][100/429]    eta: 0:00:33  time: 0.1003  data_time: 0.0025  memory: 4269  
2025/10/29 04:44:26 - mmengine - INFO - Epoch(val) [2][150/429]    eta: 0:00:28  time: 0.1000  data_time: 0.0025  memory: 4269  
2025/10/29 04:44:31 - mmengine - INFO - Epoch(val) [2][200/429]    eta: 0:00:23  time: 0.1004  data_time: 0.0026  memory: 4269  
2025/10/29 04:44:36 - mmengine - INFO - Epoch(val) [2][250/429]    eta: 0:00:18  time: 0.1005  data_time: 0.0025  memory: 4269  
2025/10/29 04:44:41 - mmengine - INFO - Epoch(val) [2][300/429]    eta: 0:00:12  time: 0.1003  data_time: 0.0025  memory: 4269  
2025/10/29 04:44:46 - mmengine - INFO - Epoch(val) [2][350/429]    eta: 0:00:07  time: 0.1007  data_time: 0.0029  memory: 4269  
2025/10/29 04:44:51 - mmengine - INFO - Epoch(val) [2][400/429]    eta: 0:00:02  time: 0.0997  data_time: 0.0025  memory: 4269  
2025/10/29 04:44:55 - mmengine - INFO - {'instance_F1_score': 0.631430584918957, 'instance_acc': 0.4667450980392157, 'image_F1_score': 0.5099188458070334, 'image_acc': 0.36655011655011654}
2025/10/29 04:44:55 - mmengine - INFO - Epoch(val) [2][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.6314  grefcoco_val/refdrone/instance_acc: 0.4667  grefcoco_val/refdrone/image_F1_score: 0.5099  grefcoco_val/refdrone/image_acc: 0.3666  data_time: 0.0027  time: 0.1005
2025/10/29 04:46:42 - mmengine - INFO - Epoch(train) [3][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:35:15  time: 2.1358  data_time: 0.0180  memory: 17591  grad_norm: 39.9094  loss: 5.1778  loss_cls: 0.1433  loss_bbox: 0.0342  loss_iou: 0.2814  d0.loss_cls: 0.1915  d0.loss_bbox: 0.0382  d0.loss_iou: 0.3008  d1.loss_cls: 0.1664  d1.loss_bbox: 0.0350  d1.loss_iou: 0.2867  d2.loss_cls: 0.1521  d2.loss_bbox: 0.0345  d2.loss_iou: 0.2840  d3.loss_cls: 0.1471  d3.loss_bbox: 0.0341  d3.loss_iou: 0.2805  d4.loss_cls: 0.1443  d4.loss_bbox: 0.0344  d4.loss_iou: 0.2813  enc_loss_cls: 0.2033  enc_loss_bbox: 0.0404  enc_loss_iou: 0.3165  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0293  dn_loss_iou: 0.2337  d0.dn_loss_cls: 0.0277  d0.dn_loss_bbox: 0.0409  d0.dn_loss_iou: 0.3119  d1.dn_loss_cls: 0.0092  d1.dn_loss_bbox: 0.0311  d1.dn_loss_iou: 0.2465  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2360  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0294  d3.dn_loss_iou: 0.2343  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0293  d4.dn_loss_iou: 0.2335  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:48:28 - mmengine - INFO - Epoch(train) [3][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:33:28  time: 2.1250  data_time: 0.0164  memory: 17568  grad_norm: 40.6024  loss: 5.6448  loss_cls: 0.1555  loss_bbox: 0.0400  loss_iou: 0.3359  d0.loss_cls: 0.2093  d0.loss_bbox: 0.0439  d0.loss_iou: 0.3570  d1.loss_cls: 0.1725  d1.loss_bbox: 0.0415  d1.loss_iou: 0.3472  d2.loss_cls: 0.1619  d2.loss_bbox: 0.0410  d2.loss_iou: 0.3429  d3.loss_cls: 0.1562  d3.loss_bbox: 0.0402  d3.loss_iou: 0.3385  d4.loss_cls: 0.1553  d4.loss_bbox: 0.0399  d4.loss_iou: 0.3354  enc_loss_cls: 0.2252  enc_loss_bbox: 0.0452  enc_loss_iou: 0.3699  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2267  d0.dn_loss_cls: 0.0251  d0.dn_loss_bbox: 0.0378  d0.dn_loss_iou: 0.3019  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2403  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2299  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2277  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2266  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:50:16 - mmengine - INFO - Epoch(train) [3][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:31:43  time: 2.1476  data_time: 0.0166  memory: 17586  grad_norm: 35.5675  loss: 5.4577  loss_cls: 0.1632  loss_bbox: 0.0377  loss_iou: 0.2993  d0.loss_cls: 0.2009  d0.loss_bbox: 0.0392  d0.loss_iou: 0.3155  d1.loss_cls: 0.1715  d1.loss_bbox: 0.0383  d1.loss_iou: 0.3110  d2.loss_cls: 0.1675  d2.loss_bbox: 0.0371  d2.loss_iou: 0.2978  d3.loss_cls: 0.1651  d3.loss_bbox: 0.0377  d3.loss_iou: 0.2995  d4.loss_cls: 0.1612  d4.loss_bbox: 0.0376  d4.loss_iou: 0.2994  enc_loss_cls: 0.2103  enc_loss_bbox: 0.0432  enc_loss_iou: 0.3352  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0302  dn_loss_iou: 0.2386  d0.dn_loss_cls: 0.0289  d0.dn_loss_bbox: 0.0420  d0.dn_loss_iou: 0.3187  d1.dn_loss_cls: 0.0104  d1.dn_loss_bbox: 0.0322  d1.dn_loss_iou: 0.2528  d2.dn_loss_cls: 0.0064  d2.dn_loss_bbox: 0.0307  d2.dn_loss_iou: 0.2420  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0302  d3.dn_loss_iou: 0.2391  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0302  d4.dn_loss_iou: 0.2384  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:52:02 - mmengine - INFO - Epoch(train) [3][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:29:56  time: 2.1296  data_time: 0.0192  memory: 17581  grad_norm: 38.8362  loss: 5.6264  loss_cls: 0.1652  loss_bbox: 0.0386  loss_iou: 0.3312  d0.loss_cls: 0.2047  d0.loss_bbox: 0.0422  d0.loss_iou: 0.3591  d1.loss_cls: 0.1761  d1.loss_bbox: 0.0406  d1.loss_iou: 0.3474  d2.loss_cls: 0.1724  d2.loss_bbox: 0.0388  d2.loss_iou: 0.3351  d3.loss_cls: 0.1697  d3.loss_bbox: 0.0385  d3.loss_iou: 0.3321  d4.loss_cls: 0.1661  d4.loss_bbox: 0.0386  d4.loss_iou: 0.3321  enc_loss_cls: 0.2223  enc_loss_bbox: 0.0442  enc_loss_iou: 0.3758  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0262  dn_loss_iou: 0.2217  d0.dn_loss_cls: 0.0271  d0.dn_loss_bbox: 0.0375  d0.dn_loss_iou: 0.2967  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0277  d1.dn_loss_iou: 0.2346  d2.dn_loss_cls: 0.0071  d2.dn_loss_bbox: 0.0265  d2.dn_loss_iou: 0.2244  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0262  d3.dn_loss_iou: 0.2219  d4.dn_loss_cls: 0.0057  d4.dn_loss_bbox: 0.0261  d4.dn_loss_iou: 0.2214  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:53:49 - mmengine - INFO - Epoch(train) [3][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:28:09  time: 2.1301  data_time: 0.0177  memory: 17586  grad_norm: 44.6056  loss: 6.1239  loss_cls: 0.1659  loss_bbox: 0.0396  loss_iou: 0.3602  d0.loss_cls: 0.2008  d0.loss_bbox: 0.0456  d0.loss_iou: 0.3848  d1.loss_cls: 0.1743  d1.loss_bbox: 0.0407  d1.loss_iou: 0.3696  d2.loss_cls: 0.1712  d2.loss_bbox: 0.0399  d2.loss_iou: 0.3623  d3.loss_cls: 0.1674  d3.loss_bbox: 0.0395  d3.loss_iou: 0.3592  d4.loss_cls: 0.1668  d4.loss_bbox: 0.0396  d4.loss_iou: 0.3598  enc_loss_cls: 0.2132  enc_loss_bbox: 0.0482  enc_loss_iou: 0.4030  dn_loss_cls: 0.0064  dn_loss_bbox: 0.0294  dn_loss_iou: 0.2664  d0.dn_loss_cls: 0.0324  d0.dn_loss_bbox: 0.0410  d0.dn_loss_iou: 0.3559  d1.dn_loss_cls: 0.0120  d1.dn_loss_bbox: 0.0311  d1.dn_loss_iou: 0.2832  d2.dn_loss_cls: 0.0077  d2.dn_loss_bbox: 0.0297  d2.dn_loss_iou: 0.2699  d3.dn_loss_cls: 0.0067  d3.dn_loss_bbox: 0.0293  d3.dn_loss_iou: 0.2669  d4.dn_loss_cls: 0.0064  d4.dn_loss_bbox: 0.0293  d4.dn_loss_iou: 0.2661  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 04:55:36 - mmengine - INFO - Epoch(train) [3][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:26:23  time: 2.1510  data_time: 0.0173  memory: 17581  grad_norm: 43.3942  loss: 5.1913  loss_cls: 0.1210  loss_bbox: 0.0337  loss_iou: 0.2959  d0.loss_cls: 0.1693  d0.loss_bbox: 0.0350  d0.loss_iou: 0.3062  d1.loss_cls: 0.1431  d1.loss_bbox: 0.0341  d1.loss_iou: 0.2995  d2.loss_cls: 0.1283  d2.loss_bbox: 0.0339  d2.loss_iou: 0.2991  d3.loss_cls: 0.1233  d3.loss_bbox: 0.0335  d3.loss_iou: 0.2945  d4.loss_cls: 0.1221  d4.loss_bbox: 0.0337  d4.loss_iou: 0.2957  enc_loss_cls: 0.1809  enc_loss_bbox: 0.0375  enc_loss_iou: 0.3223  dn_loss_cls: 0.0072  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2482  d0.dn_loss_cls: 0.0313  d0.dn_loss_bbox: 0.0400  d0.dn_loss_iou: 0.3281  d1.dn_loss_cls: 0.0131  d1.dn_loss_bbox: 0.0304  d1.dn_loss_iou: 0.2603  d2.dn_loss_cls: 0.0091  d2.dn_loss_bbox: 0.0293  d2.dn_loss_iou: 0.2505  d3.dn_loss_cls: 0.0077  d3.dn_loss_bbox: 0.0290  d3.dn_loss_iou: 0.2480  d4.dn_loss_cls: 0.0075  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2479  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 04:57:23 - mmengine - INFO - Epoch(train) [3][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:24:36  time: 2.1268  data_time: 0.0175  memory: 17585  grad_norm: 37.3745  loss: 5.0366  loss_cls: 0.1311  loss_bbox: 0.0330  loss_iou: 0.2736  d0.loss_cls: 0.1760  d0.loss_bbox: 0.0364  d0.loss_iou: 0.2925  d1.loss_cls: 0.1465  d1.loss_bbox: 0.0344  d1.loss_iou: 0.2809  d2.loss_cls: 0.1397  d2.loss_bbox: 0.0332  d2.loss_iou: 0.2748  d3.loss_cls: 0.1316  d3.loss_bbox: 0.0333  d3.loss_iou: 0.2734  d4.loss_cls: 0.1301  d4.loss_bbox: 0.0334  d4.loss_iou: 0.2742  enc_loss_cls: 0.1878  enc_loss_bbox: 0.0389  enc_loss_iou: 0.3113  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0285  dn_loss_iou: 0.2386  d0.dn_loss_cls: 0.0299  d0.dn_loss_bbox: 0.0391  d0.dn_loss_iou: 0.3171  d1.dn_loss_cls: 0.0094  d1.dn_loss_bbox: 0.0301  d1.dn_loss_iou: 0.2516  d2.dn_loss_cls: 0.0059  d2.dn_loss_bbox: 0.0288  d2.dn_loss_iou: 0.2412  d3.dn_loss_cls: 0.0048  d3.dn_loss_bbox: 0.0286  d3.dn_loss_iou: 0.2386  d4.dn_loss_cls: 0.0045  d4.dn_loss_bbox: 0.0285  d4.dn_loss_iou: 0.2383  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 04:59:10 - mmengine - INFO - Epoch(train) [3][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:22:51  time: 2.1482  data_time: 0.0164  memory: 17575  grad_norm: 34.4500  loss: 5.2502  loss_cls: 0.1457  loss_bbox: 0.0353  loss_iou: 0.2875  d0.loss_cls: 0.1902  d0.loss_bbox: 0.0380  d0.loss_iou: 0.3057  d1.loss_cls: 0.1600  d1.loss_bbox: 0.0367  d1.loss_iou: 0.2966  d2.loss_cls: 0.1511  d2.loss_bbox: 0.0360  d2.loss_iou: 0.2931  d3.loss_cls: 0.1483  d3.loss_bbox: 0.0359  d3.loss_iou: 0.2897  d4.loss_cls: 0.1447  d4.loss_bbox: 0.0354  d4.loss_iou: 0.2876  enc_loss_cls: 0.1983  enc_loss_bbox: 0.0416  enc_loss_iou: 0.3293  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0285  dn_loss_iou: 0.2380  d0.dn_loss_cls: 0.0268  d0.dn_loss_bbox: 0.0390  d0.dn_loss_iou: 0.3148  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0300  d1.dn_loss_iou: 0.2511  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0288  d2.dn_loss_iou: 0.2411  d3.dn_loss_cls: 0.0051  d3.dn_loss_bbox: 0.0285  d3.dn_loss_iou: 0.2386  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0284  d4.dn_loss_iou: 0.2377  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0006  d3.loss_num: 0.0005  d4.loss_num: 0.0006
2025/10/29 05:00:57 - mmengine - INFO - Epoch(train) [3][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:21:04  time: 2.1315  data_time: 0.0166  memory: 17585  grad_norm: 35.9952  loss: 5.0324  loss_cls: 0.1260  loss_bbox: 0.0314  loss_iou: 0.2795  d0.loss_cls: 0.1695  d0.loss_bbox: 0.0327  d0.loss_iou: 0.2864  d1.loss_cls: 0.1457  d1.loss_bbox: 0.0318  d1.loss_iou: 0.2810  d2.loss_cls: 0.1366  d2.loss_bbox: 0.0316  d2.loss_iou: 0.2815  d3.loss_cls: 0.1318  d3.loss_bbox: 0.0314  d3.loss_iou: 0.2793  d4.loss_cls: 0.1280  d4.loss_bbox: 0.0313  d4.loss_iou: 0.2789  enc_loss_cls: 0.1792  enc_loss_bbox: 0.0359  enc_loss_iou: 0.3019  dn_loss_cls: 0.0070  dn_loss_bbox: 0.0278  dn_loss_iou: 0.2426  d0.dn_loss_cls: 0.0280  d0.dn_loss_bbox: 0.0385  d0.dn_loss_iou: 0.3227  d1.dn_loss_cls: 0.0102  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2547  d2.dn_loss_cls: 0.0079  d2.dn_loss_bbox: 0.0281  d2.dn_loss_iou: 0.2460  d3.dn_loss_cls: 0.0073  d3.dn_loss_bbox: 0.0278  d3.dn_loss_iou: 0.2431  d4.dn_loss_cls: 0.0071  d4.dn_loss_bbox: 0.0277  d4.dn_loss_iou: 0.2425  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0004  d3.loss_num: 0.0005  d4.loss_num: 0.0004
2025/10/29 05:02:43 - mmengine - INFO - Epoch(train) [3][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:19:17  time: 2.1300  data_time: 0.0175  memory: 17566  grad_norm: 41.7823  loss: 4.9760  loss_cls: 0.1366  loss_bbox: 0.0324  loss_iou: 0.2689  d0.loss_cls: 0.1697  d0.loss_bbox: 0.0349  d0.loss_iou: 0.2795  d1.loss_cls: 0.1508  d1.loss_bbox: 0.0329  d1.loss_iou: 0.2715  d2.loss_cls: 0.1407  d2.loss_bbox: 0.0324  d2.loss_iou: 0.2687  d3.loss_cls: 0.1376  d3.loss_bbox: 0.0325  d3.loss_iou: 0.2700  d4.loss_cls: 0.1362  d4.loss_bbox: 0.0329  d4.loss_iou: 0.2698  enc_loss_cls: 0.1829  enc_loss_bbox: 0.0386  enc_loss_iou: 0.3034  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0281  dn_loss_iou: 0.2370  d0.dn_loss_cls: 0.0275  d0.dn_loss_bbox: 0.0392  d0.dn_loss_iou: 0.3134  d1.dn_loss_cls: 0.0087  d1.dn_loss_bbox: 0.0295  d1.dn_loss_iou: 0.2492  d2.dn_loss_cls: 0.0060  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2387  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.0280  d3.dn_loss_iou: 0.2365  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0280  d4.dn_loss_iou: 0.2367  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:04:31 - mmengine - INFO - Epoch(train) [3][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:17:32  time: 2.1564  data_time: 0.0169  memory: 17579  grad_norm: 48.6586  loss: 5.0946  loss_cls: 0.1395  loss_bbox: 0.0347  loss_iou: 0.2907  d0.loss_cls: 0.1991  d0.loss_bbox: 0.0349  d0.loss_iou: 0.2961  d1.loss_cls: 0.1652  d1.loss_bbox: 0.0350  d1.loss_iou: 0.2914  d2.loss_cls: 0.1534  d2.loss_bbox: 0.0338  d2.loss_iou: 0.2885  d3.loss_cls: 0.1460  d3.loss_bbox: 0.0341  d3.loss_iou: 0.2899  d4.loss_cls: 0.1406  d4.loss_bbox: 0.0349  d4.loss_iou: 0.2920  enc_loss_cls: 0.2027  enc_loss_bbox: 0.0381  enc_loss_iou: 0.3141  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0256  dn_loss_iou: 0.2195  d0.dn_loss_cls: 0.0277  d0.dn_loss_bbox: 0.0365  d0.dn_loss_iou: 0.2974  d1.dn_loss_cls: 0.0093  d1.dn_loss_bbox: 0.0273  d1.dn_loss_iou: 0.2317  d2.dn_loss_cls: 0.0070  d2.dn_loss_bbox: 0.0260  d2.dn_loss_iou: 0.2219  d3.dn_loss_cls: 0.0059  d3.dn_loss_bbox: 0.0256  d3.dn_loss_iou: 0.2191  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0256  d4.dn_loss_iou: 0.2189  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:06:17 - mmengine - INFO - Epoch(train) [3][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:15:44  time: 2.1124  data_time: 0.0169  memory: 17574  grad_norm: 40.8003  loss: 5.2345  loss_cls: 0.1246  loss_bbox: 0.0341  loss_iou: 0.3179  d0.loss_cls: 0.1641  d0.loss_bbox: 0.0355  d0.loss_iou: 0.3249  d1.loss_cls: 0.1399  d1.loss_bbox: 0.0344  d1.loss_iou: 0.3192  d2.loss_cls: 0.1338  d2.loss_bbox: 0.0343  d2.loss_iou: 0.3189  d3.loss_cls: 0.1267  d3.loss_bbox: 0.0341  d3.loss_iou: 0.3191  d4.loss_cls: 0.1247  d4.loss_bbox: 0.0342  d4.loss_iou: 0.3181  enc_loss_cls: 0.1758  enc_loss_bbox: 0.0392  enc_loss_iou: 0.3521  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0261  dn_loss_iou: 0.2360  d0.dn_loss_cls: 0.0263  d0.dn_loss_bbox: 0.0361  d0.dn_loss_iou: 0.3090  d1.dn_loss_cls: 0.0084  d1.dn_loss_bbox: 0.0274  d1.dn_loss_iou: 0.2481  d2.dn_loss_cls: 0.0058  d2.dn_loss_bbox: 0.0264  d2.dn_loss_iou: 0.2388  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0261  d3.dn_loss_iou: 0.2363  d4.dn_loss_cls: 0.0045  d4.dn_loss_bbox: 0.0261  d4.dn_loss_iou: 0.2358  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:08:03 - mmengine - INFO - Epoch(train) [3][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:13:57  time: 2.1283  data_time: 0.0180  memory: 17568  grad_norm: 39.9660  loss: 5.2456  loss_cls: 0.1376  loss_bbox: 0.0326  loss_iou: 0.2859  d0.loss_cls: 0.1755  d0.loss_bbox: 0.0337  d0.loss_iou: 0.2941  d1.loss_cls: 0.1478  d1.loss_bbox: 0.0326  d1.loss_iou: 0.2867  d2.loss_cls: 0.1460  d2.loss_bbox: 0.0326  d2.loss_iou: 0.2864  d3.loss_cls: 0.1430  d3.loss_bbox: 0.0322  d3.loss_iou: 0.2831  d4.loss_cls: 0.1379  d4.loss_bbox: 0.0324  d4.loss_iou: 0.2849  enc_loss_cls: 0.1872  enc_loss_bbox: 0.0378  enc_loss_iou: 0.3168  dn_loss_cls: 0.0069  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2551  d0.dn_loss_cls: 0.0292  d0.dn_loss_bbox: 0.0425  d0.dn_loss_iou: 0.3453  d1.dn_loss_cls: 0.0093  d1.dn_loss_bbox: 0.0310  d1.dn_loss_iou: 0.2690  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2581  d3.dn_loss_cls: 0.0072  d3.dn_loss_bbox: 0.0293  d3.dn_loss_iou: 0.2554  d4.dn_loss_cls: 0.0068  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2549  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:09:51 - mmengine - INFO - Epoch(train) [3][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:12:11  time: 2.1521  data_time: 0.0171  memory: 17568  grad_norm: 49.3046  loss: 5.4130  loss_cls: 0.1417  loss_bbox: 0.0371  loss_iou: 0.3190  d0.loss_cls: 0.1833  d0.loss_bbox: 0.0402  d0.loss_iou: 0.3337  d1.loss_cls: 0.1619  d1.loss_bbox: 0.0370  d1.loss_iou: 0.3184  d2.loss_cls: 0.1522  d2.loss_bbox: 0.0369  d2.loss_iou: 0.3228  d3.loss_cls: 0.1505  d3.loss_bbox: 0.0357  d3.loss_iou: 0.3159  d4.loss_cls: 0.1435  d4.loss_bbox: 0.0371  d4.loss_iou: 0.3187  enc_loss_cls: 0.1916  enc_loss_bbox: 0.0437  enc_loss_iou: 0.3572  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2358  d0.dn_loss_cls: 0.0257  d0.dn_loss_bbox: 0.0374  d0.dn_loss_iou: 0.3136  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0288  d1.dn_loss_iou: 0.2474  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0276  d2.dn_loss_iou: 0.2380  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0275  d3.dn_loss_iou: 0.2358  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2355  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:11:37 - mmengine - INFO - Epoch(train) [3][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:10:24  time: 2.1289  data_time: 0.0176  memory: 17568  grad_norm: 31.5185  loss: 5.2331  loss_cls: 0.1486  loss_bbox: 0.0360  loss_iou: 0.3038  d0.loss_cls: 0.1907  d0.loss_bbox: 0.0386  d0.loss_iou: 0.3197  d1.loss_cls: 0.1651  d1.loss_bbox: 0.0363  d1.loss_iou: 0.3053  d2.loss_cls: 0.1527  d2.loss_bbox: 0.0360  d2.loss_iou: 0.3021  d3.loss_cls: 0.1500  d3.loss_bbox: 0.0359  d3.loss_iou: 0.3025  d4.loss_cls: 0.1482  d4.loss_bbox: 0.0358  d4.loss_iou: 0.3020  enc_loss_cls: 0.2039  enc_loss_bbox: 0.0432  enc_loss_iou: 0.3426  dn_loss_cls: 0.0103  dn_loss_bbox: 0.0257  dn_loss_iou: 0.2165  d0.dn_loss_cls: 0.0300  d0.dn_loss_bbox: 0.0351  d0.dn_loss_iou: 0.2842  d1.dn_loss_cls: 0.0136  d1.dn_loss_bbox: 0.0272  d1.dn_loss_iou: 0.2282  d2.dn_loss_cls: 0.0102  d2.dn_loss_bbox: 0.0261  d2.dn_loss_iou: 0.2195  d3.dn_loss_cls: 0.0095  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2171  d4.dn_loss_cls: 0.0103  d4.dn_loss_bbox: 0.0257  d4.dn_loss_iou: 0.2165  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:13:24 - mmengine - INFO - Epoch(train) [3][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:08:38  time: 2.1390  data_time: 0.0169  memory: 17585  grad_norm: 35.9234  loss: 5.4096  loss_cls: 0.1304  loss_bbox: 0.0347  loss_iou: 0.2951  d0.loss_cls: 0.1862  d0.loss_bbox: 0.0365  d0.loss_iou: 0.3069  d1.loss_cls: 0.1508  d1.loss_bbox: 0.0365  d1.loss_iou: 0.3041  d2.loss_cls: 0.1426  d2.loss_bbox: 0.0356  d2.loss_iou: 0.2992  d3.loss_cls: 0.1370  d3.loss_bbox: 0.0347  d3.loss_iou: 0.2947  d4.loss_cls: 0.1346  d4.loss_bbox: 0.0345  d4.loss_iou: 0.2931  enc_loss_cls: 0.1911  enc_loss_bbox: 0.0425  enc_loss_iou: 0.3398  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0313  dn_loss_iou: 0.2638  d0.dn_loss_cls: 0.0298  d0.dn_loss_bbox: 0.0433  d0.dn_loss_iou: 0.3506  d1.dn_loss_cls: 0.0083  d1.dn_loss_bbox: 0.0331  d1.dn_loss_iou: 0.2782  d2.dn_loss_cls: 0.0055  d2.dn_loss_bbox: 0.0318  d2.dn_loss_iou: 0.2672  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0314  d3.dn_loss_iou: 0.2640  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0312  d4.dn_loss_iou: 0.2635  loss_num: 0.0004  d0.loss_num: 0.0008  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:15:11 - mmengine - INFO - Epoch(train) [3][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:06:51  time: 2.1342  data_time: 0.0176  memory: 17581  grad_norm: 35.1718  loss: 5.2698  loss_cls: 0.1300  loss_bbox: 0.0359  loss_iou: 0.3080  d0.loss_cls: 0.1730  d0.loss_bbox: 0.0390  d0.loss_iou: 0.3254  d1.loss_cls: 0.1477  d1.loss_bbox: 0.0372  d1.loss_iou: 0.3168  d2.loss_cls: 0.1341  d2.loss_bbox: 0.0362  d2.loss_iou: 0.3106  d3.loss_cls: 0.1319  d3.loss_bbox: 0.0360  d3.loss_iou: 0.3084  d4.loss_cls: 0.1314  d4.loss_bbox: 0.0359  d4.loss_iou: 0.3075  enc_loss_cls: 0.1950  enc_loss_bbox: 0.0423  enc_loss_iou: 0.3495  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0279  dn_loss_iou: 0.2331  d0.dn_loss_cls: 0.0286  d0.dn_loss_bbox: 0.0391  d0.dn_loss_iou: 0.3092  d1.dn_loss_cls: 0.0102  d1.dn_loss_bbox: 0.0298  d1.dn_loss_iou: 0.2471  d2.dn_loss_cls: 0.0069  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2360  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0280  d3.dn_loss_iou: 0.2334  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0279  d4.dn_loss_iou: 0.2328  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:16:57 - mmengine - INFO - Epoch(train) [3][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:05:04  time: 2.1193  data_time: 0.0169  memory: 17584  grad_norm: 32.7172  loss: 5.1117  loss_cls: 0.1186  loss_bbox: 0.0327  loss_iou: 0.2920  d0.loss_cls: 0.1655  d0.loss_bbox: 0.0346  d0.loss_iou: 0.3069  d1.loss_cls: 0.1406  d1.loss_bbox: 0.0333  d1.loss_iou: 0.2963  d2.loss_cls: 0.1260  d2.loss_bbox: 0.0337  d2.loss_iou: 0.2970  d3.loss_cls: 0.1203  d3.loss_bbox: 0.0329  d3.loss_iou: 0.2925  d4.loss_cls: 0.1195  d4.loss_bbox: 0.0327  d4.loss_iou: 0.2915  enc_loss_cls: 0.1846  enc_loss_bbox: 0.0380  enc_loss_iou: 0.3245  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2445  d0.dn_loss_cls: 0.0286  d0.dn_loss_bbox: 0.0380  d0.dn_loss_iou: 0.3185  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0289  d1.dn_loss_iou: 0.2568  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2471  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2449  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2442  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:18:01 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 05:18:44 - mmengine - INFO - Epoch(train) [3][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:03:18  time: 2.1508  data_time: 0.0172  memory: 17581  grad_norm: 40.3875  loss: 5.1955  loss_cls: 0.1148  loss_bbox: 0.0361  loss_iou: 0.3120  d0.loss_cls: 0.1561  d0.loss_bbox: 0.0380  d0.loss_iou: 0.3225  d1.loss_cls: 0.1285  d1.loss_bbox: 0.0364  d1.loss_iou: 0.3154  d2.loss_cls: 0.1208  d2.loss_bbox: 0.0366  d2.loss_iou: 0.3153  d3.loss_cls: 0.1176  d3.loss_bbox: 0.0361  d3.loss_iou: 0.3115  d4.loss_cls: 0.1150  d4.loss_bbox: 0.0360  d4.loss_iou: 0.3105  enc_loss_cls: 0.1767  enc_loss_bbox: 0.0416  enc_loss_iou: 0.3429  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0292  dn_loss_iou: 0.2389  d0.dn_loss_cls: 0.0284  d0.dn_loss_bbox: 0.0405  d0.dn_loss_iou: 0.3145  d1.dn_loss_cls: 0.0089  d1.dn_loss_bbox: 0.0311  d1.dn_loss_iou: 0.2517  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0296  d2.dn_loss_iou: 0.2419  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.0293  d3.dn_loss_iou: 0.2396  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0292  d4.dn_loss_iou: 0.2386  loss_num: 0.0003  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0003
2025/10/29 05:20:31 - mmengine - INFO - Epoch(train) [3][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 3:01:31  time: 2.1327  data_time: 0.0171  memory: 17568  grad_norm: 38.3129  loss: 5.0815  loss_cls: 0.1231  loss_bbox: 0.0341  loss_iou: 0.2887  d0.loss_cls: 0.1627  d0.loss_bbox: 0.0366  d0.loss_iou: 0.3059  d1.loss_cls: 0.1364  d1.loss_bbox: 0.0352  d1.loss_iou: 0.2971  d2.loss_cls: 0.1293  d2.loss_bbox: 0.0344  d2.loss_iou: 0.2919  d3.loss_cls: 0.1256  d3.loss_bbox: 0.0342  d3.loss_iou: 0.2891  d4.loss_cls: 0.1220  d4.loss_bbox: 0.0344  d4.loss_iou: 0.2900  enc_loss_cls: 0.1817  enc_loss_bbox: 0.0395  enc_loss_iou: 0.3271  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0287  dn_loss_iou: 0.2376  d0.dn_loss_cls: 0.0276  d0.dn_loss_bbox: 0.0398  d0.dn_loss_iou: 0.3122  d1.dn_loss_cls: 0.0088  d1.dn_loss_bbox: 0.0303  d1.dn_loss_iou: 0.2495  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0290  d2.dn_loss_iou: 0.2400  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0287  d3.dn_loss_iou: 0.2374  d4.dn_loss_cls: 0.0057  d4.dn_loss_bbox: 0.0286  d4.dn_loss_iou: 0.2373  loss_num: 0.0004  d0.loss_num: 0.0007  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0005  d4.loss_num: 0.0004
2025/10/29 05:22:17 - mmengine - INFO - Epoch(train) [3][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:59:44  time: 2.1229  data_time: 0.0171  memory: 17575  grad_norm: 36.4816  loss: 4.9154  loss_cls: 0.1222  loss_bbox: 0.0309  loss_iou: 0.2582  d0.loss_cls: 0.1685  d0.loss_bbox: 0.0325  d0.loss_iou: 0.2720  d1.loss_cls: 0.1413  d1.loss_bbox: 0.0310  d1.loss_iou: 0.2605  d2.loss_cls: 0.1346  d2.loss_bbox: 0.0309  d2.loss_iou: 0.2596  d3.loss_cls: 0.1268  d3.loss_bbox: 0.0308  d3.loss_iou: 0.2576  d4.loss_cls: 0.1250  d4.loss_bbox: 0.0308  d4.loss_iou: 0.2575  enc_loss_cls: 0.1807  enc_loss_bbox: 0.0382  enc_loss_iou: 0.2938  dn_loss_cls: 0.0062  dn_loss_bbox: 0.0293  dn_loss_iou: 0.2462  d0.dn_loss_cls: 0.0286  d0.dn_loss_bbox: 0.0410  d0.dn_loss_iou: 0.3270  d1.dn_loss_cls: 0.0105  d1.dn_loss_bbox: 0.0309  d1.dn_loss_iou: 0.2585  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0297  d2.dn_loss_iou: 0.2490  d3.dn_loss_cls: 0.0070  d3.dn_loss_bbox: 0.0294  d3.dn_loss_iou: 0.2466  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0293  d4.dn_loss_iou: 0.2460  loss_num: 0.0005  d0.loss_num: 0.0006  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:24:04 - mmengine - INFO - Epoch(train) [3][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:57:58  time: 2.1465  data_time: 0.0176  memory: 17579  grad_norm: 38.5952  loss: 5.8123  loss_cls: 0.1462  loss_bbox: 0.0420  loss_iou: 0.3510  d0.loss_cls: 0.1926  d0.loss_bbox: 0.0444  d0.loss_iou: 0.3588  d1.loss_cls: 0.1632  d1.loss_bbox: 0.0425  d1.loss_iou: 0.3542  d2.loss_cls: 0.1554  d2.loss_bbox: 0.0419  d2.loss_iou: 0.3513  d3.loss_cls: 0.1485  d3.loss_bbox: 0.0419  d3.loss_iou: 0.3509  d4.loss_cls: 0.1493  d4.loss_bbox: 0.0419  d4.loss_iou: 0.3494  enc_loss_cls: 0.2043  enc_loss_bbox: 0.0453  enc_loss_iou: 0.3839  dn_loss_cls: 0.0110  dn_loss_bbox: 0.0263  dn_loss_iou: 0.2485  d0.dn_loss_cls: 0.0332  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.3315  d1.dn_loss_cls: 0.0129  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2621  d2.dn_loss_cls: 0.0109  d2.dn_loss_bbox: 0.0266  d2.dn_loss_iou: 0.2510  d3.dn_loss_cls: 0.0104  d3.dn_loss_bbox: 0.0263  d3.dn_loss_iou: 0.2485  d4.dn_loss_cls: 0.0113  d4.dn_loss_bbox: 0.0263  d4.dn_loss_iou: 0.2483  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:25:51 - mmengine - INFO - Epoch(train) [3][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:56:11  time: 2.1264  data_time: 0.0164  memory: 17581  grad_norm: 36.3373  loss: 4.7913  loss_cls: 0.1270  loss_bbox: 0.0320  loss_iou: 0.2635  d0.loss_cls: 0.1702  d0.loss_bbox: 0.0346  d0.loss_iou: 0.2752  d1.loss_cls: 0.1405  d1.loss_bbox: 0.0333  d1.loss_iou: 0.2678  d2.loss_cls: 0.1343  d2.loss_bbox: 0.0321  d2.loss_iou: 0.2642  d3.loss_cls: 0.1312  d3.loss_bbox: 0.0319  d3.loss_iou: 0.2632  d4.loss_cls: 0.1295  d4.loss_bbox: 0.0317  d4.loss_iou: 0.2611  enc_loss_cls: 0.1852  enc_loss_bbox: 0.0380  enc_loss_iou: 0.2919  dn_loss_cls: 0.0061  dn_loss_bbox: 0.0268  dn_loss_iou: 0.2233  d0.dn_loss_cls: 0.0252  d0.dn_loss_bbox: 0.0365  d0.dn_loss_iou: 0.2874  d1.dn_loss_cls: 0.0090  d1.dn_loss_bbox: 0.0284  d1.dn_loss_iou: 0.2345  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0271  d2.dn_loss_iou: 0.2260  d3.dn_loss_cls: 0.0061  d3.dn_loss_bbox: 0.0269  d3.dn_loss_iou: 0.2237  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0268  d4.dn_loss_iou: 0.2232  loss_num: 0.0005  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0005  d4.loss_num: 0.0005
2025/10/29 05:27:37 - mmengine - INFO - Epoch(train) [3][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:54:24  time: 2.1347  data_time: 0.0170  memory: 17581  grad_norm: 37.3918  loss: 5.3895  loss_cls: 0.1415  loss_bbox: 0.0373  loss_iou: 0.3274  d0.loss_cls: 0.1868  d0.loss_bbox: 0.0391  d0.loss_iou: 0.3383  d1.loss_cls: 0.1536  d1.loss_bbox: 0.0382  d1.loss_iou: 0.3323  d2.loss_cls: 0.1481  d2.loss_bbox: 0.0375  d2.loss_iou: 0.3293  d3.loss_cls: 0.1421  d3.loss_bbox: 0.0373  d3.loss_iou: 0.3275  d4.loss_cls: 0.1425  d4.loss_bbox: 0.0372  d4.loss_iou: 0.3258  enc_loss_cls: 0.1984  enc_loss_bbox: 0.0420  enc_loss_iou: 0.3578  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0267  dn_loss_iou: 0.2262  d0.dn_loss_cls: 0.0244  d0.dn_loss_bbox: 0.0366  d0.dn_loss_iou: 0.2976  d1.dn_loss_cls: 0.0086  d1.dn_loss_bbox: 0.0282  d1.dn_loss_iou: 0.2371  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0270  d2.dn_loss_iou: 0.2283  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.0268  d3.dn_loss_iou: 0.2261  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0267  d4.dn_loss_iou: 0.2259  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:29:24 - mmengine - INFO - Epoch(train) [3][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:52:38  time: 2.1361  data_time: 0.0181  memory: 17585  grad_norm: 42.0298  loss: 5.3178  loss_cls: 0.1389  loss_bbox: 0.0340  loss_iou: 0.3264  d0.loss_cls: 0.1879  d0.loss_bbox: 0.0374  d0.loss_iou: 0.3385  d1.loss_cls: 0.1568  d1.loss_bbox: 0.0350  d1.loss_iou: 0.3335  d2.loss_cls: 0.1475  d2.loss_bbox: 0.0340  d2.loss_iou: 0.3270  d3.loss_cls: 0.1404  d3.loss_bbox: 0.0338  d3.loss_iou: 0.3271  d4.loss_cls: 0.1394  d4.loss_bbox: 0.0340  d4.loss_iou: 0.3256  enc_loss_cls: 0.2031  enc_loss_bbox: 0.0400  enc_loss_iou: 0.3592  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0245  dn_loss_iou: 0.2198  d0.dn_loss_cls: 0.0223  d0.dn_loss_bbox: 0.0342  d0.dn_loss_iou: 0.2948  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0258  d1.dn_loss_iou: 0.2307  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0249  d2.dn_loss_iou: 0.2219  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.0246  d3.dn_loss_iou: 0.2198  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0245  d4.dn_loss_iou: 0.2194  loss_num: 0.0004  d0.loss_num: 0.0007  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:31:11 - mmengine - INFO - Epoch(train) [3][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:50:51  time: 2.1294  data_time: 0.0170  memory: 17568  grad_norm: 46.0057  loss: 5.1187  loss_cls: 0.1290  loss_bbox: 0.0337  loss_iou: 0.3017  d0.loss_cls: 0.1734  d0.loss_bbox: 0.0356  d0.loss_iou: 0.3150  d1.loss_cls: 0.1402  d1.loss_bbox: 0.0351  d1.loss_iou: 0.3123  d2.loss_cls: 0.1383  d2.loss_bbox: 0.0343  d2.loss_iou: 0.3061  d3.loss_cls: 0.1316  d3.loss_bbox: 0.0342  d3.loss_iou: 0.3032  d4.loss_cls: 0.1290  d4.loss_bbox: 0.0337  d4.loss_iou: 0.3013  enc_loss_cls: 0.1790  enc_loss_bbox: 0.0392  enc_loss_iou: 0.3380  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0257  dn_loss_iou: 0.2284  d0.dn_loss_cls: 0.0247  d0.dn_loss_bbox: 0.0352  d0.dn_loss_iou: 0.3013  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.0271  d1.dn_loss_iou: 0.2403  d2.dn_loss_cls: 0.0049  d2.dn_loss_bbox: 0.0260  d2.dn_loss_iou: 0.2304  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0257  d3.dn_loss_iou: 0.2282  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0257  d4.dn_loss_iou: 0.2280  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 05:32:58 - mmengine - INFO - Epoch(train) [3][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:49:05  time: 2.1412  data_time: 0.0181  memory: 17591  grad_norm: 37.2522  loss: 4.7960  loss_cls: 0.1118  loss_bbox: 0.0328  loss_iou: 0.2790  d0.loss_cls: 0.1564  d0.loss_bbox: 0.0338  d0.loss_iou: 0.2914  d1.loss_cls: 0.1288  d1.loss_bbox: 0.0331  d1.loss_iou: 0.2850  d2.loss_cls: 0.1218  d2.loss_bbox: 0.0321  d2.loss_iou: 0.2781  d3.loss_cls: 0.1163  d3.loss_bbox: 0.0328  d3.loss_iou: 0.2791  d4.loss_cls: 0.1107  d4.loss_bbox: 0.0325  d4.loss_iou: 0.2789  enc_loss_cls: 0.1717  enc_loss_bbox: 0.0378  enc_loss_iou: 0.3079  dn_loss_cls: 0.0035  dn_loss_bbox: 0.0264  dn_loss_iou: 0.2211  d0.dn_loss_cls: 0.0248  d0.dn_loss_bbox: 0.0378  d0.dn_loss_iou: 0.3014  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0279  d1.dn_loss_iou: 0.2340  d2.dn_loss_cls: 0.0045  d2.dn_loss_bbox: 0.0268  d2.dn_loss_iou: 0.2238  d3.dn_loss_cls: 0.0037  d3.dn_loss_bbox: 0.0265  d3.dn_loss_iou: 0.2214  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0264  d4.dn_loss_iou: 0.2209  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 05:34:44 - mmengine - INFO - Epoch(train) [3][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:47:18  time: 2.1301  data_time: 0.0181  memory: 17585  grad_norm: 36.1788  loss: 4.6367  loss_cls: 0.1033  loss_bbox: 0.0306  loss_iou: 0.2837  d0.loss_cls: 0.1457  d0.loss_bbox: 0.0317  d0.loss_iou: 0.2860  d1.loss_cls: 0.1174  d1.loss_bbox: 0.0313  d1.loss_iou: 0.2848  d2.loss_cls: 0.1082  d2.loss_bbox: 0.0308  d2.loss_iou: 0.2821  d3.loss_cls: 0.1049  d3.loss_bbox: 0.0310  d3.loss_iou: 0.2850  d4.loss_cls: 0.1037  d4.loss_bbox: 0.0307  d4.loss_iou: 0.2844  enc_loss_cls: 0.1640  enc_loss_bbox: 0.0356  enc_loss_iou: 0.3101  dn_loss_cls: 0.0046  dn_loss_bbox: 0.0240  dn_loss_iou: 0.2122  d0.dn_loss_cls: 0.0217  d0.dn_loss_bbox: 0.0325  d0.dn_loss_iou: 0.2734  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0254  d1.dn_loss_iou: 0.2229  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0243  d2.dn_loss_iou: 0.2144  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0241  d3.dn_loss_iou: 0.2123  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0240  d4.dn_loss_iou: 0.2121  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 05:36:31 - mmengine - INFO - Epoch(train) [3][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:45:31  time: 2.1276  data_time: 0.0175  memory: 17575  grad_norm: 37.6253  loss: 5.0930  loss_cls: 0.1194  loss_bbox: 0.0343  loss_iou: 0.2940  d0.loss_cls: 0.1621  d0.loss_bbox: 0.0364  d0.loss_iou: 0.3102  d1.loss_cls: 0.1439  d1.loss_bbox: 0.0351  d1.loss_iou: 0.3010  d2.loss_cls: 0.1308  d2.loss_bbox: 0.0345  d2.loss_iou: 0.2957  d3.loss_cls: 0.1239  d3.loss_bbox: 0.0362  d3.loss_iou: 0.2958  d4.loss_cls: 0.1191  d4.loss_bbox: 0.0356  d4.loss_iou: 0.2941  enc_loss_cls: 0.1731  enc_loss_bbox: 0.0402  enc_loss_iou: 0.3332  dn_loss_cls: 0.0071  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2333  d0.dn_loss_cls: 0.0301  d0.dn_loss_bbox: 0.0382  d0.dn_loss_iou: 0.3103  d1.dn_loss_cls: 0.0115  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2469  d2.dn_loss_cls: 0.0085  d2.dn_loss_bbox: 0.0278  d2.dn_loss_iou: 0.2363  d3.dn_loss_cls: 0.0074  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2335  d4.dn_loss_cls: 0.0072  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2332  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 05:38:18 - mmengine - INFO - Epoch(train) [3][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:43:45  time: 2.1436  data_time: 0.0170  memory: 17575  grad_norm: 41.8092  loss: 5.5735  loss_cls: 0.1206  loss_bbox: 0.0402  loss_iou: 0.3482  d0.loss_cls: 0.1741  d0.loss_bbox: 0.0402  d0.loss_iou: 0.3489  d1.loss_cls: 0.1372  d1.loss_bbox: 0.0403  d1.loss_iou: 0.3491  d2.loss_cls: 0.1279  d2.loss_bbox: 0.0400  d2.loss_iou: 0.3459  d3.loss_cls: 0.1246  d3.loss_bbox: 0.0399  d3.loss_iou: 0.3446  d4.loss_cls: 0.1196  d4.loss_bbox: 0.0401  d4.loss_iou: 0.3467  enc_loss_cls: 0.1826  enc_loss_bbox: 0.0438  enc_loss_iou: 0.3714  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0291  dn_loss_iou: 0.2499  d0.dn_loss_cls: 0.0295  d0.dn_loss_bbox: 0.0405  d0.dn_loss_iou: 0.3307  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0309  d1.dn_loss_iou: 0.2643  d2.dn_loss_cls: 0.0060  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2528  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0291  d3.dn_loss_iou: 0.2499  d4.dn_loss_cls: 0.0047  d4.dn_loss_bbox: 0.0291  d4.dn_loss_iou: 0.2497  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:40:04 - mmengine - INFO - Epoch(train) [3][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:41:57  time: 2.1219  data_time: 0.0171  memory: 17574  grad_norm: 39.4630  loss: 4.8824  loss_cls: 0.1152  loss_bbox: 0.0357  loss_iou: 0.2908  d0.loss_cls: 0.1801  d0.loss_bbox: 0.0369  d0.loss_iou: 0.3015  d1.loss_cls: 0.1328  d1.loss_bbox: 0.0347  d1.loss_iou: 0.2934  d2.loss_cls: 0.1172  d2.loss_bbox: 0.0345  d2.loss_iou: 0.2892  d3.loss_cls: 0.1143  d3.loss_bbox: 0.0341  d3.loss_iou: 0.2880  d4.loss_cls: 0.1152  d4.loss_bbox: 0.0349  d4.loss_iou: 0.2888  enc_loss_cls: 0.1706  enc_loss_bbox: 0.0387  enc_loss_iou: 0.3166  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0261  dn_loss_iou: 0.2189  d0.dn_loss_cls: 0.0215  d0.dn_loss_bbox: 0.0359  d0.dn_loss_iou: 0.2896  d1.dn_loss_cls: 0.0074  d1.dn_loss_bbox: 0.0275  d1.dn_loss_iou: 0.2307  d2.dn_loss_cls: 0.0057  d2.dn_loss_bbox: 0.0264  d2.dn_loss_iou: 0.2216  d3.dn_loss_cls: 0.0050  d3.dn_loss_bbox: 0.0262  d3.dn_loss_iou: 0.2194  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0261  d4.dn_loss_iou: 0.2187  loss_num: 0.0004  d0.loss_num: 0.0007  d1.loss_num: 0.0005  d2.loss_num: 0.0005  d3.loss_num: 0.0004  d4.loss_num: 0.0005
2025/10/29 05:41:50 - mmengine - INFO - Epoch(train) [3][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:40:11  time: 2.1279  data_time: 0.0176  memory: 17591  grad_norm: 40.4675  loss: 5.3995  loss_cls: 0.1365  loss_bbox: 0.0364  loss_iou: 0.3108  d0.loss_cls: 0.1835  d0.loss_bbox: 0.0387  d0.loss_iou: 0.3244  d1.loss_cls: 0.1468  d1.loss_bbox: 0.0380  d1.loss_iou: 0.3209  d2.loss_cls: 0.1412  d2.loss_bbox: 0.0365  d2.loss_iou: 0.3141  d3.loss_cls: 0.1384  d3.loss_bbox: 0.0367  d3.loss_iou: 0.3133  d4.loss_cls: 0.1354  d4.loss_bbox: 0.0364  d4.loss_iou: 0.3107  enc_loss_cls: 0.1907  enc_loss_bbox: 0.0440  enc_loss_iou: 0.3398  dn_loss_cls: 0.0053  dn_loss_bbox: 0.0277  dn_loss_iou: 0.2470  d0.dn_loss_cls: 0.0281  d0.dn_loss_bbox: 0.0396  d0.dn_loss_iou: 0.3321  d1.dn_loss_cls: 0.0092  d1.dn_loss_bbox: 0.0294  d1.dn_loss_iou: 0.2598  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2505  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0278  d3.dn_loss_iou: 0.2475  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0277  d4.dn_loss_iou: 0.2466  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 05:43:38 - mmengine - INFO - Epoch(train) [3][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:38:25  time: 2.1523  data_time: 0.0174  memory: 17563  grad_norm: 45.0858  loss: 5.0738  loss_cls: 0.1158  loss_bbox: 0.0358  loss_iou: 0.2904  d0.loss_cls: 0.1831  d0.loss_bbox: 0.0379  d0.loss_iou: 0.3035  d1.loss_cls: 0.1303  d1.loss_bbox: 0.0374  d1.loss_iou: 0.3008  d2.loss_cls: 0.1251  d2.loss_bbox: 0.0363  d2.loss_iou: 0.2942  d3.loss_cls: 0.1212  d3.loss_bbox: 0.0359  d3.loss_iou: 0.2904  d4.loss_cls: 0.1162  d4.loss_bbox: 0.0357  d4.loss_iou: 0.2896  enc_loss_cls: 0.1795  enc_loss_bbox: 0.0411  enc_loss_iou: 0.3260  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0285  dn_loss_iou: 0.2344  d0.dn_loss_cls: 0.0301  d0.dn_loss_bbox: 0.0396  d0.dn_loss_iou: 0.3114  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0301  d1.dn_loss_iou: 0.2464  d2.dn_loss_cls: 0.0068  d2.dn_loss_bbox: 0.0289  d2.dn_loss_iou: 0.2372  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0285  d3.dn_loss_iou: 0.2343  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0285  d4.dn_loss_iou: 0.2341  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:45:24 - mmengine - INFO - Epoch(train) [3][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:36:38  time: 2.1250  data_time: 0.0173  memory: 17568  grad_norm: 47.8459  loss: 4.9240  loss_cls: 0.1009  loss_bbox: 0.0339  loss_iou: 0.2899  d0.loss_cls: 0.1587  d0.loss_bbox: 0.0349  d0.loss_iou: 0.2973  d1.loss_cls: 0.1245  d1.loss_bbox: 0.0352  d1.loss_iou: 0.2949  d2.loss_cls: 0.1130  d2.loss_bbox: 0.0336  d2.loss_iou: 0.2891  d3.loss_cls: 0.1048  d3.loss_bbox: 0.0326  d3.loss_iou: 0.2905  d4.loss_cls: 0.1014  d4.loss_bbox: 0.0339  d4.loss_iou: 0.2912  enc_loss_cls: 0.1618  enc_loss_bbox: 0.0379  enc_loss_iou: 0.3171  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0269  dn_loss_iou: 0.2366  d0.dn_loss_cls: 0.0266  d0.dn_loss_bbox: 0.0373  d0.dn_loss_iou: 0.3146  d1.dn_loss_cls: 0.0093  d1.dn_loss_bbox: 0.0283  d1.dn_loss_iou: 0.2480  d2.dn_loss_cls: 0.0068  d2.dn_loss_bbox: 0.0272  d2.dn_loss_iou: 0.2393  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0270  d3.dn_loss_iou: 0.2364  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0269  d4.dn_loss_iou: 0.2362  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:47:12 - mmengine - INFO - Epoch(train) [3][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:34:52  time: 2.1491  data_time: 0.0172  memory: 17573  grad_norm: 40.8576  loss: 5.5069  loss_cls: 0.1314  loss_bbox: 0.0378  loss_iou: 0.3221  d0.loss_cls: 0.1897  d0.loss_bbox: 0.0409  d0.loss_iou: 0.3393  d1.loss_cls: 0.1513  d1.loss_bbox: 0.0407  d1.loss_iou: 0.3345  d2.loss_cls: 0.1375  d2.loss_bbox: 0.0398  d2.loss_iou: 0.3279  d3.loss_cls: 0.1293  d3.loss_bbox: 0.0394  d3.loss_iou: 0.3255  d4.loss_cls: 0.1323  d4.loss_bbox: 0.0378  d4.loss_iou: 0.3232  enc_loss_cls: 0.1957  enc_loss_bbox: 0.0449  enc_loss_iou: 0.3650  dn_loss_cls: 0.0053  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2448  d0.dn_loss_cls: 0.0280  d0.dn_loss_bbox: 0.0414  d0.dn_loss_iou: 0.3268  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0311  d1.dn_loss_iou: 0.2604  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0295  d2.dn_loss_iou: 0.2483  d3.dn_loss_cls: 0.0056  d3.dn_loss_bbox: 0.0291  d3.dn_loss_iou: 0.2451  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2445  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:48:58 - mmengine - INFO - Epoch(train) [3][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:33:05  time: 2.1299  data_time: 0.0172  memory: 17580  grad_norm: 40.7443  loss: 5.7032  loss_cls: 0.1333  loss_bbox: 0.0392  loss_iou: 0.3582  d0.loss_cls: 0.1725  d0.loss_bbox: 0.0422  d0.loss_iou: 0.3771  d1.loss_cls: 0.1422  d1.loss_bbox: 0.0401  d1.loss_iou: 0.3659  d2.loss_cls: 0.1312  d2.loss_bbox: 0.0394  d2.loss_iou: 0.3614  d3.loss_cls: 0.1311  d3.loss_bbox: 0.0394  d3.loss_iou: 0.3598  d4.loss_cls: 0.1325  d4.loss_bbox: 0.0391  d4.loss_iou: 0.3574  enc_loss_cls: 0.1822  enc_loss_bbox: 0.0433  enc_loss_iou: 0.3912  dn_loss_cls: 0.0062  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2475  d0.dn_loss_cls: 0.0299  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.3245  d1.dn_loss_cls: 0.0104  d1.dn_loss_bbox: 0.0289  d1.dn_loss_iou: 0.2611  d2.dn_loss_cls: 0.0073  d2.dn_loss_bbox: 0.0278  d2.dn_loss_iou: 0.2506  d3.dn_loss_cls: 0.0064  d3.dn_loss_bbox: 0.0276  d3.dn_loss_iou: 0.2479  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0275  d4.dn_loss_iou: 0.2473  loss_num: 0.0003  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 05:50:44 - mmengine - INFO - Epoch(train) [3][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:31:18  time: 2.1243  data_time: 0.0170  memory: 17585  grad_norm: 50.1460  loss: 4.7443  loss_cls: 0.1104  loss_bbox: 0.0326  loss_iou: 0.2871  d0.loss_cls: 0.1521  d0.loss_bbox: 0.0343  d0.loss_iou: 0.3006  d1.loss_cls: 0.1183  d1.loss_bbox: 0.0341  d1.loss_iou: 0.2963  d2.loss_cls: 0.1131  d2.loss_bbox: 0.0335  d2.loss_iou: 0.2899  d3.loss_cls: 0.1114  d3.loss_bbox: 0.0332  d3.loss_iou: 0.2897  d4.loss_cls: 0.1116  d4.loss_bbox: 0.0326  d4.loss_iou: 0.2865  enc_loss_cls: 0.1651  enc_loss_bbox: 0.0376  enc_loss_iou: 0.3258  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0244  dn_loss_iou: 0.2093  d0.dn_loss_cls: 0.0228  d0.dn_loss_bbox: 0.0342  d0.dn_loss_iou: 0.2808  d1.dn_loss_cls: 0.0067  d1.dn_loss_bbox: 0.0259  d1.dn_loss_iou: 0.2210  d2.dn_loss_cls: 0.0048  d2.dn_loss_bbox: 0.0247  d2.dn_loss_iou: 0.2117  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0244  d3.dn_loss_iou: 0.2095  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0244  d4.dn_loss_iou: 0.2090  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 05:52:32 - mmengine - INFO - Epoch(train) [3][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:29:32  time: 2.1595  data_time: 0.0175  memory: 17590  grad_norm: 38.2483  loss: 4.7275  loss_cls: 0.0956  loss_bbox: 0.0331  loss_iou: 0.2782  d0.loss_cls: 0.1450  d0.loss_bbox: 0.0351  d0.loss_iou: 0.2914  d1.loss_cls: 0.1156  d1.loss_bbox: 0.0338  d1.loss_iou: 0.2820  d2.loss_cls: 0.1030  d2.loss_bbox: 0.0339  d2.loss_iou: 0.2800  d3.loss_cls: 0.0988  d3.loss_bbox: 0.0333  d3.loss_iou: 0.2773  d4.loss_cls: 0.0970  d4.loss_bbox: 0.0331  d4.loss_iou: 0.2779  enc_loss_cls: 0.1521  enc_loss_bbox: 0.0378  enc_loss_iou: 0.3096  dn_loss_cls: 0.0052  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2258  d0.dn_loss_cls: 0.0272  d0.dn_loss_bbox: 0.0385  d0.dn_loss_iou: 0.3021  d1.dn_loss_cls: 0.0089  d1.dn_loss_bbox: 0.0290  d1.dn_loss_iou: 0.2382  d2.dn_loss_cls: 0.0063  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2283  d3.dn_loss_cls: 0.0055  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2261  d4.dn_loss_cls: 0.0052  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2255  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:53:37 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 05:54:19 - mmengine - INFO - Epoch(train) [3][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:27:45  time: 2.1368  data_time: 0.0183  memory: 17591  grad_norm: 37.4127  loss: 5.0275  loss_cls: 0.1089  loss_bbox: 0.0361  loss_iou: 0.3079  d0.loss_cls: 0.1458  d0.loss_bbox: 0.0380  d0.loss_iou: 0.3218  d1.loss_cls: 0.1273  d1.loss_bbox: 0.0363  d1.loss_iou: 0.3119  d2.loss_cls: 0.1199  d2.loss_bbox: 0.0362  d2.loss_iou: 0.3089  d3.loss_cls: 0.1141  d3.loss_bbox: 0.0358  d3.loss_iou: 0.3052  d4.loss_cls: 0.1086  d4.loss_bbox: 0.0359  d4.loss_iou: 0.3075  enc_loss_cls: 0.1671  enc_loss_bbox: 0.0408  enc_loss_iou: 0.3402  dn_loss_cls: 0.0061  dn_loss_bbox: 0.0263  dn_loss_iou: 0.2242  d0.dn_loss_cls: 0.0300  d0.dn_loss_bbox: 0.0370  d0.dn_loss_iou: 0.2968  d1.dn_loss_cls: 0.0104  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2368  d2.dn_loss_cls: 0.0072  d2.dn_loss_bbox: 0.0267  d2.dn_loss_iou: 0.2277  d3.dn_loss_cls: 0.0063  d3.dn_loss_bbox: 0.0263  d3.dn_loss_iou: 0.2245  d4.dn_loss_cls: 0.0061  d4.dn_loss_bbox: 0.0263  d4.dn_loss_iou: 0.2241  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:56:06 - mmengine - INFO - Epoch(train) [3][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:25:59  time: 2.1348  data_time: 0.0179  memory: 17574  grad_norm: 42.0124  loss: 5.3024  loss_cls: 0.1201  loss_bbox: 0.0357  loss_iou: 0.3257  d0.loss_cls: 0.1633  d0.loss_bbox: 0.0340  d0.loss_iou: 0.3291  d1.loss_cls: 0.1305  d1.loss_bbox: 0.0360  d1.loss_iou: 0.3264  d2.loss_cls: 0.1285  d2.loss_bbox: 0.0352  d2.loss_iou: 0.3234  d3.loss_cls: 0.1208  d3.loss_bbox: 0.0358  d3.loss_iou: 0.3254  d4.loss_cls: 0.1199  d4.loss_bbox: 0.0355  d4.loss_iou: 0.3250  enc_loss_cls: 0.1752  enc_loss_bbox: 0.0372  enc_loss_iou: 0.3488  dn_loss_cls: 0.0068  dn_loss_bbox: 0.0252  dn_loss_iou: 0.2440  d0.dn_loss_cls: 0.0290  d0.dn_loss_bbox: 0.0354  d0.dn_loss_iou: 0.3236  d1.dn_loss_cls: 0.0109  d1.dn_loss_bbox: 0.0266  d1.dn_loss_iou: 0.2566  d2.dn_loss_cls: 0.0083  d2.dn_loss_bbox: 0.0255  d2.dn_loss_iou: 0.2460  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0252  d3.dn_loss_iou: 0.2435  d4.dn_loss_cls: 0.0065  d4.dn_loss_bbox: 0.0251  d4.dn_loss_iou: 0.2436  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 05:57:22 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 05:57:22 - mmengine - INFO - Saving checkpoint at 3 epochs
2025/10/29 05:57:36 - mmengine - INFO - Epoch(val) [3][ 50/429]    eta: 0:00:39  time: 0.1036  data_time: 0.0034  memory: 17585  
2025/10/29 05:57:41 - mmengine - INFO - Epoch(val) [3][100/429]    eta: 0:00:33  time: 0.1001  data_time: 0.0025  memory: 4269  
2025/10/29 05:57:46 - mmengine - INFO - Epoch(val) [3][150/429]    eta: 0:00:28  time: 0.1002  data_time: 0.0025  memory: 4269  
2025/10/29 05:57:51 - mmengine - INFO - Epoch(val) [3][200/429]    eta: 0:00:23  time: 0.1000  data_time: 0.0024  memory: 4269  
2025/10/29 05:57:56 - mmengine - INFO - Epoch(val) [3][250/429]    eta: 0:00:18  time: 0.1007  data_time: 0.0025  memory: 4269  
2025/10/29 05:58:01 - mmengine - INFO - Epoch(val) [3][300/429]    eta: 0:00:13  time: 0.1001  data_time: 0.0025  memory: 4269  
2025/10/29 05:58:06 - mmengine - INFO - Epoch(val) [3][350/429]    eta: 0:00:07  time: 0.1004  data_time: 0.0025  memory: 4269  
2025/10/29 05:58:11 - mmengine - INFO - Epoch(val) [3][400/429]    eta: 0:00:02  time: 0.0994  data_time: 0.0025  memory: 4269  
2025/10/29 05:58:15 - mmengine - INFO - {'instance_F1_score': 0.6609334032511799, 'instance_acc': 0.4983319109318023, 'image_F1_score': 0.5378670788253478, 'image_acc': 0.39015151515151514}
2025/10/29 05:58:15 - mmengine - INFO - Epoch(val) [3][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.6609  grefcoco_val/refdrone/instance_acc: 0.4983  grefcoco_val/refdrone/image_F1_score: 0.5379  grefcoco_val/refdrone/image_acc: 0.3902  data_time: 0.0026  time: 0.1005
2025/10/29 06:00:02 - mmengine - INFO - Epoch(train) [4][  50/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:22:58  time: 2.1205  data_time: 0.0178  memory: 17574  grad_norm: 38.0314  loss: 5.0419  loss_cls: 0.1079  loss_bbox: 0.0386  loss_iou: 0.3291  d0.loss_cls: 0.1491  d0.loss_bbox: 0.0400  d0.loss_iou: 0.3361  d1.loss_cls: 0.1168  d1.loss_bbox: 0.0391  d1.loss_iou: 0.3343  d2.loss_cls: 0.1136  d2.loss_bbox: 0.0385  d2.loss_iou: 0.3273  d3.loss_cls: 0.1095  d3.loss_bbox: 0.0385  d3.loss_iou: 0.3273  d4.loss_cls: 0.1080  d4.loss_bbox: 0.0386  d4.loss_iou: 0.3274  enc_loss_cls: 0.1597  enc_loss_bbox: 0.0425  enc_loss_iou: 0.3569  dn_loss_cls: 0.0039  dn_loss_bbox: 0.0250  dn_loss_iou: 0.2122  d0.dn_loss_cls: 0.0219  d0.dn_loss_bbox: 0.0340  d0.dn_loss_iou: 0.2800  d1.dn_loss_cls: 0.0068  d1.dn_loss_bbox: 0.0263  d1.dn_loss_iou: 0.2241  d2.dn_loss_cls: 0.0047  d2.dn_loss_bbox: 0.0252  d2.dn_loss_iou: 0.2146  d3.dn_loss_cls: 0.0041  d3.dn_loss_bbox: 0.0250  d3.dn_loss_iou: 0.2124  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0250  d4.dn_loss_iou: 0.2121  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:01:48 - mmengine - INFO - Epoch(train) [4][ 100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:21:11  time: 2.1353  data_time: 0.0181  memory: 17581  grad_norm: 40.0801  loss: 4.5506  loss_cls: 0.0778  loss_bbox: 0.0323  loss_iou: 0.2607  d0.loss_cls: 0.1227  d0.loss_bbox: 0.0344  d0.loss_iou: 0.2732  d1.loss_cls: 0.0889  d1.loss_bbox: 0.0333  d1.loss_iou: 0.2670  d2.loss_cls: 0.0826  d2.loss_bbox: 0.0324  d2.loss_iou: 0.2620  d3.loss_cls: 0.0781  d3.loss_bbox: 0.0323  d3.loss_iou: 0.2609  d4.loss_cls: 0.0777  d4.loss_bbox: 0.0323  d4.loss_iou: 0.2606  enc_loss_cls: 0.1467  enc_loss_bbox: 0.0373  enc_loss_iou: 0.2935  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2354  d0.dn_loss_cls: 0.0295  d0.dn_loss_bbox: 0.0420  d0.dn_loss_iou: 0.3183  d1.dn_loss_cls: 0.0083  d1.dn_loss_bbox: 0.0315  d1.dn_loss_iou: 0.2500  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.0300  d2.dn_loss_iou: 0.2389  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0295  d3.dn_loss_iou: 0.2358  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2353  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:03:35 - mmengine - INFO - Epoch(train) [4][ 150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:19:24  time: 2.1411  data_time: 0.0190  memory: 17581  grad_norm: 38.5511  loss: 5.0528  loss_cls: 0.1037  loss_bbox: 0.0346  loss_iou: 0.3060  d0.loss_cls: 0.1564  d0.loss_bbox: 0.0362  d0.loss_iou: 0.3184  d1.loss_cls: 0.1243  d1.loss_bbox: 0.0356  d1.loss_iou: 0.3112  d2.loss_cls: 0.1140  d2.loss_bbox: 0.0349  d2.loss_iou: 0.3087  d3.loss_cls: 0.1072  d3.loss_bbox: 0.0348  d3.loss_iou: 0.3049  d4.loss_cls: 0.1044  d4.loss_bbox: 0.0345  d4.loss_iou: 0.3056  enc_loss_cls: 0.1728  enc_loss_bbox: 0.0405  enc_loss_iou: 0.3436  dn_loss_cls: 0.0069  dn_loss_bbox: 0.0256  dn_loss_iou: 0.2307  d0.dn_loss_cls: 0.0309  d0.dn_loss_bbox: 0.0366  d0.dn_loss_iou: 0.3096  d1.dn_loss_cls: 0.0110  d1.dn_loss_bbox: 0.0274  d1.dn_loss_iou: 0.2449  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0261  d2.dn_loss_iou: 0.2338  d3.dn_loss_cls: 0.0072  d3.dn_loss_bbox: 0.0257  d3.dn_loss_iou: 0.2309  d4.dn_loss_cls: 0.0071  d4.dn_loss_bbox: 0.0256  d4.dn_loss_iou: 0.2305  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:05:22 - mmengine - INFO - Epoch(train) [4][ 200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:17:38  time: 2.1370  data_time: 0.0255  memory: 17574  grad_norm: 37.2355  loss: 4.9006  loss_cls: 0.1161  loss_bbox: 0.0341  loss_iou: 0.2842  d0.loss_cls: 0.1553  d0.loss_bbox: 0.0377  d0.loss_iou: 0.3017  d1.loss_cls: 0.1297  d1.loss_bbox: 0.0353  d1.loss_iou: 0.2943  d2.loss_cls: 0.1238  d2.loss_bbox: 0.0343  d2.loss_iou: 0.2858  d3.loss_cls: 0.1185  d3.loss_bbox: 0.0341  d3.loss_iou: 0.2834  d4.loss_cls: 0.1137  d4.loss_bbox: 0.0340  d4.loss_iou: 0.2839  enc_loss_cls: 0.1743  enc_loss_bbox: 0.0416  enc_loss_iou: 0.3281  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0264  dn_loss_iou: 0.2233  d0.dn_loss_cls: 0.0239  d0.dn_loss_bbox: 0.0373  d0.dn_loss_iou: 0.3019  d1.dn_loss_cls: 0.0071  d1.dn_loss_bbox: 0.0280  d1.dn_loss_iou: 0.2366  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0267  d2.dn_loss_iou: 0.2259  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0264  d3.dn_loss_iou: 0.2237  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0263  d4.dn_loss_iou: 0.2231  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 06:07:10 - mmengine - INFO - Epoch(train) [4][ 250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:15:52  time: 2.1484  data_time: 0.0178  memory: 17582  grad_norm: 41.9676  loss: 5.1129  loss_cls: 0.1034  loss_bbox: 0.0361  loss_iou: 0.3118  d0.loss_cls: 0.1506  d0.loss_bbox: 0.0383  d0.loss_iou: 0.3267  d1.loss_cls: 0.1213  d1.loss_bbox: 0.0369  d1.loss_iou: 0.3186  d2.loss_cls: 0.1119  d2.loss_bbox: 0.0364  d2.loss_iou: 0.3141  d3.loss_cls: 0.1060  d3.loss_bbox: 0.0362  d3.loss_iou: 0.3131  d4.loss_cls: 0.1026  d4.loss_bbox: 0.0361  d4.loss_iou: 0.3117  enc_loss_cls: 0.1584  enc_loss_bbox: 0.0410  enc_loss_iou: 0.3461  dn_loss_cls: 0.0051  dn_loss_bbox: 0.0280  dn_loss_iou: 0.2371  d0.dn_loss_cls: 0.0278  d0.dn_loss_bbox: 0.0391  d0.dn_loss_iou: 0.3146  d1.dn_loss_cls: 0.0095  d1.dn_loss_bbox: 0.0296  d1.dn_loss_iou: 0.2482  d2.dn_loss_cls: 0.0062  d2.dn_loss_bbox: 0.0285  d2.dn_loss_iou: 0.2396  d3.dn_loss_cls: 0.0053  d3.dn_loss_bbox: 0.0281  d3.dn_loss_iou: 0.2369  d4.dn_loss_cls: 0.0051  d4.dn_loss_bbox: 0.0280  d4.dn_loss_iou: 0.2367  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:08:56 - mmengine - INFO - Epoch(train) [4][ 300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:14:05  time: 2.1297  data_time: 0.0179  memory: 17581  grad_norm: 40.2134  loss: 5.0269  loss_cls: 0.0996  loss_bbox: 0.0332  loss_iou: 0.2928  d0.loss_cls: 0.1436  d0.loss_bbox: 0.0356  d0.loss_iou: 0.3150  d1.loss_cls: 0.1167  d1.loss_bbox: 0.0342  d1.loss_iou: 0.3047  d2.loss_cls: 0.1089  d2.loss_bbox: 0.0333  d2.loss_iou: 0.2959  d3.loss_cls: 0.1040  d3.loss_bbox: 0.0332  d3.loss_iou: 0.2930  d4.loss_cls: 0.0988  d4.loss_bbox: 0.0337  d4.loss_iou: 0.2946  enc_loss_cls: 0.1592  enc_loss_bbox: 0.0396  enc_loss_iou: 0.3368  dn_loss_cls: 0.0086  dn_loss_bbox: 0.0279  dn_loss_iou: 0.2421  d0.dn_loss_cls: 0.0340  d0.dn_loss_bbox: 0.0402  d0.dn_loss_iou: 0.3254  d1.dn_loss_cls: 0.0135  d1.dn_loss_bbox: 0.0298  d1.dn_loss_iou: 0.2573  d2.dn_loss_cls: 0.0096  d2.dn_loss_bbox: 0.0283  d2.dn_loss_iou: 0.2445  d3.dn_loss_cls: 0.0085  d3.dn_loss_bbox: 0.0279  d3.dn_loss_iou: 0.2421  d4.dn_loss_cls: 0.0085  d4.dn_loss_bbox: 0.0279  d4.dn_loss_iou: 0.2418  loss_num: 0.0004  d0.loss_num: 0.0006  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 06:10:42 - mmengine - INFO - Epoch(train) [4][ 350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:12:18  time: 2.1228  data_time: 0.0174  memory: 17581  grad_norm: 42.9835  loss: 5.0425  loss_cls: 0.0980  loss_bbox: 0.0359  loss_iou: 0.3057  d0.loss_cls: 0.1412  d0.loss_bbox: 0.0381  d0.loss_iou: 0.3164  d1.loss_cls: 0.1089  d1.loss_bbox: 0.0371  d1.loss_iou: 0.3106  d2.loss_cls: 0.1031  d2.loss_bbox: 0.0363  d2.loss_iou: 0.3058  d3.loss_cls: 0.1033  d3.loss_bbox: 0.0360  d3.loss_iou: 0.3028  d4.loss_cls: 0.1014  d4.loss_bbox: 0.0359  d4.loss_iou: 0.3042  enc_loss_cls: 0.1483  enc_loss_bbox: 0.0417  enc_loss_iou: 0.3375  dn_loss_cls: 0.0053  dn_loss_bbox: 0.0275  dn_loss_iou: 0.2428  d0.dn_loss_cls: 0.0292  d0.dn_loss_bbox: 0.0386  d0.dn_loss_iou: 0.3233  d1.dn_loss_cls: 0.0094  d1.dn_loss_bbox: 0.0291  d1.dn_loss_iou: 0.2557  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0279  d2.dn_loss_iou: 0.2456  d3.dn_loss_cls: 0.0058  d3.dn_loss_bbox: 0.0275  d3.dn_loss_iou: 0.2426  d4.dn_loss_cls: 0.0054  d4.dn_loss_bbox: 0.0275  d4.dn_loss_iou: 0.2424  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:12:30 - mmengine - INFO - Epoch(train) [4][ 400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:10:31  time: 2.1490  data_time: 0.0172  memory: 17572  grad_norm: 43.6480  loss: 4.6248  loss_cls: 0.1044  loss_bbox: 0.0312  loss_iou: 0.2762  d0.loss_cls: 0.1415  d0.loss_bbox: 0.0327  d0.loss_iou: 0.2913  d1.loss_cls: 0.1157  d1.loss_bbox: 0.0320  d1.loss_iou: 0.2821  d2.loss_cls: 0.1133  d2.loss_bbox: 0.0314  d2.loss_iou: 0.2768  d3.loss_cls: 0.1108  d3.loss_bbox: 0.0312  d3.loss_iou: 0.2748  d4.loss_cls: 0.1037  d4.loss_bbox: 0.0315  d4.loss_iou: 0.2785  enc_loss_cls: 0.1711  enc_loss_bbox: 0.0362  enc_loss_iou: 0.3142  dn_loss_cls: 0.0059  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2064  d0.dn_loss_cls: 0.0268  d0.dn_loss_bbox: 0.0340  d0.dn_loss_iou: 0.2804  d1.dn_loss_cls: 0.0092  d1.dn_loss_bbox: 0.0255  d1.dn_loss_iou: 0.2186  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0244  d2.dn_loss_iou: 0.2091  d3.dn_loss_cls: 0.0059  d3.dn_loss_bbox: 0.0241  d3.dn_loss_iou: 0.2062  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0240  d4.dn_loss_iou: 0.2060  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:14:16 - mmengine - INFO - Epoch(train) [4][ 450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:08:44  time: 2.1206  data_time: 0.0168  memory: 17586  grad_norm: 41.3886  loss: 4.3414  loss_cls: 0.0818  loss_bbox: 0.0265  loss_iou: 0.2312  d0.loss_cls: 0.1187  d0.loss_bbox: 0.0283  d0.loss_iou: 0.2476  d1.loss_cls: 0.0940  d1.loss_bbox: 0.0272  d1.loss_iou: 0.2369  d2.loss_cls: 0.0885  d2.loss_bbox: 0.0270  d2.loss_iou: 0.2347  d3.loss_cls: 0.0846  d3.loss_bbox: 0.0265  d3.loss_iou: 0.2310  d4.loss_cls: 0.0827  d4.loss_bbox: 0.0265  d4.loss_iou: 0.2311  enc_loss_cls: 0.1275  enc_loss_bbox: 0.0309  enc_loss_iou: 0.2608  dn_loss_cls: 0.0043  dn_loss_bbox: 0.0290  dn_loss_iou: 0.2428  d0.dn_loss_cls: 0.0285  d0.dn_loss_bbox: 0.0413  d0.dn_loss_iou: 0.3228  d1.dn_loss_cls: 0.0080  d1.dn_loss_bbox: 0.0308  d1.dn_loss_iou: 0.2553  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0294  d2.dn_loss_iou: 0.2451  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0290  d3.dn_loss_iou: 0.2429  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0290  d4.dn_loss_iou: 0.2425  loss_num: 0.0003  d0.loss_num: 0.0006  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:16:02 - mmengine - INFO - Epoch(train) [4][ 500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:06:57  time: 2.1258  data_time: 0.0169  memory: 17579  grad_norm: 42.5277  loss: 5.0521  loss_cls: 0.1028  loss_bbox: 0.0335  loss_iou: 0.3064  d0.loss_cls: 0.1452  d0.loss_bbox: 0.0363  d0.loss_iou: 0.3242  d1.loss_cls: 0.1208  d1.loss_bbox: 0.0347  d1.loss_iou: 0.3126  d2.loss_cls: 0.1071  d2.loss_bbox: 0.0343  d2.loss_iou: 0.3099  d3.loss_cls: 0.1037  d3.loss_bbox: 0.0342  d3.loss_iou: 0.3077  d4.loss_cls: 0.1015  d4.loss_bbox: 0.0337  d4.loss_iou: 0.3039  enc_loss_cls: 0.1562  enc_loss_bbox: 0.0411  enc_loss_iou: 0.3506  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0273  dn_loss_iou: 0.2363  d0.dn_loss_cls: 0.0290  d0.dn_loss_bbox: 0.0377  d0.dn_loss_iou: 0.3124  d1.dn_loss_cls: 0.0106  d1.dn_loss_bbox: 0.0290  d1.dn_loss_iou: 0.2484  d2.dn_loss_cls: 0.0071  d2.dn_loss_bbox: 0.0277  d2.dn_loss_iou: 0.2391  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2364  d4.dn_loss_cls: 0.0059  d4.dn_loss_bbox: 0.0273  d4.dn_loss_iou: 0.2361  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:17:49 - mmengine - INFO - Epoch(train) [4][ 550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:05:11  time: 2.1442  data_time: 0.0172  memory: 17568  grad_norm: 41.0941  loss: 4.7797  loss_cls: 0.1004  loss_bbox: 0.0325  loss_iou: 0.2890  d0.loss_cls: 0.1700  d0.loss_bbox: 0.0323  d0.loss_iou: 0.2952  d1.loss_cls: 0.1212  d1.loss_bbox: 0.0322  d1.loss_iou: 0.2964  d2.loss_cls: 0.1200  d2.loss_bbox: 0.0311  d2.loss_iou: 0.2873  d3.loss_cls: 0.1027  d3.loss_bbox: 0.0328  d3.loss_iou: 0.2898  d4.loss_cls: 0.1001  d4.loss_bbox: 0.0325  d4.loss_iou: 0.2887  enc_loss_cls: 0.1672  enc_loss_bbox: 0.0353  enc_loss_iou: 0.3142  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0254  dn_loss_iou: 0.2181  d0.dn_loss_cls: 0.0230  d0.dn_loss_bbox: 0.0354  d0.dn_loss_iou: 0.2923  d1.dn_loss_cls: 0.0068  d1.dn_loss_bbox: 0.0267  d1.dn_loss_iou: 0.2294  d2.dn_loss_cls: 0.0048  d2.dn_loss_bbox: 0.0257  d2.dn_loss_iou: 0.2199  d3.dn_loss_cls: 0.0041  d3.dn_loss_bbox: 0.0255  d3.dn_loss_iou: 0.2181  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0254  d4.dn_loss_iou: 0.2178  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:19:36 - mmengine - INFO - Epoch(train) [4][ 600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:03:24  time: 2.1293  data_time: 0.0172  memory: 17585  grad_norm: 38.8595  loss: 5.5037  loss_cls: 0.1019  loss_bbox: 0.0377  loss_iou: 0.3631  d0.loss_cls: 0.1582  d0.loss_bbox: 0.0384  d0.loss_iou: 0.3748  d1.loss_cls: 0.1282  d1.loss_bbox: 0.0386  d1.loss_iou: 0.3686  d2.loss_cls: 0.1141  d2.loss_bbox: 0.0377  d2.loss_iou: 0.3631  d3.loss_cls: 0.1074  d3.loss_bbox: 0.0374  d3.loss_iou: 0.3609  d4.loss_cls: 0.1029  d4.loss_bbox: 0.0377  d4.loss_iou: 0.3628  enc_loss_cls: 0.1661  enc_loss_bbox: 0.0408  enc_loss_iou: 0.3959  dn_loss_cls: 0.0062  dn_loss_bbox: 0.0255  dn_loss_iou: 0.2380  d0.dn_loss_cls: 0.0333  d0.dn_loss_bbox: 0.0353  d0.dn_loss_iou: 0.3189  d1.dn_loss_cls: 0.0109  d1.dn_loss_bbox: 0.0272  d1.dn_loss_iou: 0.2547  d2.dn_loss_cls: 0.0076  d2.dn_loss_bbox: 0.0258  d2.dn_loss_iou: 0.2422  d3.dn_loss_cls: 0.0068  d3.dn_loss_bbox: 0.0255  d3.dn_loss_iou: 0.2382  d4.dn_loss_cls: 0.0063  d4.dn_loss_bbox: 0.0255  d4.dn_loss_iou: 0.2377  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:21:24 - mmengine - INFO - Epoch(train) [4][ 650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 2:01:38  time: 2.1564  data_time: 0.0178  memory: 17603  grad_norm: 42.0843  loss: 4.1647  loss_cls: 0.0786  loss_bbox: 0.0281  loss_iou: 0.2256  d0.loss_cls: 0.1151  d0.loss_bbox: 0.0299  d0.loss_iou: 0.2335  d1.loss_cls: 0.0928  d1.loss_bbox: 0.0285  d1.loss_iou: 0.2287  d2.loss_cls: 0.0850  d2.loss_bbox: 0.0280  d2.loss_iou: 0.2253  d3.loss_cls: 0.0828  d3.loss_bbox: 0.0281  d3.loss_iou: 0.2252  d4.loss_cls: 0.0794  d4.loss_bbox: 0.0281  d4.loss_iou: 0.2255  enc_loss_cls: 0.1352  enc_loss_bbox: 0.0324  enc_loss_iou: 0.2505  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0279  dn_loss_iou: 0.2256  d0.dn_loss_cls: 0.0239  d0.dn_loss_bbox: 0.0393  d0.dn_loss_iou: 0.3029  d1.dn_loss_cls: 0.0082  d1.dn_loss_bbox: 0.0294  d1.dn_loss_iou: 0.2377  d2.dn_loss_cls: 0.0055  d2.dn_loss_bbox: 0.0282  d2.dn_loss_iou: 0.2279  d3.dn_loss_cls: 0.0045  d3.dn_loss_bbox: 0.0280  d3.dn_loss_iou: 0.2259  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0278  d4.dn_loss_iou: 0.2253  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:23:10 - mmengine - INFO - Epoch(train) [4][ 700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:59:51  time: 2.1268  data_time: 0.0174  memory: 17585  grad_norm: 37.9004  loss: 5.0885  loss_cls: 0.1076  loss_bbox: 0.0381  loss_iou: 0.3276  d0.loss_cls: 0.1552  d0.loss_bbox: 0.0375  d0.loss_iou: 0.3358  d1.loss_cls: 0.1224  d1.loss_bbox: 0.0378  d1.loss_iou: 0.3337  d2.loss_cls: 0.1132  d2.loss_bbox: 0.0371  d2.loss_iou: 0.3281  d3.loss_cls: 0.1106  d3.loss_bbox: 0.0360  d3.loss_iou: 0.3225  d4.loss_cls: 0.1102  d4.loss_bbox: 0.0375  d4.loss_iou: 0.3246  enc_loss_cls: 0.1613  enc_loss_bbox: 0.0401  enc_loss_iou: 0.3549  dn_loss_cls: 0.0049  dn_loss_bbox: 0.0257  dn_loss_iou: 0.2179  d0.dn_loss_cls: 0.0273  d0.dn_loss_bbox: 0.0353  d0.dn_loss_iou: 0.2846  d1.dn_loss_cls: 0.0094  d1.dn_loss_bbox: 0.0276  d1.dn_loss_iou: 0.2314  d2.dn_loss_cls: 0.0063  d2.dn_loss_bbox: 0.0262  d2.dn_loss_iou: 0.2205  d3.dn_loss_cls: 0.0050  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2182  d4.dn_loss_cls: 0.0050  d4.dn_loss_bbox: 0.0257  d4.dn_loss_iou: 0.2177  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:24:56 - mmengine - INFO - Epoch(train) [4][ 750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:58:04  time: 2.1284  data_time: 0.0179  memory: 17591  grad_norm: 46.9223  loss: 5.1885  loss_cls: 0.1059  loss_bbox: 0.0362  loss_iou: 0.3452  d0.loss_cls: 0.1476  d0.loss_bbox: 0.0392  d0.loss_iou: 0.3634  d1.loss_cls: 0.1200  d1.loss_bbox: 0.0369  d1.loss_iou: 0.3536  d2.loss_cls: 0.1106  d2.loss_bbox: 0.0361  d2.loss_iou: 0.3465  d3.loss_cls: 0.1090  d3.loss_bbox: 0.0360  d3.loss_iou: 0.3443  d4.loss_cls: 0.1067  d4.loss_bbox: 0.0361  d4.loss_iou: 0.3434  enc_loss_cls: 0.1590  enc_loss_bbox: 0.0412  enc_loss_iou: 0.3820  dn_loss_cls: 0.0072  dn_loss_bbox: 0.0237  dn_loss_iou: 0.2157  d0.dn_loss_cls: 0.0242  d0.dn_loss_bbox: 0.0324  d0.dn_loss_iou: 0.2823  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0250  d1.dn_loss_iou: 0.2251  d2.dn_loss_cls: 0.0080  d2.dn_loss_bbox: 0.0240  d2.dn_loss_iou: 0.2167  d3.dn_loss_cls: 0.0076  d3.dn_loss_bbox: 0.0238  d3.dn_loss_iou: 0.2161  d4.dn_loss_cls: 0.0071  d4.dn_loss_bbox: 0.0237  d4.dn_loss_iou: 0.2154  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:26:44 - mmengine - INFO - Epoch(train) [4][ 800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:56:18  time: 2.1558  data_time: 0.0171  memory: 17581  grad_norm: 54.1001  loss: 4.7362  loss_cls: 0.0888  loss_bbox: 0.0341  loss_iou: 0.2931  d0.loss_cls: 0.1390  d0.loss_bbox: 0.0378  d0.loss_iou: 0.3101  d1.loss_cls: 0.1101  d1.loss_bbox: 0.0348  d1.loss_iou: 0.2969  d2.loss_cls: 0.0995  d2.loss_bbox: 0.0340  d2.loss_iou: 0.2912  d3.loss_cls: 0.0949  d3.loss_bbox: 0.0339  d3.loss_iou: 0.2912  d4.loss_cls: 0.0900  d4.loss_bbox: 0.0338  d4.loss_iou: 0.2907  enc_loss_cls: 0.1544  enc_loss_bbox: 0.0397  enc_loss_iou: 0.3250  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0270  dn_loss_iou: 0.2157  d0.dn_loss_cls: 0.0279  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.2915  d1.dn_loss_cls: 0.0078  d1.dn_loss_bbox: 0.0285  d1.dn_loss_iou: 0.2274  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2179  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0271  d3.dn_loss_iou: 0.2156  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0270  d4.dn_loss_iou: 0.2153  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:28:31 - mmengine - INFO - Epoch(train) [4][ 850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:54:31  time: 2.1333  data_time: 0.0177  memory: 17579  grad_norm: 44.3946  loss: 4.7444  loss_cls: 0.1032  loss_bbox: 0.0339  loss_iou: 0.2875  d0.loss_cls: 0.1446  d0.loss_bbox: 0.0359  d0.loss_iou: 0.2981  d1.loss_cls: 0.1173  d1.loss_bbox: 0.0343  d1.loss_iou: 0.2908  d2.loss_cls: 0.1086  d2.loss_bbox: 0.0338  d2.loss_iou: 0.2855  d3.loss_cls: 0.0992  d3.loss_bbox: 0.0341  d3.loss_iou: 0.2892  d4.loss_cls: 0.1026  d4.loss_bbox: 0.0335  d4.loss_iou: 0.2834  enc_loss_cls: 0.1584  enc_loss_bbox: 0.0398  enc_loss_iou: 0.3222  dn_loss_cls: 0.0049  dn_loss_bbox: 0.0258  dn_loss_iou: 0.2183  d0.dn_loss_cls: 0.0249  d0.dn_loss_bbox: 0.0356  d0.dn_loss_iou: 0.2836  d1.dn_loss_cls: 0.0083  d1.dn_loss_bbox: 0.0269  d1.dn_loss_iou: 0.2280  d2.dn_loss_cls: 0.0061  d2.dn_loss_bbox: 0.0260  d2.dn_loss_iou: 0.2198  d3.dn_loss_cls: 0.0052  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2182  d4.dn_loss_cls: 0.0049  d4.dn_loss_bbox: 0.0258  d4.dn_loss_iou: 0.2181  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:30:07 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 06:30:17 - mmengine - INFO - Epoch(train) [4][ 900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:52:45  time: 2.1284  data_time: 0.0166  memory: 17585  grad_norm: 38.8777  loss: 4.3641  loss_cls: 0.0824  loss_bbox: 0.0293  loss_iou: 0.2483  d0.loss_cls: 0.1245  d0.loss_bbox: 0.0303  d0.loss_iou: 0.2546  d1.loss_cls: 0.0979  d1.loss_bbox: 0.0296  d1.loss_iou: 0.2496  d2.loss_cls: 0.0912  d2.loss_bbox: 0.0294  d2.loss_iou: 0.2489  d3.loss_cls: 0.0837  d3.loss_bbox: 0.0294  d3.loss_iou: 0.2497  d4.loss_cls: 0.0826  d4.loss_bbox: 0.0292  d4.loss_iou: 0.2479  enc_loss_cls: 0.1320  enc_loss_bbox: 0.0331  enc_loss_iou: 0.2697  dn_loss_cls: 0.0033  dn_loss_bbox: 0.0268  dn_loss_iou: 0.2313  d0.dn_loss_cls: 0.0198  d0.dn_loss_bbox: 0.0374  d0.dn_loss_iou: 0.3063  d1.dn_loss_cls: 0.0054  d1.dn_loss_bbox: 0.0284  d1.dn_loss_iou: 0.2431  d2.dn_loss_cls: 0.0039  d2.dn_loss_bbox: 0.0271  d2.dn_loss_iou: 0.2338  d3.dn_loss_cls: 0.0033  d3.dn_loss_bbox: 0.0269  d3.dn_loss_iou: 0.2313  d4.dn_loss_cls: 0.0032  d4.dn_loss_bbox: 0.0268  d4.dn_loss_iou: 0.2307  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:32:05 - mmengine - INFO - Epoch(train) [4][ 950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:50:58  time: 2.1529  data_time: 0.0183  memory: 17565  grad_norm: 40.1187  loss: 4.4904  loss_cls: 0.0902  loss_bbox: 0.0310  loss_iou: 0.2607  d0.loss_cls: 0.1246  d0.loss_bbox: 0.0335  d0.loss_iou: 0.2726  d1.loss_cls: 0.1042  d1.loss_bbox: 0.0311  d1.loss_iou: 0.2633  d2.loss_cls: 0.0956  d2.loss_bbox: 0.0311  d2.loss_iou: 0.2623  d3.loss_cls: 0.0929  d3.loss_bbox: 0.0309  d3.loss_iou: 0.2602  d4.loss_cls: 0.0926  d4.loss_bbox: 0.0308  d4.loss_iou: 0.2597  enc_loss_cls: 0.1420  enc_loss_bbox: 0.0362  enc_loss_iou: 0.2937  dn_loss_cls: 0.0046  dn_loss_bbox: 0.0269  dn_loss_iou: 0.2227  d0.dn_loss_cls: 0.0250  d0.dn_loss_bbox: 0.0376  d0.dn_loss_iou: 0.2952  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0284  d1.dn_loss_iou: 0.2339  d2.dn_loss_cls: 0.0054  d2.dn_loss_bbox: 0.0274  d2.dn_loss_iou: 0.2257  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0270  d3.dn_loss_iou: 0.2230  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0269  d4.dn_loss_iou: 0.2225  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:33:51 - mmengine - INFO - Epoch(train) [4][1000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:49:12  time: 2.1316  data_time: 0.0178  memory: 17581  grad_norm: 37.9093  loss: 4.7920  loss_cls: 0.0970  loss_bbox: 0.0306  loss_iou: 0.2875  d0.loss_cls: 0.1461  d0.loss_bbox: 0.0327  d0.loss_iou: 0.2982  d1.loss_cls: 0.1071  d1.loss_bbox: 0.0325  d1.loss_iou: 0.2949  d2.loss_cls: 0.1023  d2.loss_bbox: 0.0309  d2.loss_iou: 0.2904  d3.loss_cls: 0.0995  d3.loss_bbox: 0.0306  d3.loss_iou: 0.2868  d4.loss_cls: 0.0985  d4.loss_bbox: 0.0306  d4.loss_iou: 0.2868  enc_loss_cls: 0.1515  enc_loss_bbox: 0.0349  enc_loss_iou: 0.3180  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0246  dn_loss_iou: 0.2331  d0.dn_loss_cls: 0.0272  d0.dn_loss_bbox: 0.0342  d0.dn_loss_iou: 0.3078  d1.dn_loss_cls: 0.0085  d1.dn_loss_bbox: 0.0260  d1.dn_loss_iou: 0.2454  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0251  d2.dn_loss_iou: 0.2358  d3.dn_loss_cls: 0.0048  d3.dn_loss_bbox: 0.0247  d3.dn_loss_iou: 0.2333  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0246  d4.dn_loss_iou: 0.2329  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 06:35:38 - mmengine - INFO - Epoch(train) [4][1050/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:47:25  time: 2.1409  data_time: 0.0180  memory: 17568  grad_norm: 40.7017  loss: 4.2607  loss_cls: 0.0819  loss_bbox: 0.0282  loss_iou: 0.2274  d0.loss_cls: 0.1217  d0.loss_bbox: 0.0297  d0.loss_iou: 0.2380  d1.loss_cls: 0.0904  d1.loss_bbox: 0.0287  d1.loss_iou: 0.2315  d2.loss_cls: 0.0837  d2.loss_bbox: 0.0286  d2.loss_iou: 0.2314  d3.loss_cls: 0.0819  d3.loss_bbox: 0.0283  d3.loss_iou: 0.2281  d4.loss_cls: 0.0813  d4.loss_bbox: 0.0282  d4.loss_iou: 0.2272  enc_loss_cls: 0.1343  enc_loss_bbox: 0.0331  enc_loss_iou: 0.2589  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0295  dn_loss_iou: 0.2335  d0.dn_loss_cls: 0.0262  d0.dn_loss_bbox: 0.0417  d0.dn_loss_iou: 0.3115  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0311  d1.dn_loss_iou: 0.2446  d2.dn_loss_cls: 0.0055  d2.dn_loss_bbox: 0.0299  d2.dn_loss_iou: 0.2356  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0295  d3.dn_loss_iou: 0.2339  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.0295  d4.dn_loss_iou: 0.2333  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0003
2025/10/29 06:37:26 - mmengine - INFO - Epoch(train) [4][1100/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:45:38  time: 2.1410  data_time: 0.0174  memory: 17559  grad_norm: 39.0514  loss: 4.6688  loss_cls: 0.0798  loss_bbox: 0.0342  loss_iou: 0.2921  d0.loss_cls: 0.1233  d0.loss_bbox: 0.0348  d0.loss_iou: 0.3033  d1.loss_cls: 0.0935  d1.loss_bbox: 0.0333  d1.loss_iou: 0.2950  d2.loss_cls: 0.0867  d2.loss_bbox: 0.0328  d2.loss_iou: 0.2941  d3.loss_cls: 0.0817  d3.loss_bbox: 0.0343  d3.loss_iou: 0.2932  d4.loss_cls: 0.0810  d4.loss_bbox: 0.0342  d4.loss_iou: 0.2919  enc_loss_cls: 0.1342  enc_loss_bbox: 0.0377  enc_loss_iou: 0.3213  dn_loss_cls: 0.0057  dn_loss_bbox: 0.0269  dn_loss_iou: 0.2217  d0.dn_loss_cls: 0.0268  d0.dn_loss_bbox: 0.0372  d0.dn_loss_iou: 0.2955  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0283  d1.dn_loss_iou: 0.2344  d2.dn_loss_cls: 0.0067  d2.dn_loss_bbox: 0.0272  d2.dn_loss_iou: 0.2250  d3.dn_loss_cls: 0.0059  d3.dn_loss_bbox: 0.0270  d3.dn_loss_iou: 0.2221  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0269  d4.dn_loss_iou: 0.2215  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0002
2025/10/29 06:39:12 - mmengine - INFO - Epoch(train) [4][1150/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:43:51  time: 2.1234  data_time: 0.0176  memory: 17574  grad_norm: 44.4902  loss: 4.6304  loss_cls: 0.0862  loss_bbox: 0.0324  loss_iou: 0.2769  d0.loss_cls: 0.1221  d0.loss_bbox: 0.0344  d0.loss_iou: 0.2921  d1.loss_cls: 0.0920  d1.loss_bbox: 0.0334  d1.loss_iou: 0.2851  d2.loss_cls: 0.0910  d2.loss_bbox: 0.0328  d2.loss_iou: 0.2790  d3.loss_cls: 0.0883  d3.loss_bbox: 0.0325  d3.loss_iou: 0.2768  d4.loss_cls: 0.0868  d4.loss_bbox: 0.0324  d4.loss_iou: 0.2762  enc_loss_cls: 0.1345  enc_loss_bbox: 0.0373  enc_loss_iou: 0.3094  dn_loss_cls: 0.0043  dn_loss_bbox: 0.0262  dn_loss_iou: 0.2311  d0.dn_loss_cls: 0.0238  d0.dn_loss_bbox: 0.0366  d0.dn_loss_iou: 0.3060  d1.dn_loss_cls: 0.0072  d1.dn_loss_bbox: 0.0279  d1.dn_loss_iou: 0.2439  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0266  d2.dn_loss_iou: 0.2341  d3.dn_loss_cls: 0.0044  d3.dn_loss_bbox: 0.0263  d3.dn_loss_iou: 0.2314  d4.dn_loss_cls: 0.0043  d4.dn_loss_bbox: 0.0261  d4.dn_loss_iou: 0.2309  loss_num: 0.0004  d0.loss_num: 0.0004  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 06:41:00 - mmengine - INFO - Epoch(train) [4][1200/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:42:05  time: 2.1584  data_time: 0.0181  memory: 17573  grad_norm: 42.0818  loss: 4.9227  loss_cls: 0.0973  loss_bbox: 0.0362  loss_iou: 0.3218  d0.loss_cls: 0.1458  d0.loss_bbox: 0.0377  d0.loss_iou: 0.3328  d1.loss_cls: 0.1169  d1.loss_bbox: 0.0364  d1.loss_iou: 0.3240  d2.loss_cls: 0.1060  d2.loss_bbox: 0.0362  d2.loss_iou: 0.3237  d3.loss_cls: 0.1011  d3.loss_bbox: 0.0364  d3.loss_iou: 0.3230  d4.loss_cls: 0.0956  d4.loss_bbox: 0.0362  d4.loss_iou: 0.3225  enc_loss_cls: 0.1534  enc_loss_bbox: 0.0426  enc_loss_iou: 0.3629  dn_loss_cls: 0.0049  dn_loss_bbox: 0.0238  dn_loss_iou: 0.2073  d0.dn_loss_cls: 0.0235  d0.dn_loss_bbox: 0.0334  d0.dn_loss_iou: 0.2748  d1.dn_loss_cls: 0.0084  d1.dn_loss_bbox: 0.0254  d1.dn_loss_iou: 0.2187  d2.dn_loss_cls: 0.0055  d2.dn_loss_bbox: 0.0242  d2.dn_loss_iou: 0.2101  d3.dn_loss_cls: 0.0050  d3.dn_loss_bbox: 0.0238  d3.dn_loss_iou: 0.2074  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0238  d4.dn_loss_iou: 0.2071  loss_num: 0.0004  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0004  d4.loss_num: 0.0004
2025/10/29 06:42:46 - mmengine - INFO - Epoch(train) [4][1250/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:40:18  time: 2.1257  data_time: 0.0177  memory: 17565  grad_norm: 42.4730  loss: 5.1483  loss_cls: 0.1017  loss_bbox: 0.0378  loss_iou: 0.3302  d0.loss_cls: 0.1485  d0.loss_bbox: 0.0412  d0.loss_iou: 0.3496  d1.loss_cls: 0.1216  d1.loss_bbox: 0.0393  d1.loss_iou: 0.3363  d2.loss_cls: 0.1081  d2.loss_bbox: 0.0379  d2.loss_iou: 0.3336  d3.loss_cls: 0.1034  d3.loss_bbox: 0.0378  d3.loss_iou: 0.3314  d4.loss_cls: 0.1009  d4.loss_bbox: 0.0378  d4.loss_iou: 0.3294  enc_loss_cls: 0.1692  enc_loss_bbox: 0.0442  enc_loss_iou: 0.3711  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0258  dn_loss_iou: 0.2191  d0.dn_loss_cls: 0.0293  d0.dn_loss_bbox: 0.0363  d0.dn_loss_iou: 0.2935  d1.dn_loss_cls: 0.0101  d1.dn_loss_bbox: 0.0274  d1.dn_loss_iou: 0.2310  d2.dn_loss_cls: 0.0075  d2.dn_loss_bbox: 0.0261  d2.dn_loss_iou: 0.2216  d3.dn_loss_cls: 0.0062  d3.dn_loss_bbox: 0.0259  d3.dn_loss_iou: 0.2194  d4.dn_loss_cls: 0.0058  d4.dn_loss_bbox: 0.0258  d4.dn_loss_iou: 0.2188  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:44:32 - mmengine - INFO - Epoch(train) [4][1300/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:38:32  time: 2.1300  data_time: 0.0174  memory: 17575  grad_norm: 44.0602  loss: 4.6329  loss_cls: 0.0823  loss_bbox: 0.0349  loss_iou: 0.2714  d0.loss_cls: 0.1275  d0.loss_bbox: 0.0363  d0.loss_iou: 0.2816  d1.loss_cls: 0.0962  d1.loss_bbox: 0.0355  d1.loss_iou: 0.2761  d2.loss_cls: 0.0931  d2.loss_bbox: 0.0352  d2.loss_iou: 0.2744  d3.loss_cls: 0.0854  d3.loss_bbox: 0.0349  d3.loss_iou: 0.2721  d4.loss_cls: 0.0821  d4.loss_bbox: 0.0349  d4.loss_iou: 0.2716  enc_loss_cls: 0.1299  enc_loss_bbox: 0.0388  enc_loss_iou: 0.2954  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0281  dn_loss_iou: 0.2366  d0.dn_loss_cls: 0.0247  d0.dn_loss_bbox: 0.0380  d0.dn_loss_iou: 0.3094  d1.dn_loss_cls: 0.0078  d1.dn_loss_bbox: 0.0296  d1.dn_loss_iou: 0.2489  d2.dn_loss_cls: 0.0059  d2.dn_loss_bbox: 0.0285  d2.dn_loss_iou: 0.2399  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0282  d3.dn_loss_iou: 0.2373  d4.dn_loss_cls: 0.0047  d4.dn_loss_bbox: 0.0280  d4.dn_loss_iou: 0.2364  loss_num: 0.0002  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:46:20 - mmengine - INFO - Epoch(train) [4][1350/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:36:45  time: 2.1531  data_time: 0.0188  memory: 17554  grad_norm: 42.4011  loss: 4.8902  loss_cls: 0.0928  loss_bbox: 0.0340  loss_iou: 0.2987  d0.loss_cls: 0.1373  d0.loss_bbox: 0.0363  d0.loss_iou: 0.3164  d1.loss_cls: 0.1077  d1.loss_bbox: 0.0348  d1.loss_iou: 0.3069  d2.loss_cls: 0.0909  d2.loss_bbox: 0.0348  d2.loss_iou: 0.3061  d3.loss_cls: 0.0941  d3.loss_bbox: 0.0343  d3.loss_iou: 0.3004  d4.loss_cls: 0.0920  d4.loss_bbox: 0.0340  d4.loss_iou: 0.2987  enc_loss_cls: 0.1363  enc_loss_bbox: 0.0392  enc_loss_iou: 0.3383  dn_loss_cls: 0.0047  dn_loss_bbox: 0.0262  dn_loss_iou: 0.2353  d0.dn_loss_cls: 0.0266  d0.dn_loss_bbox: 0.0357  d0.dn_loss_iou: 0.3081  d1.dn_loss_cls: 0.0089  d1.dn_loss_bbox: 0.0276  d1.dn_loss_iou: 0.2475  d2.dn_loss_cls: 0.0059  d2.dn_loss_bbox: 0.0266  d2.dn_loss_iou: 0.2380  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0263  d3.dn_loss_iou: 0.2359  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0262  d4.dn_loss_iou: 0.2352  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:48:07 - mmengine - INFO - Epoch(train) [4][1400/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:34:58  time: 2.1293  data_time: 0.0174  memory: 17585  grad_norm: 37.9213  loss: 4.9988  loss_cls: 0.0874  loss_bbox: 0.0368  loss_iou: 0.3144  d0.loss_cls: 0.1504  d0.loss_bbox: 0.0388  d0.loss_iou: 0.3329  d1.loss_cls: 0.1080  d1.loss_bbox: 0.0382  d1.loss_iou: 0.3236  d2.loss_cls: 0.0991  d2.loss_bbox: 0.0374  d2.loss_iou: 0.3176  d3.loss_cls: 0.0889  d3.loss_bbox: 0.0374  d3.loss_iou: 0.3187  d4.loss_cls: 0.0877  d4.loss_bbox: 0.0368  d4.loss_iou: 0.3157  enc_loss_cls: 0.1542  enc_loss_bbox: 0.0421  enc_loss_iou: 0.3494  dn_loss_cls: 0.0056  dn_loss_bbox: 0.0260  dn_loss_iou: 0.2262  d0.dn_loss_cls: 0.0276  d0.dn_loss_bbox: 0.0369  d0.dn_loss_iou: 0.3047  d1.dn_loss_cls: 0.0099  d1.dn_loss_bbox: 0.0277  d1.dn_loss_iou: 0.2390  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0264  d2.dn_loss_iou: 0.2287  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0260  d3.dn_loss_iou: 0.2264  d4.dn_loss_cls: 0.0056  d4.dn_loss_bbox: 0.0260  d4.dn_loss_iou: 0.2261  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:49:54 - mmengine - INFO - Epoch(train) [4][1450/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:33:12  time: 2.1437  data_time: 0.0177  memory: 17580  grad_norm: 43.9111  loss: 4.5931  loss_cls: 0.0812  loss_bbox: 0.0303  loss_iou: 0.2711  d0.loss_cls: 0.1505  d0.loss_bbox: 0.0328  d0.loss_iou: 0.2871  d1.loss_cls: 0.0987  d1.loss_bbox: 0.0310  d1.loss_iou: 0.2784  d2.loss_cls: 0.0924  d2.loss_bbox: 0.0304  d2.loss_iou: 0.2741  d3.loss_cls: 0.0874  d3.loss_bbox: 0.0305  d3.loss_iou: 0.2726  d4.loss_cls: 0.0832  d4.loss_bbox: 0.0303  d4.loss_iou: 0.2710  enc_loss_cls: 0.1254  enc_loss_bbox: 0.0365  enc_loss_iou: 0.3094  dn_loss_cls: 0.0058  dn_loss_bbox: 0.0266  dn_loss_iou: 0.2259  d0.dn_loss_cls: 0.0252  d0.dn_loss_bbox: 0.0389  d0.dn_loss_iou: 0.3059  d1.dn_loss_cls: 0.0091  d1.dn_loss_bbox: 0.0289  d1.dn_loss_iou: 0.2407  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0271  d2.dn_loss_iou: 0.2292  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0267  d3.dn_loss_iou: 0.2266  d4.dn_loss_cls: 0.0053  d4.dn_loss_bbox: 0.0266  d4.dn_loss_iou: 0.2257  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:51:40 - mmengine - INFO - Epoch(train) [4][1500/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:31:25  time: 2.1328  data_time: 0.0170  memory: 17559  grad_norm: 52.4335  loss: 4.3413  loss_cls: 0.0779  loss_bbox: 0.0297  loss_iou: 0.2560  d0.loss_cls: 0.1437  d0.loss_bbox: 0.0309  d0.loss_iou: 0.2646  d1.loss_cls: 0.0963  d1.loss_bbox: 0.0300  d1.loss_iou: 0.2576  d2.loss_cls: 0.0884  d2.loss_bbox: 0.0297  d2.loss_iou: 0.2556  d3.loss_cls: 0.0814  d3.loss_bbox: 0.0297  d3.loss_iou: 0.2561  d4.loss_cls: 0.0782  d4.loss_bbox: 0.0297  d4.loss_iou: 0.2558  enc_loss_cls: 0.1368  enc_loss_bbox: 0.0337  enc_loss_iou: 0.2843  dn_loss_cls: 0.0046  dn_loss_bbox: 0.0251  dn_loss_iou: 0.2158  d0.dn_loss_cls: 0.0242  d0.dn_loss_bbox: 0.0348  d0.dn_loss_iou: 0.2890  d1.dn_loss_cls: 0.0076  d1.dn_loss_bbox: 0.0264  d1.dn_loss_iou: 0.2266  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0253  d2.dn_loss_iou: 0.2172  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0251  d3.dn_loss_iou: 0.2157  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0251  d4.dn_loss_iou: 0.2156  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:53:27 - mmengine - INFO - Epoch(train) [4][1550/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:29:38  time: 2.1308  data_time: 0.0171  memory: 17586  grad_norm: 47.6737  loss: 4.7583  loss_cls: 0.1066  loss_bbox: 0.0298  loss_iou: 0.2768  d0.loss_cls: 0.1595  d0.loss_bbox: 0.0315  d0.loss_iou: 0.2919  d1.loss_cls: 0.1183  d1.loss_bbox: 0.0301  d1.loss_iou: 0.2821  d2.loss_cls: 0.1104  d2.loss_bbox: 0.0295  d2.loss_iou: 0.2763  d3.loss_cls: 0.1074  d3.loss_bbox: 0.0299  d3.loss_iou: 0.2761  d4.loss_cls: 0.1038  d4.loss_bbox: 0.0298  d4.loss_iou: 0.2765  enc_loss_cls: 0.1524  enc_loss_bbox: 0.0341  enc_loss_iou: 0.3080  dn_loss_cls: 0.0149  dn_loss_bbox: 0.0250  dn_loss_iou: 0.2216  d0.dn_loss_cls: 0.0345  d0.dn_loss_bbox: 0.0358  d0.dn_loss_iou: 0.3022  d1.dn_loss_cls: 0.0144  d1.dn_loss_bbox: 0.0267  d1.dn_loss_iou: 0.2358  d2.dn_loss_cls: 0.0145  d2.dn_loss_bbox: 0.0254  d2.dn_loss_iou: 0.2240  d3.dn_loss_cls: 0.0135  d3.dn_loss_bbox: 0.0251  d3.dn_loss_iou: 0.2220  d4.dn_loss_cls: 0.0138  d4.dn_loss_bbox: 0.0250  d4.dn_loss_iou: 0.2213  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:55:14 - mmengine - INFO - Epoch(train) [4][1600/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:27:52  time: 2.1468  data_time: 0.0181  memory: 17574  grad_norm: 48.5865  loss: 4.4211  loss_cls: 0.0706  loss_bbox: 0.0302  loss_iou: 0.2600  d0.loss_cls: 0.1266  d0.loss_bbox: 0.0321  d0.loss_iou: 0.2696  d1.loss_cls: 0.0866  d1.loss_bbox: 0.0315  d1.loss_iou: 0.2652  d2.loss_cls: 0.0801  d2.loss_bbox: 0.0303  d2.loss_iou: 0.2599  d3.loss_cls: 0.0756  d3.loss_bbox: 0.0301  d3.loss_iou: 0.2594  d4.loss_cls: 0.0732  d4.loss_bbox: 0.0300  d4.loss_iou: 0.2588  enc_loss_cls: 0.1260  enc_loss_bbox: 0.0350  enc_loss_iou: 0.2851  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0279  dn_loss_iou: 0.2302  d0.dn_loss_cls: 0.0231  d0.dn_loss_bbox: 0.0392  d0.dn_loss_iou: 0.3112  d1.dn_loss_cls: 0.0063  d1.dn_loss_bbox: 0.0296  d1.dn_loss_iou: 0.2432  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0283  d2.dn_loss_iou: 0.2328  d3.dn_loss_cls: 0.0038  d3.dn_loss_bbox: 0.0279  d3.dn_loss_iou: 0.2304  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0279  d4.dn_loss_iou: 0.2299  loss_num: 0.0003  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:57:00 - mmengine - INFO - Epoch(train) [4][1650/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:26:05  time: 2.1248  data_time: 0.0177  memory: 17568  grad_norm: 40.2357  loss: 4.4310  loss_cls: 0.0685  loss_bbox: 0.0352  loss_iou: 0.2771  d0.loss_cls: 0.1218  d0.loss_bbox: 0.0357  d0.loss_iou: 0.2873  d1.loss_cls: 0.0825  d1.loss_bbox: 0.0347  d1.loss_iou: 0.2798  d2.loss_cls: 0.0742  d2.loss_bbox: 0.0346  d2.loss_iou: 0.2777  d3.loss_cls: 0.0699  d3.loss_bbox: 0.0348  d3.loss_iou: 0.2767  d4.loss_cls: 0.0689  d4.loss_bbox: 0.0352  d4.loss_iou: 0.2770  enc_loss_cls: 0.1208  enc_loss_bbox: 0.0387  enc_loss_iou: 0.3062  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0267  dn_loss_iou: 0.2151  d0.dn_loss_cls: 0.0229  d0.dn_loss_bbox: 0.0371  d0.dn_loss_iou: 0.2846  d1.dn_loss_cls: 0.0064  d1.dn_loss_bbox: 0.0281  d1.dn_loss_iou: 0.2260  d2.dn_loss_cls: 0.0045  d2.dn_loss_bbox: 0.0271  d2.dn_loss_iou: 0.2175  d3.dn_loss_cls: 0.0041  d3.dn_loss_bbox: 0.0268  d3.dn_loss_iou: 0.2154  d4.dn_loss_cls: 0.0041  d4.dn_loss_bbox: 0.0267  d4.dn_loss_iou: 0.2149  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 06:58:46 - mmengine - INFO - Epoch(train) [4][1700/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:24:18  time: 2.1198  data_time: 0.0179  memory: 17573  grad_norm: 38.1768  loss: 5.0854  loss_cls: 0.0943  loss_bbox: 0.0351  loss_iou: 0.3173  d0.loss_cls: 0.1611  d0.loss_bbox: 0.0382  d0.loss_iou: 0.3358  d1.loss_cls: 0.1218  d1.loss_bbox: 0.0361  d1.loss_iou: 0.3224  d2.loss_cls: 0.1075  d2.loss_bbox: 0.0355  d2.loss_iou: 0.3186  d3.loss_cls: 0.1002  d3.loss_bbox: 0.0351  d3.loss_iou: 0.3172  d4.loss_cls: 0.0950  d4.loss_bbox: 0.0351  d4.loss_iou: 0.3175  enc_loss_cls: 0.1595  enc_loss_bbox: 0.0417  enc_loss_iou: 0.3570  dn_loss_cls: 0.0069  dn_loss_bbox: 0.0274  dn_loss_iou: 0.2274  d0.dn_loss_cls: 0.0310  d0.dn_loss_bbox: 0.0377  d0.dn_loss_iou: 0.2980  d1.dn_loss_cls: 0.0114  d1.dn_loss_bbox: 0.0292  d1.dn_loss_iou: 0.2414  d2.dn_loss_cls: 0.0085  d2.dn_loss_bbox: 0.0278  d2.dn_loss_iou: 0.2307  d3.dn_loss_cls: 0.0072  d3.dn_loss_bbox: 0.0274  d3.dn_loss_iou: 0.2278  d4.dn_loss_cls: 0.0070  d4.dn_loss_bbox: 0.0274  d4.dn_loss_iou: 0.2273  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 07:00:34 - mmengine - INFO - Epoch(train) [4][1750/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:22:31  time: 2.1496  data_time: 0.0176  memory: 17574  grad_norm: 49.2375  loss: 4.8689  loss_cls: 0.0962  loss_bbox: 0.0321  loss_iou: 0.2902  d0.loss_cls: 0.1479  d0.loss_bbox: 0.0361  d0.loss_iou: 0.3173  d1.loss_cls: 0.1146  d1.loss_bbox: 0.0333  d1.loss_iou: 0.2998  d2.loss_cls: 0.1034  d2.loss_bbox: 0.0326  d2.loss_iou: 0.2946  d3.loss_cls: 0.0987  d3.loss_bbox: 0.0323  d3.loss_iou: 0.2916  d4.loss_cls: 0.0976  d4.loss_bbox: 0.0321  d4.loss_iou: 0.2905  enc_loss_cls: 0.1426  enc_loss_bbox: 0.0395  enc_loss_iou: 0.3408  dn_loss_cls: 0.0046  dn_loss_bbox: 0.0263  dn_loss_iou: 0.2318  d0.dn_loss_cls: 0.0253  d0.dn_loss_bbox: 0.0365  d0.dn_loss_iou: 0.3073  d1.dn_loss_cls: 0.0078  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2443  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0266  d2.dn_loss_iou: 0.2339  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0264  d3.dn_loss_iou: 0.2321  d4.dn_loss_cls: 0.0046  d4.dn_loss_bbox: 0.0263  d4.dn_loss_iou: 0.2315  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 07:02:21 - mmengine - INFO - Epoch(train) [4][1800/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:20:45  time: 2.1337  data_time: 0.0221  memory: 17567  grad_norm: 75.4589  loss: 4.2056  loss_cls: 0.0700  loss_bbox: 0.0282  loss_iou: 0.2414  d0.loss_cls: 0.1246  d0.loss_bbox: 0.0295  d0.loss_iou: 0.2509  d1.loss_cls: 0.0807  d1.loss_bbox: 0.0290  d1.loss_iou: 0.2461  d2.loss_cls: 0.0740  d2.loss_bbox: 0.0288  d2.loss_iou: 0.2441  d3.loss_cls: 0.0744  d3.loss_bbox: 0.0282  d3.loss_iou: 0.2411  d4.loss_cls: 0.0709  d4.loss_bbox: 0.0282  d4.loss_iou: 0.2412  enc_loss_cls: 0.1219  enc_loss_bbox: 0.0324  enc_loss_iou: 0.2677  dn_loss_cls: 0.0123  dn_loss_bbox: 0.0266  dn_loss_iou: 0.2163  d0.dn_loss_cls: 0.0347  d0.dn_loss_bbox: 0.0369  d0.dn_loss_iou: 0.2887  d1.dn_loss_cls: 0.0148  d1.dn_loss_bbox: 0.0279  d1.dn_loss_iou: 0.2261  d2.dn_loss_cls: 0.0114  d2.dn_loss_bbox: 0.0269  d2.dn_loss_iou: 0.2183  d3.dn_loss_cls: 0.0115  d3.dn_loss_bbox: 0.0267  d3.dn_loss_iou: 0.2167  d4.dn_loss_cls: 0.0117  d4.dn_loss_bbox: 0.0266  d4.dn_loss_iou: 0.2162  loss_num: 0.0003  d0.loss_num: 0.0006  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 07:04:07 - mmengine - INFO - Epoch(train) [4][1850/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:18:58  time: 2.1321  data_time: 0.0179  memory: 17573  grad_norm: 35.6080  loss: 4.3786  loss_cls: 0.0766  loss_bbox: 0.0280  loss_iou: 0.2612  d0.loss_cls: 0.1384  d0.loss_bbox: 0.0297  d0.loss_iou: 0.2738  d1.loss_cls: 0.0902  d1.loss_bbox: 0.0284  d1.loss_iou: 0.2661  d2.loss_cls: 0.0837  d2.loss_bbox: 0.0282  d2.loss_iou: 0.2624  d3.loss_cls: 0.0764  d3.loss_bbox: 0.0281  d3.loss_iou: 0.2625  d4.loss_cls: 0.0776  d4.loss_bbox: 0.0280  d4.loss_iou: 0.2614  enc_loss_cls: 0.1232  enc_loss_bbox: 0.0324  enc_loss_iou: 0.2932  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0254  dn_loss_iou: 0.2188  d0.dn_loss_cls: 0.0269  d0.dn_loss_bbox: 0.0363  d0.dn_loss_iou: 0.2960  d1.dn_loss_cls: 0.0082  d1.dn_loss_bbox: 0.0270  d1.dn_loss_iou: 0.2325  d2.dn_loss_cls: 0.0056  d2.dn_loss_bbox: 0.0257  d2.dn_loss_iou: 0.2221  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0255  d3.dn_loss_iou: 0.2195  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0254  d4.dn_loss_iou: 0.2186  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 07:05:44 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 07:05:55 - mmengine - INFO - Epoch(train) [4][1900/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:17:11  time: 2.1496  data_time: 0.0178  memory: 17580  grad_norm: 39.9529  loss: 4.7364  loss_cls: 0.0873  loss_bbox: 0.0315  loss_iou: 0.2811  d0.loss_cls: 0.1583  d0.loss_bbox: 0.0324  d0.loss_iou: 0.2942  d1.loss_cls: 0.1044  d1.loss_bbox: 0.0328  d1.loss_iou: 0.2899  d2.loss_cls: 0.0959  d2.loss_bbox: 0.0320  d2.loss_iou: 0.2853  d3.loss_cls: 0.0907  d3.loss_bbox: 0.0315  d3.loss_iou: 0.2816  d4.loss_cls: 0.0879  d4.loss_bbox: 0.0315  d4.loss_iou: 0.2810  enc_loss_cls: 0.1459  enc_loss_bbox: 0.0370  enc_loss_iou: 0.3175  dn_loss_cls: 0.0039  dn_loss_bbox: 0.0259  dn_loss_iou: 0.2331  d0.dn_loss_cls: 0.0234  d0.dn_loss_bbox: 0.0361  d0.dn_loss_iou: 0.3086  d1.dn_loss_cls: 0.0081  d1.dn_loss_bbox: 0.0272  d1.dn_loss_iou: 0.2449  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0262  d2.dn_loss_iou: 0.2360  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0259  d3.dn_loss_iou: 0.2333  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0259  d4.dn_loss_iou: 0.2328  loss_num: 0.0003  d0.loss_num: 0.0005  d1.loss_num: 0.0004  d2.loss_num: 0.0004  d3.loss_num: 0.0003  d4.loss_num: 0.0003
2025/10/29 07:07:41 - mmengine - INFO - Epoch(train) [4][1950/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:15:25  time: 2.1348  data_time: 0.0174  memory: 17568  grad_norm: 42.8598  loss: 5.1517  loss_cls: 0.1060  loss_bbox: 0.0314  loss_iou: 0.3274  d0.loss_cls: 0.1622  d0.loss_bbox: 0.0337  d0.loss_iou: 0.3451  d1.loss_cls: 0.1239  d1.loss_bbox: 0.0321  d1.loss_iou: 0.3348  d2.loss_cls: 0.1141  d2.loss_bbox: 0.0313  d2.loss_iou: 0.3282  d3.loss_cls: 0.1074  d3.loss_bbox: 0.0317  d3.loss_iou: 0.3283  d4.loss_cls: 0.1072  d4.loss_bbox: 0.0314  d4.loss_iou: 0.3271  enc_loss_cls: 0.1488  enc_loss_bbox: 0.0366  enc_loss_iou: 0.3714  dn_loss_cls: 0.0055  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2301  d0.dn_loss_cls: 0.0260  d0.dn_loss_bbox: 0.0347  d0.dn_loss_iou: 0.3067  d1.dn_loss_cls: 0.0098  d1.dn_loss_bbox: 0.0256  d1.dn_loss_iou: 0.2423  d2.dn_loss_cls: 0.0066  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2337  d3.dn_loss_cls: 0.0059  d3.dn_loss_bbox: 0.0242  d3.dn_loss_iou: 0.2305  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0241  d4.dn_loss_iou: 0.2299  loss_num: 0.0002  d0.loss_num: 0.0005  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0002
2025/10/29 07:09:28 - mmengine - INFO - Epoch(train) [4][2000/2035]  base_lr: 1.0000e-04 lr: 2.0000e-04  eta: 1:13:38  time: 2.1370  data_time: 0.0186  memory: 17581  grad_norm: 35.1435  loss: 4.2389  loss_cls: 0.0606  loss_bbox: 0.0301  loss_iou: 0.2536  d0.loss_cls: 0.1234  d0.loss_bbox: 0.0303  d0.loss_iou: 0.2639  d1.loss_cls: 0.0767  d1.loss_bbox: 0.0305  d1.loss_iou: 0.2573  d2.loss_cls: 0.0713  d2.loss_bbox: 0.0303  d2.loss_iou: 0.2556  d3.loss_cls: 0.0642  d3.loss_bbox: 0.0301  d3.loss_iou: 0.2541  d4.loss_cls: 0.0611  d4.loss_bbox: 0.0300  d4.loss_iou: 0.2536  enc_loss_cls: 0.1171  enc_loss_bbox: 0.0343  enc_loss_iou: 0.2877  dn_loss_cls: 0.0041  dn_loss_bbox: 0.0249  dn_loss_iou: 0.2199  d0.dn_loss_cls: 0.0269  d0.dn_loss_bbox: 0.0342  d0.dn_loss_iou: 0.2946  d1.dn_loss_cls: 0.0068  d1.dn_loss_bbox: 0.0262  d1.dn_loss_iou: 0.2324  d2.dn_loss_cls: 0.0049  d2.dn_loss_bbox: 0.0252  d2.dn_loss_iou: 0.2231  d3.dn_loss_cls: 0.0043  d3.dn_loss_bbox: 0.0249  d3.dn_loss_iou: 0.2204  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0249  d4.dn_loss_iou: 0.2198  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:10:43 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 07:10:43 - mmengine - INFO - Saving checkpoint at 4 epochs
2025/10/29 07:10:56 - mmengine - INFO - Epoch(val) [4][ 50/429]    eta: 0:00:39  time: 0.1030  data_time: 0.0033  memory: 17568  
2025/10/29 07:11:01 - mmengine - INFO - Epoch(val) [4][100/429]    eta: 0:00:33  time: 0.1000  data_time: 0.0025  memory: 4269  
2025/10/29 07:11:06 - mmengine - INFO - Epoch(val) [4][150/429]    eta: 0:00:28  time: 0.1000  data_time: 0.0025  memory: 4269  
2025/10/29 07:11:11 - mmengine - INFO - Epoch(val) [4][200/429]    eta: 0:00:23  time: 0.1000  data_time: 0.0026  memory: 4269  
2025/10/29 07:11:16 - mmengine - INFO - Epoch(val) [4][250/429]    eta: 0:00:18  time: 0.1003  data_time: 0.0025  memory: 4269  
2025/10/29 07:11:21 - mmengine - INFO - Epoch(val) [4][300/429]    eta: 0:00:12  time: 0.1001  data_time: 0.0025  memory: 4269  
2025/10/29 07:11:26 - mmengine - INFO - Epoch(val) [4][350/429]    eta: 0:00:07  time: 0.1002  data_time: 0.0024  memory: 4269  
2025/10/29 07:11:31 - mmengine - INFO - Epoch(val) [4][400/429]    eta: 0:00:02  time: 0.1008  data_time: 0.0026  memory: 4269  
2025/10/29 07:11:36 - mmengine - INFO - {'instance_F1_score': 0.6613867617748872, 'instance_acc': 0.498913380937597, 'image_F1_score': 0.5254010695187166, 'image_acc': 0.3793706293706294}
2025/10/29 07:11:36 - mmengine - INFO - Epoch(val) [4][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.6614  grefcoco_val/refdrone/instance_acc: 0.4989  grefcoco_val/refdrone/image_F1_score: 0.5254  grefcoco_val/refdrone/image_acc: 0.3794  data_time: 0.0026  time: 0.1006
2025/10/29 07:13:24 - mmengine - INFO - Epoch(train) [5][  50/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 1:10:37  time: 2.1471  data_time: 0.0175  memory: 17585  grad_norm: 33.0661  loss: 4.3591  loss_cls: 0.0757  loss_bbox: 0.0270  loss_iou: 0.2635  d0.loss_cls: 0.1283  d0.loss_bbox: 0.0290  d0.loss_iou: 0.2798  d1.loss_cls: 0.0890  d1.loss_bbox: 0.0282  d1.loss_iou: 0.2729  d2.loss_cls: 0.0826  d2.loss_bbox: 0.0279  d2.loss_iou: 0.2671  d3.loss_cls: 0.0793  d3.loss_bbox: 0.0273  d3.loss_iou: 0.2635  d4.loss_cls: 0.0765  d4.loss_bbox: 0.0270  d4.loss_iou: 0.2627  enc_loss_cls: 0.1272  enc_loss_bbox: 0.0317  enc_loss_iou: 0.3015  dn_loss_cls: 0.0035  dn_loss_bbox: 0.0232  dn_loss_iou: 0.2198  d0.dn_loss_cls: 0.0222  d0.dn_loss_bbox: 0.0315  d0.dn_loss_iou: 0.2854  d1.dn_loss_cls: 0.0070  d1.dn_loss_bbox: 0.0243  d1.dn_loss_iou: 0.2298  d2.dn_loss_cls: 0.0042  d2.dn_loss_bbox: 0.0236  d2.dn_loss_iou: 0.2222  d3.dn_loss_cls: 0.0037  d3.dn_loss_bbox: 0.0233  d3.dn_loss_iou: 0.2198  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0232  d4.dn_loss_iou: 0.2195  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:15:11 - mmengine - INFO - Epoch(train) [5][ 100/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 1:08:50  time: 2.1538  data_time: 0.0169  memory: 17586  grad_norm: 28.5129  loss: 4.3389  loss_cls: 0.0685  loss_bbox: 0.0291  loss_iou: 0.2772  d0.loss_cls: 0.1164  d0.loss_bbox: 0.0306  d0.loss_iou: 0.2878  d1.loss_cls: 0.0809  d1.loss_bbox: 0.0293  d1.loss_iou: 0.2787  d2.loss_cls: 0.0773  d2.loss_bbox: 0.0288  d2.loss_iou: 0.2748  d3.loss_cls: 0.0698  d3.loss_bbox: 0.0294  d3.loss_iou: 0.2800  d4.loss_cls: 0.0681  d4.loss_bbox: 0.0292  d4.loss_iou: 0.2784  enc_loss_cls: 0.1169  enc_loss_bbox: 0.0329  enc_loss_iou: 0.3008  dn_loss_cls: 0.0031  dn_loss_bbox: 0.0237  dn_loss_iou: 0.2142  d0.dn_loss_cls: 0.0188  d0.dn_loss_bbox: 0.0311  d0.dn_loss_iou: 0.2771  d1.dn_loss_cls: 0.0062  d1.dn_loss_bbox: 0.0249  d1.dn_loss_iou: 0.2256  d2.dn_loss_cls: 0.0040  d2.dn_loss_bbox: 0.0240  d2.dn_loss_iou: 0.2171  d3.dn_loss_cls: 0.0033  d3.dn_loss_bbox: 0.0239  d3.dn_loss_iou: 0.2146  d4.dn_loss_cls: 0.0030  d4.dn_loss_bbox: 0.0237  d4.dn_loss_iou: 0.2141  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:16:58 - mmengine - INFO - Epoch(train) [5][ 150/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 1:07:03  time: 2.1330  data_time: 0.0173  memory: 17580  grad_norm: 27.5922  loss: 4.1179  loss_cls: 0.0517  loss_bbox: 0.0293  loss_iou: 0.2531  d0.loss_cls: 0.0974  d0.loss_bbox: 0.0308  d0.loss_iou: 0.2641  d1.loss_cls: 0.0626  d1.loss_bbox: 0.0301  d1.loss_iou: 0.2611  d2.loss_cls: 0.0589  d2.loss_bbox: 0.0294  d2.loss_iou: 0.2545  d3.loss_cls: 0.0559  d3.loss_bbox: 0.0293  d3.loss_iou: 0.2528  d4.loss_cls: 0.0548  d4.loss_bbox: 0.0292  d4.loss_iou: 0.2523  enc_loss_cls: 0.1160  enc_loss_bbox: 0.0332  enc_loss_iou: 0.2793  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0252  dn_loss_iou: 0.2170  d0.dn_loss_cls: 0.0254  d0.dn_loss_bbox: 0.0334  d0.dn_loss_iou: 0.2781  d1.dn_loss_cls: 0.0075  d1.dn_loss_bbox: 0.0266  d1.dn_loss_iou: 0.2288  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0255  d2.dn_loss_iou: 0.2199  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0253  d3.dn_loss_iou: 0.2177  d4.dn_loss_cls: 0.0045  d4.dn_loss_bbox: 0.0252  d4.dn_loss_iou: 0.2169  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:18:44 - mmengine - INFO - Epoch(train) [5][ 200/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 1:05:16  time: 2.1124  data_time: 0.0170  memory: 17574  grad_norm: 28.5315  loss: 4.2552  loss_cls: 0.0521  loss_bbox: 0.0308  loss_iou: 0.2893  d0.loss_cls: 0.1067  d0.loss_bbox: 0.0323  d0.loss_iou: 0.2983  d1.loss_cls: 0.0635  d1.loss_bbox: 0.0315  d1.loss_iou: 0.2939  d2.loss_cls: 0.0592  d2.loss_bbox: 0.0306  d2.loss_iou: 0.2868  d3.loss_cls: 0.0566  d3.loss_bbox: 0.0306  d3.loss_iou: 0.2875  d4.loss_cls: 0.0517  d4.loss_bbox: 0.0309  d4.loss_iou: 0.2896  enc_loss_cls: 0.1011  enc_loss_bbox: 0.0345  enc_loss_iou: 0.3153  dn_loss_cls: 0.0032  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2017  d0.dn_loss_cls: 0.0212  d0.dn_loss_bbox: 0.0321  d0.dn_loss_iou: 0.2620  d1.dn_loss_cls: 0.0063  d1.dn_loss_bbox: 0.0255  d1.dn_loss_iou: 0.2120  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2047  d3.dn_loss_cls: 0.0035  d3.dn_loss_bbox: 0.0243  d3.dn_loss_iou: 0.2026  d4.dn_loss_cls: 0.0032  d4.dn_loss_bbox: 0.0241  d4.dn_loss_iou: 0.2017  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:20:31 - mmengine - INFO - Epoch(train) [5][ 250/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 1:03:30  time: 2.1382  data_time: 0.0171  memory: 17580  grad_norm: 31.9660  loss: 4.1265  loss_cls: 0.0478  loss_bbox: 0.0275  loss_iou: 0.2514  d0.loss_cls: 0.1162  d0.loss_bbox: 0.0290  d0.loss_iou: 0.2612  d1.loss_cls: 0.0655  d1.loss_bbox: 0.0281  d1.loss_iou: 0.2561  d2.loss_cls: 0.0554  d2.loss_bbox: 0.0277  d2.loss_iou: 0.2542  d3.loss_cls: 0.0513  d3.loss_bbox: 0.0275  d3.loss_iou: 0.2514  d4.loss_cls: 0.0480  d4.loss_bbox: 0.0275  d4.loss_iou: 0.2513  enc_loss_cls: 0.1109  enc_loss_bbox: 0.0312  enc_loss_iou: 0.2766  dn_loss_cls: 0.0043  dn_loss_bbox: 0.0256  dn_loss_iou: 0.2226  d0.dn_loss_cls: 0.0244  d0.dn_loss_bbox: 0.0343  d0.dn_loss_iou: 0.2865  d1.dn_loss_cls: 0.0080  d1.dn_loss_bbox: 0.0271  d1.dn_loss_iou: 0.2342  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0260  d2.dn_loss_iou: 0.2257  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2230  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0256  d4.dn_loss_iou: 0.2225  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:22:17 - mmengine - INFO - Epoch(train) [5][ 300/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 1:01:43  time: 2.1309  data_time: 0.0188  memory: 17595  grad_norm: 30.4033  loss: 4.3755  loss_cls: 0.0699  loss_bbox: 0.0308  loss_iou: 0.2760  d0.loss_cls: 0.1236  d0.loss_bbox: 0.0314  d0.loss_iou: 0.2842  d1.loss_cls: 0.0856  d1.loss_bbox: 0.0311  d1.loss_iou: 0.2809  d2.loss_cls: 0.0748  d2.loss_bbox: 0.0309  d2.loss_iou: 0.2793  d3.loss_cls: 0.0725  d3.loss_bbox: 0.0309  d3.loss_iou: 0.2765  d4.loss_cls: 0.0697  d4.loss_bbox: 0.0307  d4.loss_iou: 0.2759  enc_loss_cls: 0.1261  enc_loss_bbox: 0.0348  enc_loss_iou: 0.3070  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0236  dn_loss_iou: 0.2134  d0.dn_loss_cls: 0.0221  d0.dn_loss_bbox: 0.0318  d0.dn_loss_iou: 0.2745  d1.dn_loss_cls: 0.0067  d1.dn_loss_bbox: 0.0248  d1.dn_loss_iou: 0.2241  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0239  d2.dn_loss_iou: 0.2160  d3.dn_loss_cls: 0.0038  d3.dn_loss_bbox: 0.0237  d3.dn_loss_iou: 0.2141  d4.dn_loss_cls: 0.0037  d4.dn_loss_bbox: 0.0237  d4.dn_loss_iou: 0.2134  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0003  d2.loss_num: 0.0003  d3.loss_num: 0.0003  d4.loss_num: 0.0002
2025/10/29 07:24:03 - mmengine - INFO - Epoch(train) [5][ 350/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:59:56  time: 2.1228  data_time: 0.0174  memory: 17580  grad_norm: 25.8809  loss: 3.8443  loss_cls: 0.0512  loss_bbox: 0.0271  loss_iou: 0.2396  d0.loss_cls: 0.1004  d0.loss_bbox: 0.0287  d0.loss_iou: 0.2474  d1.loss_cls: 0.0621  d1.loss_bbox: 0.0279  d1.loss_iou: 0.2453  d2.loss_cls: 0.0563  d2.loss_bbox: 0.0275  d2.loss_iou: 0.2397  d3.loss_cls: 0.0534  d3.loss_bbox: 0.0270  d3.loss_iou: 0.2381  d4.loss_cls: 0.0521  d4.loss_bbox: 0.0271  d4.loss_iou: 0.2396  enc_loss_cls: 0.0985  enc_loss_bbox: 0.0311  enc_loss_iou: 0.2674  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0225  dn_loss_iou: 0.2001  d0.dn_loss_cls: 0.0208  d0.dn_loss_bbox: 0.0298  d0.dn_loss_iou: 0.2565  d1.dn_loss_cls: 0.0066  d1.dn_loss_bbox: 0.0237  d1.dn_loss_iou: 0.2091  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0229  d2.dn_loss_iou: 0.2020  d3.dn_loss_cls: 0.0039  d3.dn_loss_bbox: 0.0226  d3.dn_loss_iou: 0.2003  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0225  d4.dn_loss_iou: 0.2000  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:25:50 - mmengine - INFO - Epoch(train) [5][ 400/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:58:09  time: 2.1425  data_time: 0.0172  memory: 17580  grad_norm: 29.5322  loss: 4.4040  loss_cls: 0.0493  loss_bbox: 0.0319  loss_iou: 0.2865  d0.loss_cls: 0.1072  d0.loss_bbox: 0.0327  d0.loss_iou: 0.2927  d1.loss_cls: 0.0614  d1.loss_bbox: 0.0321  d1.loss_iou: 0.2898  d2.loss_cls: 0.0531  d2.loss_bbox: 0.0319  d2.loss_iou: 0.2879  d3.loss_cls: 0.0489  d3.loss_bbox: 0.0318  d3.loss_iou: 0.2871  d4.loss_cls: 0.0493  d4.loss_bbox: 0.0319  d4.loss_iou: 0.2865  enc_loss_cls: 0.1126  enc_loss_bbox: 0.0354  enc_loss_iou: 0.3110  dn_loss_cls: 0.0050  dn_loss_bbox: 0.0250  dn_loss_iou: 0.2243  d0.dn_loss_cls: 0.0298  d0.dn_loss_bbox: 0.0341  d0.dn_loss_iou: 0.2932  d1.dn_loss_cls: 0.0097  d1.dn_loss_bbox: 0.0263  d1.dn_loss_iou: 0.2363  d2.dn_loss_cls: 0.0063  d2.dn_loss_bbox: 0.0253  d2.dn_loss_iou: 0.2268  d3.dn_loss_cls: 0.0054  d3.dn_loss_bbox: 0.0251  d3.dn_loss_iou: 0.2249  d4.dn_loss_cls: 0.0051  d4.dn_loss_bbox: 0.0250  d4.dn_loss_iou: 0.2243  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:27:36 - mmengine - INFO - Epoch(train) [5][ 450/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:56:23  time: 2.1181  data_time: 0.0170  memory: 17574  grad_norm: 28.9139  loss: 4.0604  loss_cls: 0.0604  loss_bbox: 0.0271  loss_iou: 0.2438  d0.loss_cls: 0.0970  d0.loss_bbox: 0.0287  d0.loss_iou: 0.2575  d1.loss_cls: 0.0655  d1.loss_bbox: 0.0285  d1.loss_iou: 0.2564  d2.loss_cls: 0.0594  d2.loss_bbox: 0.0276  d2.loss_iou: 0.2505  d3.loss_cls: 0.0568  d3.loss_bbox: 0.0274  d3.loss_iou: 0.2480  d4.loss_cls: 0.0607  d4.loss_bbox: 0.0271  d4.loss_iou: 0.2438  enc_loss_cls: 0.1011  enc_loss_bbox: 0.0307  enc_loss_iou: 0.2682  dn_loss_cls: 0.0038  dn_loss_bbox: 0.0258  dn_loss_iou: 0.2178  d0.dn_loss_cls: 0.0233  d0.dn_loss_bbox: 0.0343  d0.dn_loss_iou: 0.2787  d1.dn_loss_cls: 0.0069  d1.dn_loss_bbox: 0.0272  d1.dn_loss_iou: 0.2285  d2.dn_loss_cls: 0.0046  d2.dn_loss_bbox: 0.0261  d2.dn_loss_iou: 0.2205  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2184  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0258  d4.dn_loss_iou: 0.2177  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:29:23 - mmengine - INFO - Epoch(train) [5][ 500/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:54:36  time: 2.1417  data_time: 0.0171  memory: 17599  grad_norm: 26.8342  loss: 4.3359  loss_cls: 0.0581  loss_bbox: 0.0328  loss_iou: 0.2792  d0.loss_cls: 0.1088  d0.loss_bbox: 0.0342  d0.loss_iou: 0.2925  d1.loss_cls: 0.0740  d1.loss_bbox: 0.0326  d1.loss_iou: 0.2825  d2.loss_cls: 0.0675  d2.loss_bbox: 0.0323  d2.loss_iou: 0.2802  d3.loss_cls: 0.0624  d3.loss_bbox: 0.0322  d3.loss_iou: 0.2790  d4.loss_cls: 0.0585  d4.loss_bbox: 0.0329  d4.loss_iou: 0.2792  enc_loss_cls: 0.1163  enc_loss_bbox: 0.0363  enc_loss_iou: 0.3079  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0243  dn_loss_iou: 0.2131  d0.dn_loss_cls: 0.0214  d0.dn_loss_bbox: 0.0319  d0.dn_loss_iou: 0.2726  d1.dn_loss_cls: 0.0072  d1.dn_loss_bbox: 0.0256  d1.dn_loss_iou: 0.2244  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2160  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0244  d3.dn_loss_iou: 0.2135  d4.dn_loss_cls: 0.0044  d4.dn_loss_bbox: 0.0243  d4.dn_loss_iou: 0.2131  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:31:10 - mmengine - INFO - Epoch(train) [5][ 550/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:52:49  time: 2.1272  data_time: 0.0175  memory: 17575  grad_norm: 27.9573  loss: 4.6482  loss_cls: 0.0686  loss_bbox: 0.0332  loss_iou: 0.3103  d0.loss_cls: 0.1086  d0.loss_bbox: 0.0344  d0.loss_iou: 0.3202  d1.loss_cls: 0.0836  d1.loss_bbox: 0.0337  d1.loss_iou: 0.3136  d2.loss_cls: 0.0770  d2.loss_bbox: 0.0334  d2.loss_iou: 0.3103  d3.loss_cls: 0.0693  d3.loss_bbox: 0.0333  d3.loss_iou: 0.3104  d4.loss_cls: 0.0688  d4.loss_bbox: 0.0337  d4.loss_iou: 0.3124  enc_loss_cls: 0.1207  enc_loss_bbox: 0.0347  enc_loss_iou: 0.3339  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0235  dn_loss_iou: 0.2214  d0.dn_loss_cls: 0.0231  d0.dn_loss_bbox: 0.0318  d0.dn_loss_iou: 0.2849  d1.dn_loss_cls: 0.0066  d1.dn_loss_bbox: 0.0249  d1.dn_loss_iou: 0.2323  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0238  d2.dn_loss_iou: 0.2245  d3.dn_loss_cls: 0.0038  d3.dn_loss_bbox: 0.0236  d3.dn_loss_iou: 0.2218  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0235  d4.dn_loss_iou: 0.2213  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:32:56 - mmengine - INFO - Epoch(train) [5][ 600/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:51:02  time: 2.1221  data_time: 0.0176  memory: 17585  grad_norm: 24.8414  loss: 4.2198  loss_cls: 0.0489  loss_bbox: 0.0313  loss_iou: 0.2718  d0.loss_cls: 0.1092  d0.loss_bbox: 0.0320  d0.loss_iou: 0.2795  d1.loss_cls: 0.0647  d1.loss_bbox: 0.0317  d1.loss_iou: 0.2784  d2.loss_cls: 0.0582  d2.loss_bbox: 0.0313  d2.loss_iou: 0.2742  d3.loss_cls: 0.0516  d3.loss_bbox: 0.0314  d3.loss_iou: 0.2731  d4.loss_cls: 0.0498  d4.loss_bbox: 0.0313  d4.loss_iou: 0.2722  enc_loss_cls: 0.1064  enc_loss_bbox: 0.0357  enc_loss_iou: 0.3002  dn_loss_cls: 0.0039  dn_loss_bbox: 0.0236  dn_loss_iou: 0.2140  d0.dn_loss_cls: 0.0209  d0.dn_loss_bbox: 0.0316  d0.dn_loss_iou: 0.2751  d1.dn_loss_cls: 0.0062  d1.dn_loss_bbox: 0.0249  d1.dn_loss_iou: 0.2256  d2.dn_loss_cls: 0.0046  d2.dn_loss_bbox: 0.0240  d2.dn_loss_iou: 0.2174  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0236  d3.dn_loss_iou: 0.2146  d4.dn_loss_cls: 0.0039  d4.dn_loss_bbox: 0.0236  d4.dn_loss_iou: 0.2140  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:34:43 - mmengine - INFO - Epoch(train) [5][ 650/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:49:16  time: 2.1473  data_time: 0.0185  memory: 17574  grad_norm: 27.2045  loss: 4.1086  loss_cls: 0.0529  loss_bbox: 0.0293  loss_iou: 0.2574  d0.loss_cls: 0.1067  d0.loss_bbox: 0.0311  d0.loss_iou: 0.2684  d1.loss_cls: 0.0645  d1.loss_bbox: 0.0299  d1.loss_iou: 0.2626  d2.loss_cls: 0.0537  d2.loss_bbox: 0.0297  d2.loss_iou: 0.2602  d3.loss_cls: 0.0533  d3.loss_bbox: 0.0293  d3.loss_iou: 0.2576  d4.loss_cls: 0.0545  d4.loss_bbox: 0.0291  d4.loss_iou: 0.2555  enc_loss_cls: 0.1112  enc_loss_bbox: 0.0329  enc_loss_iou: 0.2842  dn_loss_cls: 0.0032  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2122  d0.dn_loss_cls: 0.0261  d0.dn_loss_bbox: 0.0324  d0.dn_loss_iou: 0.2764  d1.dn_loss_cls: 0.0067  d1.dn_loss_bbox: 0.0255  d1.dn_loss_iou: 0.2227  d2.dn_loss_cls: 0.0040  d2.dn_loss_bbox: 0.0246  d2.dn_loss_iou: 0.2156  d3.dn_loss_cls: 0.0033  d3.dn_loss_bbox: 0.0243  d3.dn_loss_iou: 0.2127  d4.dn_loss_cls: 0.0033  d4.dn_loss_bbox: 0.0241  d4.dn_loss_iou: 0.2121  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:36:30 - mmengine - INFO - Epoch(train) [5][ 700/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:47:29  time: 2.1363  data_time: 0.0172  memory: 17579  grad_norm: 26.6321  loss: 3.9790  loss_cls: 0.0403  loss_bbox: 0.0275  loss_iou: 0.2617  d0.loss_cls: 0.0915  d0.loss_bbox: 0.0288  d0.loss_iou: 0.2716  d1.loss_cls: 0.0566  d1.loss_bbox: 0.0280  d1.loss_iou: 0.2654  d2.loss_cls: 0.0480  d2.loss_bbox: 0.0277  d2.loss_iou: 0.2633  d3.loss_cls: 0.0420  d3.loss_bbox: 0.0276  d3.loss_iou: 0.2622  d4.loss_cls: 0.0408  d4.loss_bbox: 0.0275  d4.loss_iou: 0.2617  enc_loss_cls: 0.0972  enc_loss_bbox: 0.0312  enc_loss_iou: 0.2912  dn_loss_cls: 0.0034  dn_loss_bbox: 0.0227  dn_loss_iou: 0.2032  d0.dn_loss_cls: 0.0214  d0.dn_loss_bbox: 0.0312  d0.dn_loss_iou: 0.2669  d1.dn_loss_cls: 0.0063  d1.dn_loss_bbox: 0.0239  d1.dn_loss_iou: 0.2139  d2.dn_loss_cls: 0.0042  d2.dn_loss_bbox: 0.0230  d2.dn_loss_iou: 0.2062  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0227  d3.dn_loss_iou: 0.2038  d4.dn_loss_cls: 0.0034  d4.dn_loss_bbox: 0.0227  d4.dn_loss_iou: 0.2032  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:38:16 - mmengine - INFO - Epoch(train) [5][ 750/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:45:42  time: 2.1247  data_time: 0.0173  memory: 17591  grad_norm: 25.4332  loss: 3.7502  loss_cls: 0.0385  loss_bbox: 0.0258  loss_iou: 0.2288  d0.loss_cls: 0.0909  d0.loss_bbox: 0.0270  d0.loss_iou: 0.2368  d1.loss_cls: 0.0505  d1.loss_bbox: 0.0261  d1.loss_iou: 0.2317  d2.loss_cls: 0.0427  d2.loss_bbox: 0.0259  d2.loss_iou: 0.2308  d3.loss_cls: 0.0402  d3.loss_bbox: 0.0259  d3.loss_iou: 0.2294  d4.loss_cls: 0.0383  d4.loss_bbox: 0.0258  d4.loss_iou: 0.2291  enc_loss_cls: 0.0929  enc_loss_bbox: 0.0289  enc_loss_iou: 0.2478  dn_loss_cls: 0.0036  dn_loss_bbox: 0.0246  dn_loss_iou: 0.2080  d0.dn_loss_cls: 0.0250  d0.dn_loss_bbox: 0.0341  d0.dn_loss_iou: 0.2746  d1.dn_loss_cls: 0.0067  d1.dn_loss_bbox: 0.0258  d1.dn_loss_iou: 0.2192  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0249  d2.dn_loss_iou: 0.2106  d3.dn_loss_cls: 0.0038  d3.dn_loss_bbox: 0.0246  d3.dn_loss_iou: 0.2087  d4.dn_loss_cls: 0.0037  d4.dn_loss_bbox: 0.0246  d4.dn_loss_iou: 0.2080  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:40:03 - mmengine - INFO - Epoch(train) [5][ 800/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:43:56  time: 2.1427  data_time: 0.0176  memory: 17581  grad_norm: 27.6254  loss: 4.1296  loss_cls: 0.0528  loss_bbox: 0.0314  loss_iou: 0.2515  d0.loss_cls: 0.0837  d0.loss_bbox: 0.0335  d0.loss_iou: 0.2742  d1.loss_cls: 0.0604  d1.loss_bbox: 0.0321  d1.loss_iou: 0.2610  d2.loss_cls: 0.0576  d2.loss_bbox: 0.0320  d2.loss_iou: 0.2585  d3.loss_cls: 0.0535  d3.loss_bbox: 0.0316  d3.loss_iou: 0.2532  d4.loss_cls: 0.0513  d4.loss_bbox: 0.0316  d4.loss_iou: 0.2534  enc_loss_cls: 0.0927  enc_loss_bbox: 0.0359  enc_loss_iou: 0.2882  dn_loss_cls: 0.0044  dn_loss_bbox: 0.0257  dn_loss_iou: 0.2194  d0.dn_loss_cls: 0.0232  d0.dn_loss_bbox: 0.0348  d0.dn_loss_iou: 0.2832  d1.dn_loss_cls: 0.0068  d1.dn_loss_bbox: 0.0272  d1.dn_loss_iou: 0.2304  d2.dn_loss_cls: 0.0053  d2.dn_loss_bbox: 0.0260  d2.dn_loss_iou: 0.2217  d3.dn_loss_cls: 0.0047  d3.dn_loss_bbox: 0.0258  d3.dn_loss_iou: 0.2200  d4.dn_loss_cls: 0.0045  d4.dn_loss_bbox: 0.0257  d4.dn_loss_iou: 0.2194  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:41:50 - mmengine - INFO - Epoch(train) [5][ 850/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:42:09  time: 2.1344  data_time: 0.0173  memory: 17581  grad_norm: 26.2372  loss: 4.0605  loss_cls: 0.0538  loss_bbox: 0.0275  loss_iou: 0.2584  d0.loss_cls: 0.0988  d0.loss_bbox: 0.0294  d0.loss_iou: 0.2740  d1.loss_cls: 0.0652  d1.loss_bbox: 0.0282  d1.loss_iou: 0.2638  d2.loss_cls: 0.0601  d2.loss_bbox: 0.0275  d2.loss_iou: 0.2569  d3.loss_cls: 0.0590  d3.loss_bbox: 0.0275  d3.loss_iou: 0.2582  d4.loss_cls: 0.0573  d4.loss_bbox: 0.0275  d4.loss_iou: 0.2585  enc_loss_cls: 0.1015  enc_loss_bbox: 0.0326  enc_loss_iou: 0.2932  dn_loss_cls: 0.0028  dn_loss_bbox: 0.0237  dn_loss_iou: 0.2064  d0.dn_loss_cls: 0.0185  d0.dn_loss_bbox: 0.0317  d0.dn_loss_iou: 0.2668  d1.dn_loss_cls: 0.0052  d1.dn_loss_bbox: 0.0250  d1.dn_loss_iou: 0.2169  d2.dn_loss_cls: 0.0034  d2.dn_loss_bbox: 0.0241  d2.dn_loss_iou: 0.2096  d3.dn_loss_cls: 0.0028  d3.dn_loss_bbox: 0.0239  d3.dn_loss_iou: 0.2068  d4.dn_loss_cls: 0.0027  d4.dn_loss_bbox: 0.0237  d4.dn_loss_iou: 0.2064  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:42:12 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 07:43:37 - mmengine - INFO - Epoch(train) [5][ 900/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:40:22  time: 2.1396  data_time: 0.0181  memory: 17586  grad_norm: 28.7693  loss: 4.4113  loss_cls: 0.0549  loss_bbox: 0.0331  loss_iou: 0.2779  d0.loss_cls: 0.1056  d0.loss_bbox: 0.0331  d0.loss_iou: 0.2901  d1.loss_cls: 0.0681  d1.loss_bbox: 0.0319  d1.loss_iou: 0.2840  d2.loss_cls: 0.0629  d2.loss_bbox: 0.0334  d2.loss_iou: 0.2797  d3.loss_cls: 0.0561  d3.loss_bbox: 0.0333  d3.loss_iou: 0.2787  d4.loss_cls: 0.0550  d4.loss_bbox: 0.0331  d4.loss_iou: 0.2779  enc_loss_cls: 0.1111  enc_loss_bbox: 0.0356  enc_loss_iou: 0.3093  dn_loss_cls: 0.0034  dn_loss_bbox: 0.0252  dn_loss_iou: 0.2299  d0.dn_loss_cls: 0.0223  d0.dn_loss_bbox: 0.0331  d0.dn_loss_iou: 0.2966  d1.dn_loss_cls: 0.0062  d1.dn_loss_bbox: 0.0264  d1.dn_loss_iou: 0.2425  d2.dn_loss_cls: 0.0040  d2.dn_loss_bbox: 0.0255  d2.dn_loss_iou: 0.2328  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0252  d3.dn_loss_iou: 0.2302  d4.dn_loss_cls: 0.0035  d4.dn_loss_bbox: 0.0252  d4.dn_loss_iou: 0.2298  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:45:24 - mmengine - INFO - Epoch(train) [5][ 950/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:38:35  time: 2.1327  data_time: 0.0173  memory: 17580  grad_norm: 26.6344  loss: 4.1135  loss_cls: 0.0511  loss_bbox: 0.0286  loss_iou: 0.2693  d0.loss_cls: 0.1015  d0.loss_bbox: 0.0301  d0.loss_iou: 0.2815  d1.loss_cls: 0.0672  d1.loss_bbox: 0.0293  d1.loss_iou: 0.2751  d2.loss_cls: 0.0582  d2.loss_bbox: 0.0293  d2.loss_iou: 0.2720  d3.loss_cls: 0.0524  d3.loss_bbox: 0.0287  d3.loss_iou: 0.2704  d4.loss_cls: 0.0507  d4.loss_bbox: 0.0286  d4.loss_iou: 0.2692  enc_loss_cls: 0.1018  enc_loss_bbox: 0.0320  enc_loss_iou: 0.2978  dn_loss_cls: 0.0031  dn_loss_bbox: 0.0234  dn_loss_iou: 0.2042  d0.dn_loss_cls: 0.0201  d0.dn_loss_bbox: 0.0313  d0.dn_loss_iou: 0.2624  d1.dn_loss_cls: 0.0063  d1.dn_loss_bbox: 0.0247  d1.dn_loss_iou: 0.2149  d2.dn_loss_cls: 0.0039  d2.dn_loss_bbox: 0.0238  d2.dn_loss_iou: 0.2073  d3.dn_loss_cls: 0.0032  d3.dn_loss_bbox: 0.0235  d3.dn_loss_iou: 0.2049  d4.dn_loss_cls: 0.0031  d4.dn_loss_bbox: 0.0234  d4.dn_loss_iou: 0.2042  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:47:10 - mmengine - INFO - Epoch(train) [5][1000/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:36:49  time: 2.1327  data_time: 0.0173  memory: 17568  grad_norm: 30.3220  loss: 3.7925  loss_cls: 0.0315  loss_bbox: 0.0312  loss_iou: 0.2389  d0.loss_cls: 0.0765  d0.loss_bbox: 0.0328  d0.loss_iou: 0.2465  d1.loss_cls: 0.0484  d1.loss_bbox: 0.0319  d1.loss_iou: 0.2416  d2.loss_cls: 0.0361  d2.loss_bbox: 0.0316  d2.loss_iou: 0.2402  d3.loss_cls: 0.0340  d3.loss_bbox: 0.0313  d3.loss_iou: 0.2379  d4.loss_cls: 0.0329  d4.loss_bbox: 0.0312  d4.loss_iou: 0.2385  enc_loss_cls: 0.0861  enc_loss_bbox: 0.0346  enc_loss_iou: 0.2624  dn_loss_cls: 0.0026  dn_loss_bbox: 0.0261  dn_loss_iou: 0.2057  d0.dn_loss_cls: 0.0200  d0.dn_loss_bbox: 0.0354  d0.dn_loss_iou: 0.2668  d1.dn_loss_cls: 0.0054  d1.dn_loss_bbox: 0.0278  d1.dn_loss_iou: 0.2170  d2.dn_loss_cls: 0.0033  d2.dn_loss_bbox: 0.0266  d2.dn_loss_iou: 0.2088  d3.dn_loss_cls: 0.0028  d3.dn_loss_bbox: 0.0262  d3.dn_loss_iou: 0.2063  d4.dn_loss_cls: 0.0026  d4.dn_loss_bbox: 0.0261  d4.dn_loss_iou: 0.2056  loss_num: 0.0001  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:48:58 - mmengine - INFO - Epoch(train) [5][1050/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:35:02  time: 2.1421  data_time: 0.0171  memory: 17574  grad_norm: 34.2216  loss: 3.9634  loss_cls: 0.0445  loss_bbox: 0.0283  loss_iou: 0.2548  d0.loss_cls: 0.0863  d0.loss_bbox: 0.0302  d0.loss_iou: 0.2728  d1.loss_cls: 0.0559  d1.loss_bbox: 0.0290  d1.loss_iou: 0.2623  d2.loss_cls: 0.0484  d2.loss_bbox: 0.0290  d2.loss_iou: 0.2604  d3.loss_cls: 0.0466  d3.loss_bbox: 0.0289  d3.loss_iou: 0.2581  d4.loss_cls: 0.0456  d4.loss_bbox: 0.0283  d4.loss_iou: 0.2540  enc_loss_cls: 0.0928  enc_loss_bbox: 0.0331  enc_loss_iou: 0.2915  dn_loss_cls: 0.0039  dn_loss_bbox: 0.0230  dn_loss_iou: 0.2022  d0.dn_loss_cls: 0.0223  d0.dn_loss_bbox: 0.0314  d0.dn_loss_iou: 0.2643  d1.dn_loss_cls: 0.0073  d1.dn_loss_bbox: 0.0242  d1.dn_loss_iou: 0.2118  d2.dn_loss_cls: 0.0045  d2.dn_loss_bbox: 0.0233  d2.dn_loss_iou: 0.2044  d3.dn_loss_cls: 0.0039  d3.dn_loss_bbox: 0.0230  d3.dn_loss_iou: 0.2027  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0230  d4.dn_loss_iou: 0.2021  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:50:44 - mmengine - INFO - Epoch(train) [5][1100/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:33:15  time: 2.1371  data_time: 0.0173  memory: 17581  grad_norm: 27.1613  loss: 3.9890  loss_cls: 0.0418  loss_bbox: 0.0279  loss_iou: 0.2562  d0.loss_cls: 0.0863  d0.loss_bbox: 0.0299  d0.loss_iou: 0.2737  d1.loss_cls: 0.0565  d1.loss_bbox: 0.0289  d1.loss_iou: 0.2629  d2.loss_cls: 0.0461  d2.loss_bbox: 0.0286  d2.loss_iou: 0.2608  d3.loss_cls: 0.0446  d3.loss_bbox: 0.0281  d3.loss_iou: 0.2572  d4.loss_cls: 0.0380  d4.loss_bbox: 0.0283  d4.loss_iou: 0.2577  enc_loss_cls: 0.0923  enc_loss_bbox: 0.0325  enc_loss_iou: 0.2891  dn_loss_cls: 0.0048  dn_loss_bbox: 0.0237  dn_loss_iou: 0.2083  d0.dn_loss_cls: 0.0233  d0.dn_loss_bbox: 0.0319  d0.dn_loss_iou: 0.2653  d1.dn_loss_cls: 0.0071  d1.dn_loss_bbox: 0.0249  d1.dn_loss_iou: 0.2178  d2.dn_loss_cls: 0.0051  d2.dn_loss_bbox: 0.0240  d2.dn_loss_iou: 0.2101  d3.dn_loss_cls: 0.0049  d3.dn_loss_bbox: 0.0238  d3.dn_loss_iou: 0.2084  d4.dn_loss_cls: 0.0048  d4.dn_loss_bbox: 0.0237  d4.dn_loss_iou: 0.2083  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 07:52:31 - mmengine - INFO - Epoch(train) [5][1150/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:31:28  time: 2.1274  data_time: 0.0179  memory: 17586  grad_norm: 25.5303  loss: 3.8851  loss_cls: 0.0372  loss_bbox: 0.0279  loss_iou: 0.2471  d0.loss_cls: 0.0834  d0.loss_bbox: 0.0290  d0.loss_iou: 0.2551  d1.loss_cls: 0.0493  d1.loss_bbox: 0.0284  d1.loss_iou: 0.2513  d2.loss_cls: 0.0429  d2.loss_bbox: 0.0281  d2.loss_iou: 0.2487  d3.loss_cls: 0.0391  d3.loss_bbox: 0.0281  d3.loss_iou: 0.2479  d4.loss_cls: 0.0378  d4.loss_bbox: 0.0279  d4.loss_iou: 0.2471  enc_loss_cls: 0.0931  enc_loss_bbox: 0.0309  enc_loss_iou: 0.2706  dn_loss_cls: 0.0016  dn_loss_bbox: 0.0250  dn_loss_iou: 0.2124  d0.dn_loss_cls: 0.0186  d0.dn_loss_bbox: 0.0326  d0.dn_loss_iou: 0.2700  d1.dn_loss_cls: 0.0048  d1.dn_loss_bbox: 0.0259  d1.dn_loss_iou: 0.2217  d2.dn_loss_cls: 0.0025  d2.dn_loss_bbox: 0.0251  d2.dn_loss_iou: 0.2141  d3.dn_loss_cls: 0.0019  d3.dn_loss_bbox: 0.0251  d3.dn_loss_iou: 0.2127  d4.dn_loss_cls: 0.0017  d4.dn_loss_bbox: 0.0250  d4.dn_loss_iou: 0.2124  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 07:54:18 - mmengine - INFO - Epoch(train) [5][1200/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:29:42  time: 2.1416  data_time: 0.0178  memory: 17591  grad_norm: 26.3665  loss: 3.9479  loss_cls: 0.0430  loss_bbox: 0.0306  loss_iou: 0.2594  d0.loss_cls: 0.0858  d0.loss_bbox: 0.0326  d0.loss_iou: 0.2746  d1.loss_cls: 0.0516  d1.loss_bbox: 0.0316  d1.loss_iou: 0.2672  d2.loss_cls: 0.0443  d2.loss_bbox: 0.0313  d2.loss_iou: 0.2650  d3.loss_cls: 0.0465  d3.loss_bbox: 0.0307  d3.loss_iou: 0.2597  d4.loss_cls: 0.0444  d4.loss_bbox: 0.0306  d4.loss_iou: 0.2596  enc_loss_cls: 0.0967  enc_loss_bbox: 0.0351  enc_loss_iou: 0.2899  dn_loss_cls: 0.0026  dn_loss_bbox: 0.0235  dn_loss_iou: 0.1973  d0.dn_loss_cls: 0.0203  d0.dn_loss_bbox: 0.0313  d0.dn_loss_iou: 0.2507  d1.dn_loss_cls: 0.0048  d1.dn_loss_bbox: 0.0248  d1.dn_loss_iou: 0.2067  d2.dn_loss_cls: 0.0032  d2.dn_loss_bbox: 0.0239  d2.dn_loss_iou: 0.1999  d3.dn_loss_cls: 0.0027  d3.dn_loss_bbox: 0.0236  d3.dn_loss_iou: 0.1978  d4.dn_loss_cls: 0.0027  d4.dn_loss_bbox: 0.0235  d4.dn_loss_iou: 0.1972  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 07:56:05 - mmengine - INFO - Epoch(train) [5][1250/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:27:55  time: 2.1356  data_time: 0.0179  memory: 17574  grad_norm: 28.7878  loss: 3.9604  loss_cls: 0.0379  loss_bbox: 0.0322  loss_iou: 0.2696  d0.loss_cls: 0.0768  d0.loss_bbox: 0.0336  d0.loss_iou: 0.2809  d1.loss_cls: 0.0444  d1.loss_bbox: 0.0331  d1.loss_iou: 0.2758  d2.loss_cls: 0.0373  d2.loss_bbox: 0.0330  d2.loss_iou: 0.2740  d3.loss_cls: 0.0343  d3.loss_bbox: 0.0328  d3.loss_iou: 0.2730  d4.loss_cls: 0.0387  d4.loss_bbox: 0.0321  d4.loss_iou: 0.2691  enc_loss_cls: 0.0848  enc_loss_bbox: 0.0363  enc_loss_iou: 0.2983  dn_loss_cls: 0.0036  dn_loss_bbox: 0.0228  dn_loss_iou: 0.1952  d0.dn_loss_cls: 0.0223  d0.dn_loss_bbox: 0.0309  d0.dn_loss_iou: 0.2534  d1.dn_loss_cls: 0.0064  d1.dn_loss_bbox: 0.0238  d1.dn_loss_iou: 0.2047  d2.dn_loss_cls: 0.0041  d2.dn_loss_bbox: 0.0230  d2.dn_loss_iou: 0.1973  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0228  d3.dn_loss_iou: 0.1958  d4.dn_loss_cls: 0.0036  d4.dn_loss_bbox: 0.0228  d4.dn_loss_iou: 0.1952  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 07:57:51 - mmengine - INFO - Epoch(train) [5][1300/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:26:08  time: 2.1270  data_time: 0.0170  memory: 17581  grad_norm: 27.7693  loss: 3.8200  loss_cls: 0.0296  loss_bbox: 0.0271  loss_iou: 0.2328  d0.loss_cls: 0.0775  d0.loss_bbox: 0.0301  d0.loss_iou: 0.2490  d1.loss_cls: 0.0461  d1.loss_bbox: 0.0277  d1.loss_iou: 0.2374  d2.loss_cls: 0.0376  d2.loss_bbox: 0.0273  d2.loss_iou: 0.2346  d3.loss_cls: 0.0331  d3.loss_bbox: 0.0272  d3.loss_iou: 0.2338  d4.loss_cls: 0.0298  d4.loss_bbox: 0.0272  d4.loss_iou: 0.2334  enc_loss_cls: 0.0782  enc_loss_bbox: 0.0328  enc_loss_iou: 0.2647  dn_loss_cls: 0.0034  dn_loss_bbox: 0.0268  dn_loss_iou: 0.2180  d0.dn_loss_cls: 0.0238  d0.dn_loss_bbox: 0.0362  d0.dn_loss_iou: 0.2820  d1.dn_loss_cls: 0.0066  d1.dn_loss_bbox: 0.0281  d1.dn_loss_iou: 0.2284  d2.dn_loss_cls: 0.0044  d2.dn_loss_bbox: 0.0271  d2.dn_loss_iou: 0.2201  d3.dn_loss_cls: 0.0036  d3.dn_loss_bbox: 0.0269  d3.dn_loss_iou: 0.2186  d4.dn_loss_cls: 0.0034  d4.dn_loss_bbox: 0.0268  d4.dn_loss_iou: 0.2180  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0001  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 07:59:38 - mmengine - INFO - Epoch(train) [5][1350/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:24:22  time: 2.1368  data_time: 0.0176  memory: 17568  grad_norm: 30.9518  loss: 4.3605  loss_cls: 0.0515  loss_bbox: 0.0300  loss_iou: 0.2961  d0.loss_cls: 0.0996  d0.loss_bbox: 0.0313  d0.loss_iou: 0.3050  d1.loss_cls: 0.0639  d1.loss_bbox: 0.0304  d1.loss_iou: 0.2992  d2.loss_cls: 0.0552  d2.loss_bbox: 0.0303  d2.loss_iou: 0.2961  d3.loss_cls: 0.0499  d3.loss_bbox: 0.0303  d3.loss_iou: 0.2991  d4.loss_cls: 0.0496  d4.loss_bbox: 0.0304  d4.loss_iou: 0.2981  enc_loss_cls: 0.1066  enc_loss_bbox: 0.0332  enc_loss_iou: 0.3238  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0226  dn_loss_iou: 0.2145  d0.dn_loss_cls: 0.0217  d0.dn_loss_bbox: 0.0299  d0.dn_loss_iou: 0.2755  d1.dn_loss_cls: 0.0069  d1.dn_loss_bbox: 0.0236  d1.dn_loss_iou: 0.2245  d2.dn_loss_cls: 0.0048  d2.dn_loss_bbox: 0.0229  d2.dn_loss_iou: 0.2159  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0227  d3.dn_loss_iou: 0.2150  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.0227  d4.dn_loss_iou: 0.2144  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 08:01:24 - mmengine - INFO - Epoch(train) [5][1400/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:22:35  time: 2.1304  data_time: 0.0184  memory: 17573  grad_norm: 27.3631  loss: 4.2692  loss_cls: 0.0435  loss_bbox: 0.0307  loss_iou: 0.2830  d0.loss_cls: 0.1114  d0.loss_bbox: 0.0325  d0.loss_iou: 0.2987  d1.loss_cls: 0.0627  d1.loss_bbox: 0.0316  d1.loss_iou: 0.2894  d2.loss_cls: 0.0516  d2.loss_bbox: 0.0311  d2.loss_iou: 0.2870  d3.loss_cls: 0.0468  d3.loss_bbox: 0.0308  d3.loss_iou: 0.2838  d4.loss_cls: 0.0433  d4.loss_bbox: 0.0307  d4.loss_iou: 0.2830  enc_loss_cls: 0.0983  enc_loss_bbox: 0.0352  enc_loss_iou: 0.3197  dn_loss_cls: 0.0040  dn_loss_bbox: 0.0234  dn_loss_iou: 0.2116  d0.dn_loss_cls: 0.0233  d0.dn_loss_bbox: 0.0307  d0.dn_loss_iou: 0.2706  d1.dn_loss_cls: 0.0077  d1.dn_loss_bbox: 0.0249  d1.dn_loss_iou: 0.2244  d2.dn_loss_cls: 0.0049  d2.dn_loss_bbox: 0.0238  d2.dn_loss_iou: 0.2147  d3.dn_loss_cls: 0.0042  d3.dn_loss_bbox: 0.0235  d3.dn_loss_iou: 0.2127  d4.dn_loss_cls: 0.0040  d4.dn_loss_bbox: 0.0234  d4.dn_loss_iou: 0.2117  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 08:03:12 - mmengine - INFO - Epoch(train) [5][1450/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:20:48  time: 2.1493  data_time: 0.0170  memory: 17568  grad_norm: 24.3293  loss: 4.0617  loss_cls: 0.0428  loss_bbox: 0.0316  loss_iou: 0.2622  d0.loss_cls: 0.1004  d0.loss_bbox: 0.0329  d0.loss_iou: 0.2725  d1.loss_cls: 0.0624  d1.loss_bbox: 0.0322  d1.loss_iou: 0.2673  d2.loss_cls: 0.0539  d2.loss_bbox: 0.0317  d2.loss_iou: 0.2629  d3.loss_cls: 0.0472  d3.loss_bbox: 0.0317  d3.loss_iou: 0.2630  d4.loss_cls: 0.0447  d4.loss_bbox: 0.0316  d4.loss_iou: 0.2621  enc_loss_cls: 0.1050  enc_loss_bbox: 0.0353  enc_loss_iou: 0.2906  dn_loss_cls: 0.0054  dn_loss_bbox: 0.0240  dn_loss_iou: 0.2023  d0.dn_loss_cls: 0.0240  d0.dn_loss_bbox: 0.0321  d0.dn_loss_iou: 0.2621  d1.dn_loss_cls: 0.0095  d1.dn_loss_bbox: 0.0251  d1.dn_loss_iou: 0.2122  d2.dn_loss_cls: 0.0065  d2.dn_loss_bbox: 0.0243  d2.dn_loss_iou: 0.2050  d3.dn_loss_cls: 0.0057  d3.dn_loss_bbox: 0.0241  d3.dn_loss_iou: 0.2028  d4.dn_loss_cls: 0.0055  d4.dn_loss_bbox: 0.0240  d4.dn_loss_iou: 0.2022  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 08:04:58 - mmengine - INFO - Epoch(train) [5][1500/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:19:01  time: 2.1271  data_time: 0.0172  memory: 17592  grad_norm: 25.3982  loss: 3.8064  loss_cls: 0.0341  loss_bbox: 0.0289  loss_iou: 0.2575  d0.loss_cls: 0.0810  d0.loss_bbox: 0.0304  d0.loss_iou: 0.2680  d1.loss_cls: 0.0491  d1.loss_bbox: 0.0296  d1.loss_iou: 0.2637  d2.loss_cls: 0.0414  d2.loss_bbox: 0.0293  d2.loss_iou: 0.2604  d3.loss_cls: 0.0381  d3.loss_bbox: 0.0290  d3.loss_iou: 0.2583  d4.loss_cls: 0.0352  d4.loss_bbox: 0.0289  d4.loss_iou: 0.2575  enc_loss_cls: 0.0818  enc_loss_bbox: 0.0320  enc_loss_iou: 0.2822  dn_loss_cls: 0.0031  dn_loss_bbox: 0.0221  dn_loss_iou: 0.1895  d0.dn_loss_cls: 0.0193  d0.dn_loss_bbox: 0.0295  d0.dn_loss_iou: 0.2470  d1.dn_loss_cls: 0.0058  d1.dn_loss_bbox: 0.0234  d1.dn_loss_iou: 0.2001  d2.dn_loss_cls: 0.0039  d2.dn_loss_bbox: 0.0224  d2.dn_loss_iou: 0.1922  d3.dn_loss_cls: 0.0034  d3.dn_loss_bbox: 0.0222  d3.dn_loss_iou: 0.1905  d4.dn_loss_cls: 0.0032  d4.dn_loss_bbox: 0.0221  d4.dn_loss_iou: 0.1894  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 08:06:45 - mmengine - INFO - Epoch(train) [5][1550/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:17:15  time: 2.1373  data_time: 0.0185  memory: 17580  grad_norm: 27.2913  loss: 3.5715  loss_cls: 0.0276  loss_bbox: 0.0267  loss_iou: 0.2244  d0.loss_cls: 0.0636  d0.loss_bbox: 0.0284  d0.loss_iou: 0.2349  d1.loss_cls: 0.0396  d1.loss_bbox: 0.0273  d1.loss_iou: 0.2274  d2.loss_cls: 0.0329  d2.loss_bbox: 0.0272  d2.loss_iou: 0.2288  d3.loss_cls: 0.0302  d3.loss_bbox: 0.0268  d3.loss_iou: 0.2241  d4.loss_cls: 0.0277  d4.loss_bbox: 0.0267  d4.loss_iou: 0.2244  enc_loss_cls: 0.0750  enc_loss_bbox: 0.0312  enc_loss_iou: 0.2491  dn_loss_cls: 0.0031  dn_loss_bbox: 0.0234  dn_loss_iou: 0.2012  d0.dn_loss_cls: 0.0206  d0.dn_loss_bbox: 0.0315  d0.dn_loss_iou: 0.2576  d1.dn_loss_cls: 0.0058  d1.dn_loss_bbox: 0.0246  d1.dn_loss_iou: 0.2111  d2.dn_loss_cls: 0.0038  d2.dn_loss_bbox: 0.0237  d2.dn_loss_iou: 0.2038  d3.dn_loss_cls: 0.0033  d3.dn_loss_bbox: 0.0235  d3.dn_loss_iou: 0.2017  d4.dn_loss_cls: 0.0031  d4.dn_loss_bbox: 0.0234  d4.dn_loss_iou: 0.2012  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 08:08:32 - mmengine - INFO - Epoch(train) [5][1600/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:15:28  time: 2.1459  data_time: 0.0175  memory: 17594  grad_norm: 33.6516  loss: 3.9001  loss_cls: 0.0356  loss_bbox: 0.0283  loss_iou: 0.2575  d0.loss_cls: 0.0859  d0.loss_bbox: 0.0298  d0.loss_iou: 0.2703  d1.loss_cls: 0.0480  d1.loss_bbox: 0.0289  d1.loss_iou: 0.2624  d2.loss_cls: 0.0385  d2.loss_bbox: 0.0287  d2.loss_iou: 0.2608  d3.loss_cls: 0.0350  d3.loss_bbox: 0.0284  d3.loss_iou: 0.2583  d4.loss_cls: 0.0350  d4.loss_bbox: 0.0283  d4.loss_iou: 0.2577  enc_loss_cls: 0.0851  enc_loss_bbox: 0.0319  enc_loss_iou: 0.2863  dn_loss_cls: 0.0030  dn_loss_bbox: 0.0225  dn_loss_iou: 0.2019  d0.dn_loss_cls: 0.0216  d0.dn_loss_bbox: 0.0314  d0.dn_loss_iou: 0.2659  d1.dn_loss_cls: 0.0061  d1.dn_loss_bbox: 0.0239  d1.dn_loss_iou: 0.2137  d2.dn_loss_cls: 0.0040  d2.dn_loss_bbox: 0.0229  d2.dn_loss_iou: 0.2052  d3.dn_loss_cls: 0.0034  d3.dn_loss_bbox: 0.0226  d3.dn_loss_iou: 0.2028  d4.dn_loss_cls: 0.0031  d4.dn_loss_bbox: 0.0225  d4.dn_loss_iou: 0.2018  loss_num: 0.0001  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 08:10:19 - mmengine - INFO - Epoch(train) [5][1650/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:13:41  time: 2.1300  data_time: 0.0177  memory: 17599  grad_norm: 28.5455  loss: 3.5468  loss_cls: 0.0198  loss_bbox: 0.0268  loss_iou: 0.2200  d0.loss_cls: 0.0623  d0.loss_bbox: 0.0278  d0.loss_iou: 0.2269  d1.loss_cls: 0.0352  d1.loss_bbox: 0.0270  d1.loss_iou: 0.2223  d2.loss_cls: 0.0250  d2.loss_bbox: 0.0268  d2.loss_iou: 0.2220  d3.loss_cls: 0.0214  d3.loss_bbox: 0.0272  d3.loss_iou: 0.2210  d4.loss_cls: 0.0202  d4.loss_bbox: 0.0268  d4.loss_iou: 0.2200  enc_loss_cls: 0.0710  enc_loss_bbox: 0.0300  enc_loss_iou: 0.2400  dn_loss_cls: 0.0025  dn_loss_bbox: 0.0252  dn_loss_iou: 0.2101  d0.dn_loss_cls: 0.0218  d0.dn_loss_bbox: 0.0329  d0.dn_loss_iou: 0.2662  d1.dn_loss_cls: 0.0056  d1.dn_loss_bbox: 0.0263  d1.dn_loss_iou: 0.2181  d2.dn_loss_cls: 0.0034  d2.dn_loss_bbox: 0.0255  d2.dn_loss_iou: 0.2120  d3.dn_loss_cls: 0.0027  d3.dn_loss_bbox: 0.0254  d3.dn_loss_iou: 0.2110  d4.dn_loss_cls: 0.0026  d4.dn_loss_bbox: 0.0252  d4.dn_loss_iou: 0.2101  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 08:12:05 - mmengine - INFO - Epoch(train) [5][1700/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:11:55  time: 2.1327  data_time: 0.0174  memory: 17585  grad_norm: 25.5905  loss: 3.6423  loss_cls: 0.0337  loss_bbox: 0.0256  loss_iou: 0.2257  d0.loss_cls: 0.0771  d0.loss_bbox: 0.0274  d0.loss_iou: 0.2429  d1.loss_cls: 0.0452  d1.loss_bbox: 0.0263  d1.loss_iou: 0.2319  d2.loss_cls: 0.0395  d2.loss_bbox: 0.0259  d2.loss_iou: 0.2285  d3.loss_cls: 0.0353  d3.loss_bbox: 0.0257  d3.loss_iou: 0.2264  d4.loss_cls: 0.0346  d4.loss_bbox: 0.0256  d4.loss_iou: 0.2258  enc_loss_cls: 0.0851  enc_loss_bbox: 0.0286  enc_loss_iou: 0.2550  dn_loss_cls: 0.0019  dn_loss_bbox: 0.0231  dn_loss_iou: 0.2035  d0.dn_loss_cls: 0.0182  d0.dn_loss_bbox: 0.0304  d0.dn_loss_iou: 0.2599  d1.dn_loss_cls: 0.0039  d1.dn_loss_bbox: 0.0242  d1.dn_loss_iou: 0.2137  d2.dn_loss_cls: 0.0025  d2.dn_loss_bbox: 0.0235  d2.dn_loss_iou: 0.2059  d3.dn_loss_cls: 0.0022  d3.dn_loss_bbox: 0.0232  d3.dn_loss_iou: 0.2041  d4.dn_loss_cls: 0.0020  d4.dn_loss_bbox: 0.0231  d4.dn_loss_iou: 0.2034  loss_num: 0.0002  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 08:13:53 - mmengine - INFO - Epoch(train) [5][1750/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:10:08  time: 2.1549  data_time: 0.0181  memory: 17568  grad_norm: 26.7167  loss: 4.0174  loss_cls: 0.0414  loss_bbox: 0.0311  loss_iou: 0.2701  d0.loss_cls: 0.0890  d0.loss_bbox: 0.0317  d0.loss_iou: 0.2752  d1.loss_cls: 0.0556  d1.loss_bbox: 0.0314  d1.loss_iou: 0.2737  d2.loss_cls: 0.0464  d2.loss_bbox: 0.0313  d2.loss_iou: 0.2722  d3.loss_cls: 0.0443  d3.loss_bbox: 0.0312  d3.loss_iou: 0.2705  d4.loss_cls: 0.0393  d4.loss_bbox: 0.0312  d4.loss_iou: 0.2709  enc_loss_cls: 0.0988  enc_loss_bbox: 0.0340  enc_loss_iou: 0.2948  dn_loss_cls: 0.0045  dn_loss_bbox: 0.0234  dn_loss_iou: 0.1962  d0.dn_loss_cls: 0.0236  d0.dn_loss_bbox: 0.0314  d0.dn_loss_iou: 0.2567  d1.dn_loss_cls: 0.0078  d1.dn_loss_bbox: 0.0247  d1.dn_loss_iou: 0.2075  d2.dn_loss_cls: 0.0052  d2.dn_loss_bbox: 0.0237  d2.dn_loss_iou: 0.1987  d3.dn_loss_cls: 0.0046  d3.dn_loss_bbox: 0.0235  d3.dn_loss_iou: 0.1970  d4.dn_loss_cls: 0.0045  d4.dn_loss_bbox: 0.0234  d4.dn_loss_iou: 0.1962  loss_num: 0.0001  d0.loss_num: 0.0002  d1.loss_num: 0.0001  d2.loss_num: 0.0001  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 08:15:40 - mmengine - INFO - Epoch(train) [5][1800/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:08:21  time: 2.1352  data_time: 0.0177  memory: 17563  grad_norm: 28.9597  loss: 4.2526  loss_cls: 0.0413  loss_bbox: 0.0272  loss_iou: 0.2772  d0.loss_cls: 0.1033  d0.loss_bbox: 0.0287  d0.loss_iou: 0.2864  d1.loss_cls: 0.0604  d1.loss_bbox: 0.0279  d1.loss_iou: 0.2822  d2.loss_cls: 0.0517  d2.loss_bbox: 0.0276  d2.loss_iou: 0.2793  d3.loss_cls: 0.0464  d3.loss_bbox: 0.0273  d3.loss_iou: 0.2774  d4.loss_cls: 0.0444  d4.loss_bbox: 0.0267  d4.loss_iou: 0.2752  enc_loss_cls: 0.0998  enc_loss_bbox: 0.0303  enc_loss_iou: 0.3025  dn_loss_cls: 0.0042  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2235  d0.dn_loss_cls: 0.0241  d0.dn_loss_bbox: 0.0319  d0.dn_loss_iou: 0.2904  d1.dn_loss_cls: 0.0069  d1.dn_loss_bbox: 0.0254  d1.dn_loss_iou: 0.2361  d2.dn_loss_cls: 0.0047  d2.dn_loss_bbox: 0.0245  d2.dn_loss_iou: 0.2274  d3.dn_loss_cls: 0.0043  d3.dn_loss_bbox: 0.0242  d3.dn_loss_iou: 0.2244  d4.dn_loss_cls: 0.0042  d4.dn_loss_bbox: 0.0241  d4.dn_loss_iou: 0.2235  loss_num: 0.0001  d0.loss_num: 0.0004  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 08:17:27 - mmengine - INFO - Epoch(train) [5][1850/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:06:34  time: 2.1503  data_time: 0.0183  memory: 17584  grad_norm: 34.7410  loss: 4.6364  loss_cls: 0.0562  loss_bbox: 0.0321  loss_iou: 0.3205  d0.loss_cls: 0.1123  d0.loss_bbox: 0.0333  d0.loss_iou: 0.3311  d1.loss_cls: 0.0693  d1.loss_bbox: 0.0326  d1.loss_iou: 0.3281  d2.loss_cls: 0.0603  d2.loss_bbox: 0.0325  d2.loss_iou: 0.3242  d3.loss_cls: 0.0582  d3.loss_bbox: 0.0322  d3.loss_iou: 0.3214  d4.loss_cls: 0.0561  d4.loss_bbox: 0.0321  d4.loss_iou: 0.3209  enc_loss_cls: 0.1073  enc_loss_bbox: 0.0354  enc_loss_iou: 0.3494  dn_loss_cls: 0.0037  dn_loss_bbox: 0.0241  dn_loss_iou: 0.2192  d0.dn_loss_cls: 0.0214  d0.dn_loss_bbox: 0.0321  d0.dn_loss_iou: 0.2816  d1.dn_loss_cls: 0.0061  d1.dn_loss_bbox: 0.0253  d1.dn_loss_iou: 0.2292  d2.dn_loss_cls: 0.0046  d2.dn_loss_bbox: 0.0245  d2.dn_loss_iou: 0.2228  d3.dn_loss_cls: 0.0040  d3.dn_loss_bbox: 0.0242  d3.dn_loss_iou: 0.2201  d4.dn_loss_cls: 0.0038  d4.dn_loss_bbox: 0.0241  d4.dn_loss_iou: 0.2192  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 08:17:49 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 08:19:14 - mmengine - INFO - Epoch(train) [5][1900/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:04:48  time: 2.1316  data_time: 0.0188  memory: 17575  grad_norm: 24.7690  loss: 4.0354  loss_cls: 0.0387  loss_bbox: 0.0277  loss_iou: 0.2608  d0.loss_cls: 0.0872  d0.loss_bbox: 0.0292  d0.loss_iou: 0.2734  d1.loss_cls: 0.0516  d1.loss_bbox: 0.0287  d1.loss_iou: 0.2696  d2.loss_cls: 0.0432  d2.loss_bbox: 0.0280  d2.loss_iou: 0.2650  d3.loss_cls: 0.0399  d3.loss_bbox: 0.0279  d3.loss_iou: 0.2637  d4.loss_cls: 0.0394  d4.loss_bbox: 0.0276  d4.loss_iou: 0.2612  enc_loss_cls: 0.0953  enc_loss_bbox: 0.0320  enc_loss_iou: 0.2911  dn_loss_cls: 0.0036  dn_loss_bbox: 0.0240  dn_loss_iou: 0.2124  d0.dn_loss_cls: 0.0207  d0.dn_loss_bbox: 0.0329  d0.dn_loss_iou: 0.2795  d1.dn_loss_cls: 0.0064  d1.dn_loss_bbox: 0.0253  d1.dn_loss_iou: 0.2233  d2.dn_loss_cls: 0.0043  d2.dn_loss_bbox: 0.0243  d2.dn_loss_iou: 0.2155  d3.dn_loss_cls: 0.0037  d3.dn_loss_bbox: 0.0241  d3.dn_loss_iou: 0.2131  d4.dn_loss_cls: 0.0036  d4.dn_loss_bbox: 0.0240  d4.dn_loss_iou: 0.2124  loss_num: 0.0002  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0002
2025/10/29 08:21:01 - mmengine - INFO - Epoch(train) [5][1950/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:03:01  time: 2.1409  data_time: 0.0185  memory: 17591  grad_norm: 24.8798  loss: 4.1806  loss_cls: 0.0403  loss_bbox: 0.0296  loss_iou: 0.2704  d0.loss_cls: 0.0794  d0.loss_bbox: 0.0306  d0.loss_iou: 0.2805  d1.loss_cls: 0.0557  d1.loss_bbox: 0.0305  d1.loss_iou: 0.2765  d2.loss_cls: 0.0493  d2.loss_bbox: 0.0297  d2.loss_iou: 0.2724  d3.loss_cls: 0.0445  d3.loss_bbox: 0.0297  d3.loss_iou: 0.2712  d4.loss_cls: 0.0413  d4.loss_bbox: 0.0296  d4.loss_iou: 0.2703  enc_loss_cls: 0.0901  enc_loss_bbox: 0.0335  enc_loss_iou: 0.2979  dn_loss_cls: 0.0029  dn_loss_bbox: 0.0250  dn_loss_iou: 0.2238  d0.dn_loss_cls: 0.0239  d0.dn_loss_bbox: 0.0331  d0.dn_loss_iou: 0.2893  d1.dn_loss_cls: 0.0063  d1.dn_loss_bbox: 0.0263  d1.dn_loss_iou: 0.2360  d2.dn_loss_cls: 0.0036  d2.dn_loss_bbox: 0.0254  d2.dn_loss_iou: 0.2265  d3.dn_loss_cls: 0.0031  d3.dn_loss_bbox: 0.0251  d3.dn_loss_iou: 0.2246  d4.dn_loss_cls: 0.0029  d4.dn_loss_bbox: 0.0250  d4.dn_loss_iou: 0.2237  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0001  d4.loss_num: 0.0001
2025/10/29 08:22:48 - mmengine - INFO - Epoch(train) [5][2000/2035]  base_lr: 1.0000e-05 lr: 2.0000e-05  eta: 0:01:14  time: 2.1450  data_time: 0.0185  memory: 17574  grad_norm: 25.9780  loss: 3.5998  loss_cls: 0.0277  loss_bbox: 0.0255  loss_iou: 0.2202  d0.loss_cls: 0.0735  d0.loss_bbox: 0.0271  d0.loss_iou: 0.2340  d1.loss_cls: 0.0397  d1.loss_bbox: 0.0262  d1.loss_iou: 0.2249  d2.loss_cls: 0.0325  d2.loss_bbox: 0.0256  d2.loss_iou: 0.2214  d3.loss_cls: 0.0294  d3.loss_bbox: 0.0255  d3.loss_iou: 0.2197  d4.loss_cls: 0.0276  d4.loss_bbox: 0.0255  d4.loss_iou: 0.2202  enc_loss_cls: 0.0806  enc_loss_bbox: 0.0296  enc_loss_iou: 0.2485  dn_loss_cls: 0.0025  dn_loss_bbox: 0.0254  dn_loss_iou: 0.2079  d0.dn_loss_cls: 0.0183  d0.dn_loss_bbox: 0.0331  d0.dn_loss_iou: 0.2656  d1.dn_loss_cls: 0.0056  d1.dn_loss_bbox: 0.0265  d1.dn_loss_iou: 0.2174  d2.dn_loss_cls: 0.0034  d2.dn_loss_bbox: 0.0257  d2.dn_loss_iou: 0.2103  d3.dn_loss_cls: 0.0026  d3.dn_loss_bbox: 0.0255  d3.dn_loss_iou: 0.2086  d4.dn_loss_cls: 0.0025  d4.dn_loss_bbox: 0.0254  d4.dn_loss_iou: 0.2079  loss_num: 0.0001  d0.loss_num: 0.0003  d1.loss_num: 0.0002  d2.loss_num: 0.0002  d3.loss_num: 0.0002  d4.loss_num: 0.0001
2025/10/29 08:24:02 - mmengine - INFO - Exp name: epoch_50_pretrain_num_grounding_dino_swin-b_finetune_8xb4_5e_refdrone_zero_31_20251029_021801
2025/10/29 08:24:02 - mmengine - INFO - Saving checkpoint at 5 epochs
2025/10/29 08:24:15 - mmengine - INFO - Epoch(val) [5][ 50/429]    eta: 0:00:39  time: 0.1046  data_time: 0.0035  memory: 17569  
2025/10/29 08:24:20 - mmengine - INFO - Epoch(val) [5][100/429]    eta: 0:00:33  time: 0.1007  data_time: 0.0026  memory: 4269  
2025/10/29 08:24:25 - mmengine - INFO - Epoch(val) [5][150/429]    eta: 0:00:28  time: 0.1009  data_time: 0.0026  memory: 4269  
2025/10/29 08:24:31 - mmengine - INFO - Epoch(val) [5][200/429]    eta: 0:00:23  time: 0.1013  data_time: 0.0026  memory: 4269  
2025/10/29 08:24:36 - mmengine - INFO - Epoch(val) [5][250/429]    eta: 0:00:18  time: 0.1011  data_time: 0.0026  memory: 4269  
2025/10/29 08:24:41 - mmengine - INFO - Epoch(val) [5][300/429]    eta: 0:00:13  time: 0.1004  data_time: 0.0026  memory: 4269  
2025/10/29 08:24:46 - mmengine - INFO - Epoch(val) [5][350/429]    eta: 0:00:08  time: 0.1007  data_time: 0.0026  memory: 4269  
2025/10/29 08:24:51 - mmengine - INFO - Epoch(val) [5][400/429]    eta: 0:00:02  time: 0.1000  data_time: 0.0026  memory: 4269  
2025/10/29 08:24:55 - mmengine - INFO - {'instance_F1_score': 0.7251214467798567, 'instance_acc': 0.5722305389221557, 'image_F1_score': 0.5784523300555793, 'image_acc': 0.4254079254079254}
2025/10/29 08:24:55 - mmengine - INFO - Epoch(val) [5][429/429]    grefcoco_val/refdrone/instance_F1_score: 0.7251  grefcoco_val/refdrone/instance_acc: 0.5722  grefcoco_val/refdrone/image_F1_score: 0.5785  grefcoco_val/refdrone/image_acc: 0.4254  data_time: 0.0027  time: 0.1011
